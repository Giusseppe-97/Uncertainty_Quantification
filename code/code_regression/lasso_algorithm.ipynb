{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso\n",
    "Similar to the \"RidgeRegression\" class, we can implement the Lasso algorithm.\n",
    "\n",
    "Please type in the codes in the specified space to complete the construction of the class ``Lasso``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# THIS SHOULD BE ADDAPTED TO THE DATASETS\n",
    "\n",
    "def simulation(m):\n",
    "    \"\"\"\n",
    "    Generate a specified number of samples according to the sparse linear model.\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    m : num_samples\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    x (matrix, m*num_variables) : Input or features \n",
    "    y (matrix, m*1): Output or labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate independent and identically distributed samples as inputs.\n",
    "    x1 = np.random.normal(3,1,[m,1])\n",
    "    x2 = np.random.uniform(0,1,[m,1])\n",
    "    x3 = np.random.normal(1,4,[m,1])\n",
    "    x4 = np.random.normal(-1,1,[m,1])\n",
    "    x5 = np.random.normal(0,1,[m,1])\n",
    "    x6 = np.random.uniform(-1,1,[m,1])\n",
    "    # Generate the true outputs according to the sparse linear model.\n",
    "    y = x1 + 3*x2 + 2 + np.random.normal(0,0.1,[m,1])\n",
    "    return np.hstack([x1,x2,x3,x4,x5,x6]), y\n",
    "\n",
    "# Generate 5000 samples and split them into training and test dataset.\n",
    "X, y = simulation(5000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso():\n",
    "    '''\n",
    "    This is a class for Lasso algorithm.\n",
    "    \n",
    "    The class contains the hyper parameters of the lasso algorithm as attributes, such as the regurization \n",
    "    parameter(Lambda) of L_1 penality.\n",
    "    It also contains the functions for initializing the class, fitting the lasso model and use the fitted \n",
    "    model to predict test samples.\n",
    "    \n",
    "    Attributes:\n",
    "        Lambda:    regularization parameter for L_1 penalty\n",
    "        max_itr:   maximum number of iteration for gradient descent\n",
    "        tol:       if the change in loss is smaller than tol, then we stop iteration\n",
    "        W:         concatenation of weight w and bias b\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, Lambda=0.5, max_itr=100, tol=0.0001):\n",
    "        '''\n",
    "        Initialize the RidgeRegression class\n",
    "        '''\n",
    "        self.Lambda = Lambda\n",
    "        self.max_itr = max_itr\n",
    "        self.tol = tol  \n",
    "    \n",
    "    def _loss_lasso(self, X, y, W):\n",
    "        '''\n",
    "        Calculating the regularized empirical loss\n",
    "        '''\n",
    "        return ((y-X@W).T@(y-X@W))[0,0] + self.Lambda * np.sum(np.abs(W[:X.shape[1]-1,0]))\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        estimate the weight and bias in the lasso model by coordinate gradient descent\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_train*num_variables): input of training samples\n",
    "            y (matrix, num_test*1): output of training samples\n",
    "            \n",
    "        Returns:\n",
    "            self.W (matrix, (num_variables+1)*1): estimation of weight w and bias b\n",
    "        '''\n",
    "        m = x.shape[0]\n",
    "        ### Add the all-one vector to the last column \n",
    "        X = np.concatenate((x,np.ones((m,1))),axis=1)\n",
    "        # weight and bias initialization\n",
    "        d = X.shape[1]\n",
    "        self.W = np.zeros((d,1))\n",
    "        \n",
    "        ### Use the cooridinate gradient descent to update W\n",
    "        previous_loss = self._loss_lasso(X, y, self.W)\n",
    "        for i in range(self.max_itr):\n",
    "            ### Update bias\n",
    "            self.W[-1,0] = np.mean(y.T-x@self.W[:-1,0])\n",
    "            ### Update W_j, j=0,...,d-2\n",
    "            for j in range(d-1):\n",
    "                # Calculate r_j = Y - X@W, with W[j,0]=0 and other elements in W unchanged\n",
    "                copy_W = self.W.copy()\n",
    "                copy_W[j,0] = 0\n",
    "                rj = y - X@copy_W\n",
    "                # Calculate X[:,j]@r_j and X[:,j].T@X[:,j]\n",
    "                aj = X[:,j].T@X[:,j]\n",
    "                bj = 2 * X[:,j]@rj / m \n",
    "                if bj <= -self.Lambda:\n",
    "                    self.W[j,0] = (bj + self.Lambda)/(2*aj)*m\n",
    "                elif bj >= self.Lambda:\n",
    "                    self.W[j,0] = (bj - self.Lambda)/(2*aj)*m\n",
    "                else:\n",
    "                    self.W[j,0] = 0\n",
    "            current_loss = self._loss_lasso(X, y, self.W)\n",
    "            if previous_loss - current_loss < self.tol:\n",
    "                print(f'Converged after {i} iterations')\n",
    "                break\n",
    "            else:\n",
    "                previous_loss = current_loss\n",
    "        return self.W\n",
    "    \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        predict the output of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted outputs of test samples\n",
    "        ''' \n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x,np.ones((m,1))),axis=1)\n",
    "        return  X@self.W\n",
    "from sklearn.metrics import mean_squared_error\n",
    "### Initial the class Lasso by assigning values to the parameters.\n",
    "model = Lasso(Lambda = 0.015, max_itr=10000, tol=1e-5)\n",
    "### Fit model with training data\n",
    "W = model.fit(X_train, y_train)\n",
    "### Predict the output of test samples\n",
    "y_pred = model.predict(X_test)\n",
    "### Evaluate the model by calculating the MSE of test samples.\n",
    "mse = mean_squared_error(y_pred, y_test)\n",
    "### Print MSE \n",
    "print(\"MSE of Lasso is {}\".format(mse))\n",
    "### Print the estimated w and b\n",
    "print(\"The weight w of Lasso is \\n {}.\".format(W[:X_test.shape[1],0].T))\n",
    "print(\"The bias b of Lasso is {}.\".format(W[X_test.shape[1],0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
