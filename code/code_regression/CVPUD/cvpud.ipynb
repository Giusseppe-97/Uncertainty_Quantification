{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVPUD dataset\n",
    "## Context\n",
    "## Regression Tree\n",
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from encodings import search_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import necessary tools from the sklearn library\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "# Import sklearn library tools used ONLY for validating my results\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score, recall_score, precision_score, auc, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import randint as sp_randint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_regression\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    scaler_y = MinMaxScaler()\n",
    "    data_y = np.array(data_y)\n",
    "    data_y = scaler_y.fit_transform(data_y.reshape(-1,1))\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # read dataset from csv file\n",
    "    data_name = \"CVPUD_regression\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "    \n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.33, random_state=2200)\n",
    "    return train_X, test_X, train_y, test_y, data_X, data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression():\n",
    "    '''\n",
    "    This is a class for ridge regression algorithm.\n",
    "    \n",
    "    The class contains the hyper parameters of the ridge regression algorithm as attributes, such as the regurization \n",
    "    parameter(Lambda).\n",
    "    It also contains the functions for initializing the class, calculating the loss, fitting the ridge regression \n",
    "    model and use the fitted model to predict test samples.\n",
    "    \n",
    "    Attributes:\n",
    "        lr:        learning rate of gradient descent\n",
    "        Lambda:    regularization parameter for L_2 penalty\n",
    "        max_itr:   maximum number of iteration for gradient descent\n",
    "        tol:       if the change in loss is smaller than tol, then we stop iteration\n",
    "        W:         concatenation of weight w and bias b\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, lr, Lambda, max_itr, tol):\n",
    "        '''\n",
    "        Initialize the RidgeRegression class\n",
    "        '''\n",
    "        self.lr = lr\n",
    "        self.Lambda = Lambda\n",
    "        self.max_itr = max_itr\n",
    "        self.tol = tol\n",
    "        \n",
    "    def _loss_ridge(self, X, y, W):\n",
    "        '''\n",
    "        Calculating the regularized empirical loss\n",
    "        '''\n",
    "        return ((y-X@W).T@(y-X@W))[0,0] + self.Lambda * np.sum(W[:X.shape[1]-1,0]**2)\n",
    "    \n",
    "    \n",
    "    def fit(self,x,y):  \n",
    "        '''\n",
    "        estimate the weight and bias in the ridge regression model by gradient descent\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_train*num_variables): input of training samples\n",
    "            y (matrix, num_test*1): output of training samples\n",
    "            \n",
    "        Returns:\n",
    "            self.W (matrix, (num_variables+1)*1): estimation of weight w and bias b\n",
    "        ''' \n",
    "        m = x.shape[0]\n",
    "        ### Add the all-one vector to the last column \n",
    "        X = np.concatenate((x,np.ones((m,1))),axis=1)\n",
    "        d = X.shape[1]\n",
    "        n= X.shape[0]\n",
    "        self.W = np.ones((d,1)) \n",
    "        \n",
    "        previous_loss = self._loss_ridge(X, y, self.W)\n",
    "        \n",
    "        for i in range(self.max_itr):\n",
    "            ### Use the gradient descent to update self.W\n",
    "            ##################################\n",
    "            ##################################\n",
    "            ##### Write your codes below #####\n",
    "            \n",
    "            #calculating the gradient\n",
    "            grad = (2*np.dot(np.transpose(X),np.dot(X,self.W)-y)+2*self.Lambda*((np.append(self.W[:-1],[0])).reshape(-1,1)))/n\n",
    "            #updating the weight vector\n",
    "            self.W = self.W - self.lr * grad\n",
    "            \n",
    "            ##################################\n",
    "            ##################################\n",
    "            current_loss = self._loss_ridge(X, y, self.W)\n",
    "            \n",
    "            if previous_loss - current_loss < self.tol:\n",
    "                print(f'Converged after {i} iterations')\n",
    "                break\n",
    "            else:\n",
    "                previous_loss = current_loss\n",
    "                \n",
    "        return self.W\n",
    "    \n",
    "    def predict(self,x): \n",
    "        '''\n",
    "        predict the output of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted outputs of test samples\n",
    "        ''' \n",
    "        ##################################\n",
    "        ##################################\n",
    "        ##### Write your codes below #####\n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x,np.ones((m,1))),axis=1)    \n",
    "        y = np.dot(X,self.W)   \n",
    "        return(y)  \n",
    "        ##################################\n",
    "        ##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "### Initial the class RidgeRegression by assigning values to the parameters.\n",
    "model = RidgeRegression(lr=0.01, Lambda=0.002, max_itr = 20000, tol = 1e-5)\n",
    "### Fit model with training data\n",
    "W = model.fit(main()[0], main()[2])\n",
    "### Predict the output of test samples\n",
    "y_pred = model.predict(main()[1])\n",
    "### Evaluate the model by calculating the MSE of test samples.\n",
    "mse = mean_squared_error(y_pred, main()[3])\n",
    "### Print MSE \n",
    "print(\"MSE of Ridge Regression is {}\".format(mse))\n",
    "### Print the estimated w and b\n",
    "print(\"The weight w of Ridge Regression is \\n {}.\".format(W[:main()[2].shape[1],0].T))\n",
    "print(\"The bias b of Ridge Regression is {}.\".format(W[main()[2].shape[1],0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_regression\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    scaler_y = MinMaxScaler()\n",
    "    data_y = np.array(data_y)\n",
    "    data_y = scaler_y.fit_transform(data_y.reshape(-1,1)).reshape(-1)\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # read dataset from csv file\n",
    "    data_name = \"CVPUD_regression\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "    data_X, data_y = pd.DataFrame(data_X), pd.Series(data_y)\n",
    "    data_X.columns = ['x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10','x11','x12','x13','x14','x15','x16','x17','x18','x19','x20','x21','x22','x23','x24','x25','x26','x27','x28','x29','x30','x31','x32','x33','x34','x35','x36','x37','x38','x39','x40','x41','x42','x43','x44','x45','x46','x47','x48','x49','x50','x51','x52','x53','x54','x55','x56','x57','x58','x59','x60','x61','x62','x63','x64','x65','x66','x67','x68','x69','x70','x71','x72','x73','x74','x75','x76','x77','x78','x79','x80','x81','x82','x83','x84','x85','x86','x87','x88','x89','x90','x91','x92','x93','x94','x95','x96','x97','x98','x99','x100','x101','x102','x103','x104','x105','x106','x107','x108','x109','x110','x111','x112','x113','x114','x115','x116','x117','x118','x119','x120','x121','x122','x123','x124','x125','x126','x127','x128','x129','x130','x131','x132','x133','x134','x135','x136','x137', 'x138', 'x139', 'x140','x141', 'x142', 'x143', 'x144', 'x145','x146']\n",
    "    data_y = (data_y - data_y.mean()) / data_y.std()\n",
    "\n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.33, random_state=2200)\n",
    "    return train_X, test_X, train_y, test_y, data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am loading the full dataset and renaming the columns to keep better track of each attribute\n",
    "data_dir = \"..\\..\\..\\data\\data_regression\"\n",
    "data_path = os.path.join(data_dir, \"CVPUD_regression.csv\")\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Implement the class of regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### criterion function for regression tree\n",
    "def sum_squared_distance_to_mean(X, y):\n",
    "    '''\n",
    "    function used to calculate the squared error.\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    return np.var(y) * n\n",
    "\n",
    "class DecisionTreeRegressor(object):\n",
    "    '''\n",
    "    This class is for decision tree regression\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of decision tree regression\n",
    "        - tolS: a tolerance on the error reduction. we will stop the splitting if\n",
    "            low error reduction.\n",
    "        - tree: a nested dictionary representing the decision tree structure.\n",
    "    '''\n",
    "    def __init__(self, criterion, tolS=0.1):\n",
    "        # Initialization\n",
    "        self.criterion = criterion\n",
    "        self.tolS = tolS\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        function used to fit the decision tree regressor\n",
    "\n",
    "        Args:\n",
    "            X - features of training samples, a pandas dataframe with shape (n, d), where\n",
    "                X.columns is the column name of X, and we can use X['feat'] to index all\n",
    "                values of the feature named `feat`.\n",
    "            y - target values (continuous, scaled) of training samples, a pandas series with shape (n,)\n",
    "        '''\n",
    "        self.tree = self.create_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to fit the decision tree regressor\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "\n",
    "        Returns:\n",
    "            y - predictions of test samples, a pandas series with shape (n,)\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y = []\n",
    "        for i in range(n):\n",
    "            y.append(self.predict_each(X.iloc[i], self.tree))\n",
    "        y = pd.Series(y)\n",
    "        y.index = X.index\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def choose_best_split(X, y, criterion, tolS=1):\n",
    "        '''\n",
    "        function used to choose the best split feature and split point\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "            y - target values of training samples, a pandas series with shape (n,)\n",
    "            criterion - function used to measure the quality of a split\n",
    "            tolS - a tolerance on the error reduction.\n",
    "\n",
    "        Returns:\n",
    "            best_feat: the feature used to split on for this node\n",
    "            best_split: the value of the feature used to split for this node\n",
    "        '''\n",
    "        # used to calculate the error reduction\n",
    "        origin_score = criterion(X, y)\n",
    "        # initialize\n",
    "        best_feat, best_split = None, None\n",
    "        best_score = np.inf\n",
    "        # search for each feature\n",
    "        for feat in X.columns:\n",
    "            # if all values of this feature are equal, do not split this feature\n",
    "            X_feat_value = set(X[feat])\n",
    "            if len(X_feat_value) == 1:\n",
    "                continue\n",
    "            # otherwise, search for each possible split point of this feature\n",
    "            for split in X_feat_value:\n",
    "                # divide the dataset into two parts according to the split\n",
    "                idx1 = X[feat] < split\n",
    "                idx2 = X[feat] >= split\n",
    "                # calculate score to evaluate the quality of a split\n",
    "                score1 = criterion(X.loc[idx1], y.loc[idx1])\n",
    "                score2 = criterion(X.loc[idx2], y.loc[idx2])\n",
    "                score = score1 + score2\n",
    "                if score < best_score:\n",
    "                    # choose the split with the largest (variance) reduction\n",
    "                    best_feat = feat\n",
    "                    best_split = split\n",
    "                    best_score = score\n",
    "        if origin_score - best_score < tolS:\n",
    "            # Do not split if the (variance) reduction is low\n",
    "            return None, None\n",
    "        else:\n",
    "            # return the feature and the value used for the split\n",
    "            return best_feat, best_split\n",
    "\n",
    "    def create_tree(self, X, y):\n",
    "        '''\n",
    "        build the decision tree regressor in a recursive manner.\n",
    "        use a dictionary to represent tree node:\n",
    "            if the tree node is an internal node, the dictionary will have the following five items:\n",
    "                - tree[\"is_leaf\"] stores whether the tree node is a leaf node or not.\n",
    "                - tree[\"split_feat\"] stores the feature used to split on for this node\n",
    "                - tree[\"split_point\"] stores the value of the feature used to split\n",
    "                - tree[\"left\"] is a (nested) dictionary used to store the left subtree of this node.\n",
    "                - tree[\"right\"] is a (nested) dictionary used to store the right subtree of this node.\n",
    "            if the tree node is a leaf node, the dictionary will have the following two items:\n",
    "                - tree[\"is_leaf\"] stores whether the tree node is a leaf node or not.\n",
    "                - tree[\"value\"] stores the prediction at the leaf node\n",
    "        returns a nested dictionary used to store the tree structure.\n",
    "        '''\n",
    "        Tree = {}\n",
    "        # create a leaf node if all values are equal\n",
    "        y_values, y_counts = np.unique(y, return_counts=True)\n",
    "        if len(y_counts) == 1:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            Tree[\"value\"] = y_values[0]\n",
    "            return Tree\n",
    "        # create a leaf node if feature set is empty (a low error reduction)\n",
    "        feat, split = self.choose_best_split(X, y, self.criterion, self.tolS)\n",
    "        if feat is None:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            Tree[\"value\"] = y.mean()\n",
    "            return Tree\n",
    "        # otherwise, create an internal node\n",
    "        Tree[\"is_leaf\"] = False\n",
    "        Tree[\"split_feat\"] = feat\n",
    "        Tree[\"split_point\"] = split\n",
    "        # divide the dataset (X, y) into two parts according to tree split\n",
    "        # build the left subtree\n",
    "        idx = X[feat] < split\n",
    "        Tree[\"left\"] = self.create_tree(X.loc[idx], y.loc[idx])\n",
    "        # build the right subtree\n",
    "        idx = X[feat] >= split\n",
    "        Tree[\"right\"] = self.create_tree(X.loc[idx], y.loc[idx])\n",
    "        return Tree\n",
    "\n",
    "    @staticmethod\n",
    "    def predict_each(x, tree):\n",
    "        '''\n",
    "        for each sample, get the prediction of decision tree regressor in a recursive manner.\n",
    "\n",
    "        Args:\n",
    "            x - features of a sample, a pandas Series with shape (d,)\n",
    "            tree - a nested dictionary representing the decision tree structure.\n",
    "\n",
    "        Returns:\n",
    "            the prediction of the sample `x`\n",
    "        '''\n",
    "        if tree[\"is_leaf\"] is True:\n",
    "            # if the `tree` is a leaf node, get the prediction at the leaf node\n",
    "            return tree[\"value\"]\n",
    "        else:\n",
    "            # the 'tree' is a nested dictionary with the following four items:\n",
    "            #     `split_feat`, `split_point`, `left`, 'right`.\n",
    "            # get the feature used to split on for this tree\n",
    "            feat = tree[\"split_feat\"]\n",
    "            # get the value of the feature used to split\n",
    "            split = tree[\"split_point\"]\n",
    "            # get the value of the feature for the sample `x`\n",
    "            value = x[feat]\n",
    "            if value < split:\n",
    "                # search for the left subtree\n",
    "                return DecisionTreeRegressor.predict_each(x, tree[\"left\"])\n",
    "            else:\n",
    "                # search for the right subtree\n",
    "                return DecisionTreeRegressor.predict_each(x, tree[\"right\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # dataset preparation\n",
    "    X, y = main()[0], main()[2]\n",
    "    # initialize the decision tree regressor\n",
    "    model = DecisionTreeRegressor(criterion=sum_squared_distance_to_mean, tolS=0.1)\n",
    "    # fit the regression tree\n",
    "    model.fit(X, y)\n",
    "    # get the prediction of samples\n",
    "    y_hat = model.predict(main()[1])\n",
    "    # calculate the mean squared error of samples\n",
    "    print(\"The MSE of Random Regression Tree is:\", ((y - y_hat)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppor Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am loading the full dataset and renaming the columns to keep better track of each attribute\n",
    "data_dir = \"..\\..\\..\\data\\data_regression\"\n",
    "data_path = os.path.join(data_dir, \"CVPUD_regression.csv\")\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation map to see how features are correlated with LeagueIndex\n",
    "corrmat = df.corr()\n",
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(corrmat, vmax=0.9, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "# X = np.array(df.iloc[:,:-1])[:2214,:]\n",
    "# y = np.array(df.iloc[:,-1])[:2214]\n",
    "X = np.array(df.iloc[:,:-1])\n",
    "y = np.array(df.iloc[:,-1])\n",
    "# normalize\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "scaler_y = MinMaxScaler()\n",
    "data_y = np.array(y)\n",
    "data_y = scaler_y.fit_transform(data_y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "# split the train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implementation of SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVR():\n",
    "    '''\n",
    "    This is a class for support vector regressor.\n",
    "    \n",
    "    The class contains the hyper-parameters such as $C$, $\\epsilon$ and the kernel bandwidth $\\sigma$. It also contains \n",
    "    the alpha vector, the tolerance for prediction error and the maximum number of iteration.\n",
    "    \n",
    "    It contains the functions for calculating the kernel matrix, fitting the model to estimate alpha, hat_alpha and b \n",
    "    with SMO algorithm, making predictions and other fundamental functions.\n",
    "    \n",
    "    Attributes:\n",
    "        C (positive number)         - the hyperparameter for SVM algorithm\n",
    "        sigma (positive number)     - the kernel bandwidth $\\sigma$ of Gaussian kernel \n",
    "        toler (positive number)     - $\\epsilon$, the threshold value of prediction error. If the prediction error of \n",
    "                                      a sample is larger than this value, the corresponding alphas[i] or hat_alpha[i]\n",
    "                                      will be probably updated.\n",
    "        maxIter (positive integer)  - the maximum number of iteration to search a pair of alpha's to update\n",
    "        alphas (vector, num_samples) - the alpha vector in the dual problem \n",
    "        hat_alphas (vector, num_samples) - the hat_alpha vector in the dual problem \n",
    "        b (number)                  - the bias b\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, C = 1, sigma = 1, toler = 1, maxIter = 10):\n",
    "        self.C = C\n",
    "        self.sigma = sigma\n",
    "        self.toler = toler\n",
    "        self.maxIter = maxIter\n",
    "        self.alphas = 0\n",
    "        self.hat_alphas = 0\n",
    "        self.b = 0\n",
    "        \n",
    "    def rbfkernel(self, X, Y):\n",
    "        '''\n",
    "        Calculate the kernel matrix whose (i,j)-th entry is $k(X[i,:], Y[j,:])$.\n",
    "        '''\n",
    "        m = X.shape[0]\n",
    "        n = Y.shape[0]\n",
    "        K = np.zeros(shape=(m, n))\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                K[i, j] = np.sum((X[i,:] - Y[j,:])**2)\n",
    "        K = np.exp(-K / (2 * self.sigma**2))\n",
    "        return K\n",
    "\n",
    "    def selectJrand(self, i, m):\n",
    "        '''\n",
    "        Randomly choose an index $j\\neq i$ from 0 to m-1 and choose alpha[i] (or hat_alpha[i]) if hat_alpha[i] = 0 (or alpha[i]=0).\n",
    "        '''\n",
    "        j = i\n",
    "        while (j == i):\n",
    "            j = int(np.random.uniform(0, m))\n",
    "        if self.alphas[j] == 0:\n",
    "            choose_hat_j = 1\n",
    "        else:\n",
    "            choose_hat_j = 0\n",
    "        return (j, choose_hat_j)\n",
    "\n",
    "\n",
    "    def clipAlpha(self, aj, H, L):\n",
    "        '''\n",
    "        Clip the vale aj by the lower bound L and upper bound H\n",
    "        '''\n",
    "        if aj > H:\n",
    "            aj = H\n",
    "        if L > aj:\n",
    "            aj = L\n",
    "        return aj\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        estimate the alphas vector, hat_alpha vector and bias in the SVM model\n",
    "        \n",
    "        Args: \n",
    "            X_train (matrix, num_train*num_features): features of training samples\n",
    "            y_train (vector, num_train): output of training samples\n",
    "            \n",
    "        Returns:\n",
    "            self.b (a number)                     : the bias\n",
    "            self.alphas (vector, num_samples)     : the alpha vector \n",
    "            self.hat_alphas (vector, num_samples) : the hat_alpha vector \n",
    "        ''' \n",
    "        K_train = self.rbfkernel(X_train, X_train)\n",
    "        self.X_train = X_train\n",
    "        m = K_train.shape[0]\n",
    "        self.alphas = np.zeros((m, ))\n",
    "        self.hat_alphas = np.zeros((m, ))\n",
    "        num_iter = 0\n",
    "        while (num_iter < self.maxIter):\n",
    "            alphaPairsChanged = 0\n",
    "            # optimize for each data vector (with kernel trick)\n",
    "            for i in range(m):  \n",
    "                ### choose hat_alphas[i] or alphas[i] to update\n",
    "                if self.hat_alphas[i] == 0:\n",
    "                    choose_hat_i = 0\n",
    "                else:\n",
    "                    choose_hat_i = 1\n",
    "                fXi = (self.hat_alphas - self.alphas) @ K_train[i, :] + self.b\n",
    "                Ei = fXi - y_train[i]\n",
    "                ### if statement checks if an example violates KKT conditions\n",
    "                if (abs(Ei) < self.toler) or (self.alphas[i] == self.C) or (self.hat_alphas[i] == self.C):\n",
    "                    continue\n",
    "                else:\n",
    "                    j, choose_hat_j = self.selectJrand(i, m)\n",
    "                    fXj = (self.hat_alphas-self.alphas) @ K_train[j, :] + self.b\n",
    "                    Ej = fXj - y_train[j]\n",
    "                    alphaJold = self.alphas[j]\n",
    "                    alphaIold = self.alphas[i]\n",
    "                    hat_alphaJold = self.hat_alphas[j]\n",
    "                    hat_alphaIold = self.hat_alphas[i]\n",
    "                    if (choose_hat_i == 0) and (choose_hat_j == 1):\n",
    "                        L = max(0, self.alphas[i] - self.hat_alphas[j])\n",
    "                        H = min(self.C, self.C + self.alphas[i] - self.hat_alphas[j])\n",
    "                    elif (choose_hat_i == 1) and (choose_hat_j == 0):\n",
    "                        L = max(0, self.hat_alphas[i] - self.alphas[j])\n",
    "                        H = min(self.C, self.C + self.hat_alphas[i] - self.alphas[j])\n",
    "                    elif (choose_hat_i == 0) and (choose_hat_j == 0):\n",
    "                        L = max(0, self.alphas[i] + self.alphas[j] - self.C)\n",
    "                        H = min(self.C, self.alphas[i] + self.alphas[j])\n",
    "                    else:\n",
    "                        L = max(0, self.hat_alphas[i] + self.hat_alphas[j] - self.C)\n",
    "                        H = min(self.C, self.hat_alphas[i] + self.hat_alphas[j])\n",
    "                    if L == H:\n",
    "                        continue\n",
    "                    eta = 2.0 * K_train[j, i] - K_train[j, j] - K_train[i, i]\n",
    "                    if eta >= 0:\n",
    "                        continue\n",
    "                    ### Update i \n",
    "                    if (choose_hat_i == 0) and (choose_hat_j == 1):\n",
    "                        self.alphas[i] += (2 * self.toler - Ei + Ej) / eta\n",
    "                    elif (choose_hat_i == 1) and (choose_hat_j == 0):\n",
    "                        self.hat_alphas[i] += (2 * self.toler + Ei - Ej) / eta\n",
    "                    elif (choose_hat_i == 0) and (choose_hat_j == 0):\n",
    "                        self.alphas[i] += (-Ei + Ej) / eta\n",
    "                    else:\n",
    "                        self.hat_alphas[i] += (Ei - Ej) / eta\n",
    "                    if choose_hat_i == 0:\n",
    "                        self.alphas[i] = self.clipAlpha(self.alphas[i], H, L)\n",
    "                    else:\n",
    "                        self.hat_alphas[i] = self.clipAlpha(self.hat_alphas[i], H, L)\n",
    "                    ### Whether the change amount is large enough or not\n",
    "                    if choose_hat_i == 0:\n",
    "                        if (abs(self.alphas[i] - alphaIold) < 0.00001):\n",
    "                            continue\n",
    "                    else:\n",
    "                        if (abs(self.hat_alphas[i] - hat_alphaIold) < 0.00001):\n",
    "                            continue\n",
    "                    # update j by the same amount as i\n",
    "                    if (choose_hat_i == 0) and (choose_hat_j == 1):\n",
    "                        self.hat_alphas[j] += self.alphas[i] - alphaIold\n",
    "                    elif (choose_hat_i == 1) and (choose_hat_j == 0):\n",
    "                        self.alphas[j] += self.hat_alphas[i] - hat_alphaIold\n",
    "                    elif (choose_hat_i == 0) and (choose_hat_j == 0):\n",
    "                        self.alphas[j] += alphaIold - self.alphas[i]\n",
    "                    else:\n",
    "                        self.hat_alphas[j] += hat_alphaIold - self.hat_alphas[i]\n",
    "                    # update b\n",
    "                    b1 = self.b - Ej - (self.hat_alphas[j] - hat_alphaJold - (self.alphas[j] - alphaJold))    \\\n",
    "                        * K_train[j, j] - (self.hat_alphas[i] - hat_alphaIold - (self.alphas[i] - alphaIold)) * K_train[i, j]\n",
    "                    b2 = self.b - Ei - (self.hat_alphas[j] - hat_alphaJold - (self.alphas[j] - alphaJold))   \\\n",
    "                        * K_train[j, i] - (self.hat_alphas[i] - hat_alphaIold - (self.alphas[i] - alphaIold)) * K_train[i, i]\n",
    "                    if ((0 < self.alphas[j]) and (self.C > self.alphas[j])) or ((0 < self.hat_alphas[j]) and (self.C > self.hat_alphas[j])): \n",
    "                        self.b = b1\n",
    "                    elif ((0 < self.alphas[i]) and (self.C > self.alphas[i])) or ((0 < self.hat_alphas[i]) and (self.C > self.hat_alphas[i])): \n",
    "                        self.b = b2\n",
    "                    else: self.b = (b1 + b2) / 2.0\n",
    "                    alphaPairsChanged += 1\n",
    "            if (alphaPairsChanged == 0): \n",
    "                num_iter += 1\n",
    "            else: \n",
    "                num_iter = 0\n",
    "        return self.b, self.alphas, self.hat_alphas\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        predict the label of test samples\n",
    "        Args:\n",
    "            X_test(matrix, num_test*num_features): features of test samples\n",
    "        Returns:\n",
    "            y_hat(vector, num_test): the predicted ouput of test samples\n",
    "        '''\n",
    "        K_test = self.rbfkernel(self.X_train, X_test)\n",
    "        m = K_test.shape[0]\n",
    "        y_hat = K_test.T @ (self.hat_alphas - self.alphas) + self.b\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Run SVR on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "model_SVR = SVR(C = 0.5, sigma = 5, toler = 5, maxIter = 30)\n",
    "b, alphas, hat_alphas = model_SVR.fit(X_train, y_train)\n",
    "y_test_hat = model_SVR.predict(X_test)\n",
    "mse = mean_squared_error(y_test_hat, y_test)\n",
    "print(\"The MSE of SVR is:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
