{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVPUD dataset\n",
    "## Context\n",
    "## Regression Tree\n",
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from encodings import search_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import necessary tools from the sklearn library\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "# Import sklearn library tools used ONLY for validating my results\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score, recall_score, precision_score, auc, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import randint as sp_randint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_regression\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # read dataset from csv file\n",
    "    data_name = \"CVPUD_regression\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "    data_X, data_y = pd.DataFrame(data_X), pd.Series(data_y)\n",
    "    data_y = (data_y - data_y.mean()) / data_y.std()\n",
    "\n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.33, random_state=2200)\n",
    "    return train_X, test_X, train_y, test_y, data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject#</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>test_time</th>\n",
       "      <th>motor_UPDRS</th>\n",
       "      <th>Jitter(%)</th>\n",
       "      <th>Jitter(Abs)</th>\n",
       "      <th>Jitter:RAP</th>\n",
       "      <th>Jitter:PPQ5</th>\n",
       "      <th>Jitter:DDP</th>\n",
       "      <th>...</th>\n",
       "      <th>Shimmer:APQ3</th>\n",
       "      <th>Shimmer:APQ5</th>\n",
       "      <th>Shimmer:APQ11</th>\n",
       "      <th>Shimmer:DDA</th>\n",
       "      <th>NHR</th>\n",
       "      <th>HNR</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>PPE</th>\n",
       "      <th>total_UPDRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>28.199</td>\n",
       "      <td>0.00662</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.00401</td>\n",
       "      <td>0.00317</td>\n",
       "      <td>0.01204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01438</td>\n",
       "      <td>0.01309</td>\n",
       "      <td>0.01662</td>\n",
       "      <td>0.04314</td>\n",
       "      <td>0.014290</td>\n",
       "      <td>21.640</td>\n",
       "      <td>0.41888</td>\n",
       "      <td>0.54842</td>\n",
       "      <td>0.16006</td>\n",
       "      <td>34.398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>12.6660</td>\n",
       "      <td>28.447</td>\n",
       "      <td>0.00300</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>0.00150</td>\n",
       "      <td>0.00395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00994</td>\n",
       "      <td>0.01072</td>\n",
       "      <td>0.01689</td>\n",
       "      <td>0.02982</td>\n",
       "      <td>0.011112</td>\n",
       "      <td>27.183</td>\n",
       "      <td>0.43493</td>\n",
       "      <td>0.56477</td>\n",
       "      <td>0.10810</td>\n",
       "      <td>34.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>19.6810</td>\n",
       "      <td>28.695</td>\n",
       "      <td>0.00481</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.00205</td>\n",
       "      <td>0.00208</td>\n",
       "      <td>0.00616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00734</td>\n",
       "      <td>0.00844</td>\n",
       "      <td>0.01458</td>\n",
       "      <td>0.02202</td>\n",
       "      <td>0.020220</td>\n",
       "      <td>23.047</td>\n",
       "      <td>0.46222</td>\n",
       "      <td>0.54405</td>\n",
       "      <td>0.21014</td>\n",
       "      <td>35.389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6470</td>\n",
       "      <td>28.905</td>\n",
       "      <td>0.00528</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.00191</td>\n",
       "      <td>0.00264</td>\n",
       "      <td>0.00573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01106</td>\n",
       "      <td>0.01265</td>\n",
       "      <td>0.01963</td>\n",
       "      <td>0.03317</td>\n",
       "      <td>0.027837</td>\n",
       "      <td>24.445</td>\n",
       "      <td>0.48730</td>\n",
       "      <td>0.57794</td>\n",
       "      <td>0.33277</td>\n",
       "      <td>35.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6420</td>\n",
       "      <td>29.187</td>\n",
       "      <td>0.00335</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.00093</td>\n",
       "      <td>0.00130</td>\n",
       "      <td>0.00278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00679</td>\n",
       "      <td>0.00929</td>\n",
       "      <td>0.01819</td>\n",
       "      <td>0.02036</td>\n",
       "      <td>0.011625</td>\n",
       "      <td>26.126</td>\n",
       "      <td>0.47188</td>\n",
       "      <td>0.56122</td>\n",
       "      <td>0.19361</td>\n",
       "      <td>36.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject#  age  sex  test_time  motor_UPDRS  Jitter(%)  Jitter(Abs)  \\\n",
       "0         1   72    0     5.6431       28.199    0.00662     0.000034   \n",
       "1         1   72    0    12.6660       28.447    0.00300     0.000017   \n",
       "2         1   72    0    19.6810       28.695    0.00481     0.000025   \n",
       "3         1   72    0    25.6470       28.905    0.00528     0.000027   \n",
       "4         1   72    0    33.6420       29.187    0.00335     0.000020   \n",
       "\n",
       "   Jitter:RAP  Jitter:PPQ5  Jitter:DDP  ...  Shimmer:APQ3  Shimmer:APQ5  \\\n",
       "0     0.00401      0.00317     0.01204  ...       0.01438       0.01309   \n",
       "1     0.00132      0.00150     0.00395  ...       0.00994       0.01072   \n",
       "2     0.00205      0.00208     0.00616  ...       0.00734       0.00844   \n",
       "3     0.00191      0.00264     0.00573  ...       0.01106       0.01265   \n",
       "4     0.00093      0.00130     0.00278  ...       0.00679       0.00929   \n",
       "\n",
       "   Shimmer:APQ11  Shimmer:DDA       NHR     HNR     RPDE      DFA      PPE  \\\n",
       "0        0.01662      0.04314  0.014290  21.640  0.41888  0.54842  0.16006   \n",
       "1        0.01689      0.02982  0.011112  27.183  0.43493  0.56477  0.10810   \n",
       "2        0.01458      0.02202  0.020220  23.047  0.46222  0.54405  0.21014   \n",
       "3        0.01963      0.03317  0.027837  24.445  0.48730  0.57794  0.33277   \n",
       "4        0.01819      0.02036  0.011625  26.126  0.47188  0.56122  0.19361   \n",
       "\n",
       "   total_UPDRS  \n",
       "0       34.398  \n",
       "1       34.894  \n",
       "2       35.389  \n",
       "3       35.810  \n",
       "4       36.375  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am loading the full dataset and renaming the columns to keep better track of each attribute\n",
    "data_dir = \"..\\..\\..\\data\\data_regression\"\n",
    "data_path = os.path.join(data_dir, \"CVPUD_regression.csv\")\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Implement the class of regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### criterion function for regression tree\n",
    "def sum_squared_distance_to_mean(X, y):\n",
    "    '''\n",
    "    function used to calculate the squared error.\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    return np.var(y) * n\n",
    "\n",
    "class DecisionTreeRegressor(object):\n",
    "    '''\n",
    "    This class is for decision tree regression\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of decision tree regression\n",
    "        - tolS: a tolerance on the error reduction. we will stop the splitting if\n",
    "            low error reduction.\n",
    "        - tree: a nested dictionary representing the decision tree structure.\n",
    "    '''\n",
    "    def __init__(self, criterion, tolS=0.1):\n",
    "        # Initialization\n",
    "        self.criterion = criterion\n",
    "        self.tolS = tolS\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        function used to fit the decision tree regressor\n",
    "\n",
    "        Args:\n",
    "            X - features of training samples, a pandas dataframe with shape (n, d), where\n",
    "                X.columns is the column name of X, and we can use X['feat'] to index all\n",
    "                values of the feature named `feat`.\n",
    "            y - target values (continuous, scaled) of training samples, a pandas series with shape (n,)\n",
    "        '''\n",
    "        self.tree = self.create_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to fit the decision tree regressor\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "\n",
    "        Returns:\n",
    "            y - predictions of test samples, a pandas series with shape (n,)\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y = []\n",
    "        for i in range(n):\n",
    "            y.append(self.predict_each(X.iloc[i], self.tree))\n",
    "        y = pd.Series(y)\n",
    "        y.index = X.index\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def choose_best_split(X, y, criterion, tolS=1):\n",
    "        '''\n",
    "        function used to choose the best split feature and split point\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "            y - target values of training samples, a pandas series with shape (n,)\n",
    "            criterion - function used to measure the quality of a split\n",
    "            tolS - a tolerance on the error reduction.\n",
    "\n",
    "        Returns:\n",
    "            best_feat: the feature used to split on for this node\n",
    "            best_split: the value of the feature used to split for this node\n",
    "        '''\n",
    "        # used to calculate the error reduction\n",
    "        origin_score = criterion(X, y)\n",
    "        # initialize\n",
    "        best_feat, best_split = None, None\n",
    "        best_score = np.inf\n",
    "        # search for each feature\n",
    "        for feat in X.columns:\n",
    "            # if all values of this feature are equal, do not split this feature\n",
    "            X_feat_value = set(X[feat])\n",
    "            if len(X_feat_value) == 1:\n",
    "                continue\n",
    "            # otherwise, search for each possible split point of this feature\n",
    "            for split in X_feat_value:\n",
    "                # divide the dataset into two parts according to the split\n",
    "                idx1 = X[feat] < split\n",
    "                idx2 = X[feat] >= split\n",
    "                # calculate score to evaluate the quality of a split\n",
    "                score1 = criterion(X.loc[idx1], y.loc[idx1])\n",
    "                score2 = criterion(X.loc[idx2], y.loc[idx2])\n",
    "                score = score1 + score2\n",
    "                if score < best_score:\n",
    "                    # choose the split with the largest (variance) reduction\n",
    "                    best_feat = feat\n",
    "                    best_split = split\n",
    "                    best_score = score\n",
    "        if origin_score - best_score < tolS:\n",
    "            # Do not split if the (variance) reduction is low\n",
    "            return None, None\n",
    "        else:\n",
    "            # return the feature and the value used for the split\n",
    "            return best_feat, best_split\n",
    "\n",
    "    def create_tree(self, X, y):\n",
    "        '''\n",
    "        build the decision tree regressor in a recursive manner.\n",
    "        use a dictionary to represent tree node:\n",
    "            if the tree node is an internal node, the dictionary will have the following five items:\n",
    "                - tree[\"is_leaf\"] stores whether the tree node is a leaf node or not.\n",
    "                - tree[\"split_feat\"] stores the feature used to split on for this node\n",
    "                - tree[\"split_point\"] stores the value of the feature used to split\n",
    "                - tree[\"left\"] is a (nested) dictionary used to store the left subtree of this node.\n",
    "                - tree[\"right\"] is a (nested) dictionary used to store the right subtree of this node.\n",
    "            if the tree node is a leaf node, the dictionary will have the following two items:\n",
    "                - tree[\"is_leaf\"] stores whether the tree node is a leaf node or not.\n",
    "                - tree[\"value\"] stores the prediction at the leaf node\n",
    "        returns a nested dictionary used to store the tree structure.\n",
    "        '''\n",
    "        Tree = {}\n",
    "        # create a leaf node if all values are equal\n",
    "        y_values, y_counts = np.unique(y, return_counts=True)\n",
    "        if len(y_counts) == 1:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            Tree[\"value\"] = y_values[0]\n",
    "            return Tree\n",
    "        # create a leaf node if feature set is empty (a low error reduction)\n",
    "        feat, split = self.choose_best_split(X, y, self.criterion, self.tolS)\n",
    "        if feat is None:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            Tree[\"value\"] = y.mean()\n",
    "            return Tree\n",
    "        # otherwise, create an internal node\n",
    "        Tree[\"is_leaf\"] = False\n",
    "        Tree[\"split_feat\"] = feat\n",
    "        Tree[\"split_point\"] = split\n",
    "        # divide the dataset (X, y) into two parts according to tree split\n",
    "        # build the left subtree\n",
    "        idx = X[feat] < split\n",
    "        Tree[\"left\"] = self.create_tree(X.loc[idx], y.loc[idx])\n",
    "        # build the right subtree\n",
    "        idx = X[feat] >= split\n",
    "        Tree[\"right\"] = self.create_tree(X.loc[idx], y.loc[idx])\n",
    "        return Tree\n",
    "\n",
    "    @staticmethod\n",
    "    def predict_each(x, tree):\n",
    "        '''\n",
    "        for each sample, get the prediction of decision tree regressor in a recursive manner.\n",
    "\n",
    "        Args:\n",
    "            x - features of a sample, a pandas Series with shape (d,)\n",
    "            tree - a nested dictionary representing the decision tree structure.\n",
    "\n",
    "        Returns:\n",
    "            the prediction of the sample `x`\n",
    "        '''\n",
    "        if tree[\"is_leaf\"] is True:\n",
    "            # if the `tree` is a leaf node, get the prediction at the leaf node\n",
    "            return tree[\"value\"]\n",
    "        else:\n",
    "            # the 'tree' is a nested dictionary with the following four items:\n",
    "            #     `split_feat`, `split_point`, `left`, 'right`.\n",
    "            # get the feature used to split on for this tree\n",
    "            feat = tree[\"split_feat\"]\n",
    "            # get the value of the feature used to split\n",
    "            split = tree[\"split_point\"]\n",
    "            # get the value of the feature for the sample `x`\n",
    "            value = x[feat]\n",
    "            if value < split:\n",
    "                # search for the left subtree\n",
    "                return DecisionTreeRegressor.predict_each(x, tree[\"left\"])\n",
    "            else:\n",
    "                # search for the right subtree\n",
    "                return DecisionTreeRegressor.predict_each(x, tree[\"right\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # dataset preparation\n",
    "    X, y = main()[0], main()[2]\n",
    "    # initialize the decision tree regressor\n",
    "    model = DecisionTreeRegressor(criterion=sum_squared_distance_to_mean, tolS=0.1)\n",
    "    # fit the regression tree\n",
    "    model.fit(X, y)\n",
    "    # get the prediction of samples\n",
    "    y_hat = model.predict(main()[1])\n",
    "    # calculate the mean squared error of samples\n",
    "    print(\"The MSE of Random Regression Tree is:\", ((y - y_hat)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of Random Regression Tree is: nan\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suppor Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am loading the full dataset and renaming the columns to keep better track of each attribute\n",
    "data_dir = \"..\\..\\..\\data\\data_regression\"\n",
    "data_path = os.path.join(data_dir, \"CVPUD_regression.csv\")\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "df.head()\n",
    "df.shape\n",
    "X = np.array(df.iloc[:,:-1])[:2214,:]\n",
    "y = np.array(df.iloc[:,-1])[:2214]\n",
    "# normalize\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "# split the train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Implementation of SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVR():\n",
    "    '''\n",
    "    This is a class for support vector regressor.\n",
    "    \n",
    "    The class contains the hyper-parameters such as $C$, $\\epsilon$ and the kernel bandwidth $\\sigma$. It also contains \n",
    "    the alpha vector, the tolerance for prediction error and the maximum number of iteration.\n",
    "    \n",
    "    It contains the functions for calculating the kernel matrix, fitting the model to estimate alpha, hat_alpha and b \n",
    "    with SMO algorithm, making predictions and other fundamental functions.\n",
    "    \n",
    "    Attributes:\n",
    "        C (positive number)         - the hyperparameter for SVM algorithm\n",
    "        sigma (positive number)     - the kernel bandwidth $\\sigma$ of Gaussian kernel \n",
    "        toler (positive number)     - $\\epsilon$, the threshold value of prediction error. If the prediction error of \n",
    "                                      a sample is larger than this value, the corresponding alphas[i] or hat_alpha[i]\n",
    "                                      will be probably updated.\n",
    "        maxIter (positive integer)  - the maximum number of iteration to search a pair of alpha's to update\n",
    "        alphas (vector, num_samples) - the alpha vector in the dual problem \n",
    "        hat_alphas (vector, num_samples) - the hat_alpha vector in the dual problem \n",
    "        b (number)                  - the bias b\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, C = 1, sigma = 1, toler = 1, maxIter = 10):\n",
    "        self.C = C\n",
    "        self.sigma = sigma\n",
    "        self.toler = toler\n",
    "        self.maxIter = maxIter\n",
    "        self.alphas = 0\n",
    "        self.hat_alphas = 0\n",
    "        self.b = 0\n",
    "        \n",
    "    def rbfkernel(self, X, Y):\n",
    "        '''\n",
    "        Calculate the kernel matrix whose (i,j)-th entry is $k(X[i,:], Y[j,:])$.\n",
    "        '''\n",
    "        m = X.shape[0]\n",
    "        n = Y.shape[0]\n",
    "        K = np.zeros(shape=(m, n))\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                K[i, j] = np.sum((X[i,:] - Y[j,:])**2)\n",
    "        K = np.exp(-K / (2 * self.sigma**2))\n",
    "        return K\n",
    "\n",
    "    def selectJrand(self, i, m):\n",
    "        '''\n",
    "        Randomly choose an index $j\\neq i$ from 0 to m-1 and choose alpha[i] (or hat_alpha[i]) if hat_alpha[i] = 0 (or alpha[i]=0).\n",
    "        '''\n",
    "        j = i\n",
    "        while (j == i):\n",
    "            j = int(np.random.uniform(0, m))\n",
    "        if self.alphas[j] == 0:\n",
    "            choose_hat_j = 1\n",
    "        else:\n",
    "            choose_hat_j = 0\n",
    "        return (j, choose_hat_j)\n",
    "\n",
    "\n",
    "    def clipAlpha(self, aj, H, L):\n",
    "        '''\n",
    "        Clip the vale aj by the lower bound L and upper bound H\n",
    "        '''\n",
    "        if aj > H:\n",
    "            aj = H\n",
    "        if L > aj:\n",
    "            aj = L\n",
    "        return aj\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        estimate the alphas vector, hat_alpha vector and bias in the SVM model\n",
    "        \n",
    "        Args: \n",
    "            X_train (matrix, num_train*num_features): features of training samples\n",
    "            y_train (vector, num_train): output of training samples\n",
    "            \n",
    "        Returns:\n",
    "            self.b (a number)                     : the bias\n",
    "            self.alphas (vector, num_samples)     : the alpha vector \n",
    "            self.hat_alphas (vector, num_samples) : the hat_alpha vector \n",
    "        ''' \n",
    "        K_train = self.rbfkernel(X_train, X_train)\n",
    "        self.X_train = X_train\n",
    "        m = K_train.shape[0]\n",
    "        self.alphas = np.zeros((m, ))\n",
    "        self.hat_alphas = np.zeros((m, ))\n",
    "        num_iter = 0\n",
    "        while (num_iter < self.maxIter):\n",
    "            alphaPairsChanged = 0\n",
    "            # optimize for each data vector (with kernel trick)\n",
    "            for i in range(m):  \n",
    "                ### choose hat_alphas[i] or alphas[i] to update\n",
    "                if self.hat_alphas[i] == 0:\n",
    "                    choose_hat_i = 0\n",
    "                else:\n",
    "                    choose_hat_i = 1\n",
    "                fXi = (self.hat_alphas - self.alphas) @ K_train[i, :] + self.b\n",
    "                Ei = fXi - y_train[i]\n",
    "                ### if statement checks if an example violates KKT conditions\n",
    "                if (abs(Ei) < self.toler) or (self.alphas[i] == self.C) or (self.hat_alphas[i] == self.C):\n",
    "                    continue\n",
    "                else:\n",
    "                    j, choose_hat_j = self.selectJrand(i, m)\n",
    "                    fXj = (self.hat_alphas-self.alphas) @ K_train[j, :] + self.b\n",
    "                    Ej = fXj - y_train[j]\n",
    "                    alphaJold = self.alphas[j]\n",
    "                    alphaIold = self.alphas[i]\n",
    "                    hat_alphaJold = self.hat_alphas[j]\n",
    "                    hat_alphaIold = self.hat_alphas[i]\n",
    "                    if (choose_hat_i == 0) and (choose_hat_j == 1):\n",
    "                        L = max(0, self.alphas[i] - self.hat_alphas[j])\n",
    "                        H = min(self.C, self.C + self.alphas[i] - self.hat_alphas[j])\n",
    "                    elif (choose_hat_i == 1) and (choose_hat_j == 0):\n",
    "                        L = max(0, self.hat_alphas[i] - self.alphas[j])\n",
    "                        H = min(self.C, self.C + self.hat_alphas[i] - self.alphas[j])\n",
    "                    elif (choose_hat_i == 0) and (choose_hat_j == 0):\n",
    "                        L = max(0, self.alphas[i] + self.alphas[j] - self.C)\n",
    "                        H = min(self.C, self.alphas[i] + self.alphas[j])\n",
    "                    else:\n",
    "                        L = max(0, self.hat_alphas[i] + self.hat_alphas[j] - self.C)\n",
    "                        H = min(self.C, self.hat_alphas[i] + self.hat_alphas[j])\n",
    "                    if L == H:\n",
    "                        continue\n",
    "                    eta = 2.0 * K_train[j, i] - K_train[j, j] - K_train[i, i]\n",
    "                    if eta >= 0:\n",
    "                        continue\n",
    "                    ### Update i \n",
    "                    if (choose_hat_i == 0) and (choose_hat_j == 1):\n",
    "                        self.alphas[i] += (2 * self.toler - Ei + Ej) / eta\n",
    "                    elif (choose_hat_i == 1) and (choose_hat_j == 0):\n",
    "                        self.hat_alphas[i] += (2 * self.toler + Ei - Ej) / eta\n",
    "                    elif (choose_hat_i == 0) and (choose_hat_j == 0):\n",
    "                        self.alphas[i] += (-Ei + Ej) / eta\n",
    "                    else:\n",
    "                        self.hat_alphas[i] += (Ei - Ej) / eta\n",
    "                    if choose_hat_i == 0:\n",
    "                        self.alphas[i] = self.clipAlpha(self.alphas[i], H, L)\n",
    "                    else:\n",
    "                        self.hat_alphas[i] = self.clipAlpha(self.hat_alphas[i], H, L)\n",
    "                    ### Whether the change amount is large enough or not\n",
    "                    if choose_hat_i == 0:\n",
    "                        if (abs(self.alphas[i] - alphaIold) < 0.00001):\n",
    "                            continue\n",
    "                    else:\n",
    "                        if (abs(self.hat_alphas[i] - hat_alphaIold) < 0.00001):\n",
    "                            continue\n",
    "                    # update j by the same amount as i\n",
    "                    if (choose_hat_i == 0) and (choose_hat_j == 1):\n",
    "                        self.hat_alphas[j] += self.alphas[i] - alphaIold\n",
    "                    elif (choose_hat_i == 1) and (choose_hat_j == 0):\n",
    "                        self.alphas[j] += self.hat_alphas[i] - hat_alphaIold\n",
    "                    elif (choose_hat_i == 0) and (choose_hat_j == 0):\n",
    "                        self.alphas[j] += alphaIold - self.alphas[i]\n",
    "                    else:\n",
    "                        self.hat_alphas[j] += hat_alphaIold - self.hat_alphas[i]\n",
    "                    # update b\n",
    "                    b1 = self.b - Ej - (self.hat_alphas[j] - hat_alphaJold - (self.alphas[j] - alphaJold))    \\\n",
    "                        * K_train[j, j] - (self.hat_alphas[i] - hat_alphaIold - (self.alphas[i] - alphaIold)) * K_train[i, j]\n",
    "                    b2 = self.b - Ei - (self.hat_alphas[j] - hat_alphaJold - (self.alphas[j] - alphaJold))   \\\n",
    "                        * K_train[j, i] - (self.hat_alphas[i] - hat_alphaIold - (self.alphas[i] - alphaIold)) * K_train[i, i]\n",
    "                    if ((0 < self.alphas[j]) and (self.C > self.alphas[j])) or ((0 < self.hat_alphas[j]) and (self.C > self.hat_alphas[j])): \n",
    "                        self.b = b1\n",
    "                    elif ((0 < self.alphas[i]) and (self.C > self.alphas[i])) or ((0 < self.hat_alphas[i]) and (self.C > self.hat_alphas[i])): \n",
    "                        self.b = b2\n",
    "                    else: self.b = (b1 + b2) / 2.0\n",
    "                    alphaPairsChanged += 1\n",
    "            if (alphaPairsChanged == 0): \n",
    "                num_iter += 1\n",
    "            else: \n",
    "                num_iter = 0\n",
    "        return self.b, self.alphas, self.hat_alphas\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        predict the label of test samples\n",
    "        Args:\n",
    "            X_test(matrix, num_test*num_features): features of test samples\n",
    "        Returns:\n",
    "            y_hat(vector, num_test): the predicted ouput of test samples\n",
    "        '''\n",
    "        K_test = self.rbfkernel(self.X_train, X_test)\n",
    "        m = K_test.shape[0]\n",
    "        y_hat = K_test.T @ (self.hat_alphas - self.alphas) + self.b\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Run SVR on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of SVR is: 7796709.366960324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "model_SVR = SVR(C = 0.5, sigma = 5, toler = 5, maxIter = 30)\n",
    "b, alphas, hat_alphas = model_SVR.fit(X_train, y_train)\n",
    "y_test_hat = model_SVR.predict(X_test)\n",
    "mse = mean_squared_error(y_test_hat, y_test)\n",
    "print(\"The MSE of SVR is:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
