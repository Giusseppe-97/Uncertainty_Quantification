{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regression Tree\n",
    "# 1.1 Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age       sex       bmi        bp        s1        s2        s3  \\\n",
      "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
      "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
      "2  0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
      "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
      "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
      "\n",
      "         s4        s5        s6  \n",
      "0 -0.002592  0.019908 -0.017646  \n",
      "1 -0.039493 -0.068330 -0.092204  \n",
      "2 -0.002592  0.002864 -0.025930  \n",
      "3  0.034309  0.022692 -0.009362  \n",
      "4 -0.002592 -0.031991 -0.046641  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "def load_dataset():\n",
    "    '''\n",
    "    function used for dataset preparation\n",
    "\n",
    "    Returns:\n",
    "        X - features of training samples, a pandas dataframe with shape (n, d), where\n",
    "            X.columns is the column name of X, and we can use X['feat'] to index all\n",
    "            values of the feature named `feat`.\n",
    "        y - target values (continuous, scaled) of training samples, a pandas series with shape (n,)\n",
    "    '''\n",
    "    X, y = load_diabetes(return_X_y=True)\n",
    "    X, y = pd.DataFrame(X), pd.Series(y)\n",
    "    X.columns = ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n",
    "    y = (y - y.mean()) / y.std()\n",
    "    return X, y\n",
    "X, y = load_dataset()\n",
    "print(X.iloc[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Implement the class of regression tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### criterion function for regression tree\n",
    "def sum_squared_distance_to_mean(X, y):\n",
    "    '''\n",
    "    function used to calculate the squared error.\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    return np.var(y) * n\n",
    "\n",
    "class DecisionTreeRegressor(object):\n",
    "    '''\n",
    "    This class is for decision tree regression\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of decision tree regression\n",
    "        - tolS: a tolerance on the error reduction. we will stop the splitting if\n",
    "            low error reduction.\n",
    "        - tree: a nested dictionary representing the decision tree structure.\n",
    "    '''\n",
    "    def __init__(self, criterion, tolS=0.1):\n",
    "        # Initialization\n",
    "        self.criterion = criterion\n",
    "        self.tolS = tolS\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        function used to fit the decision tree regressor\n",
    "\n",
    "        Args:\n",
    "            X - features of training samples, a pandas dataframe with shape (n, d), where\n",
    "                X.columns is the column name of X, and we can use X['feat'] to index all\n",
    "                values of the feature named `feat`.\n",
    "            y - target values (continuous, scaled) of training samples, a pandas series with shape (n,)\n",
    "        '''\n",
    "        self.tree = self.create_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to fit the decision tree regressor\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "\n",
    "        Returns:\n",
    "            y - predictions of test samples, a pandas series with shape (n,)\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y = []\n",
    "        for i in range(n):\n",
    "            y.append(self.predict_each(X.iloc[i], self.tree))\n",
    "        y = pd.Series(y)\n",
    "        y.index = X.index\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def choose_best_split(X, y, criterion, tolS=1):\n",
    "        '''\n",
    "        function used to choose the best split feature and split point\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "            y - target values of training samples, a pandas series with shape (n,)\n",
    "            criterion - function used to measure the quality of a split\n",
    "            tolS - a tolerance on the error reduction.\n",
    "\n",
    "        Returns:\n",
    "            best_feat: the feature used to split on for this node\n",
    "            best_split: the value of the feature used to split for this node\n",
    "        '''\n",
    "        # used to calculate the error reduction\n",
    "        origin_score = criterion(X, y)\n",
    "        # initialize\n",
    "        best_feat, best_split = None, None\n",
    "        best_score = np.inf\n",
    "        # search for each feature\n",
    "        for feat in X.columns:\n",
    "            # if all values of this feature are equal, do not split this feature\n",
    "            X_feat_value = set(X[feat])\n",
    "            if len(X_feat_value) == 1:\n",
    "                continue\n",
    "            # otherwise, search for each possible split point of this feature\n",
    "            for split in X_feat_value:\n",
    "                # divide the dataset into two parts according to the split\n",
    "                idx1 = X[feat] < split\n",
    "                idx2 = X[feat] >= split\n",
    "                # calculate score to evaluate the quality of a split\n",
    "                score1 = criterion(X.loc[idx1], y.loc[idx1])\n",
    "                score2 = criterion(X.loc[idx2], y.loc[idx2])\n",
    "                score = score1 + score2\n",
    "                if score < best_score:\n",
    "                    # choose the split with the largest (variance) reduction\n",
    "                    best_feat = feat\n",
    "                    best_split = split\n",
    "                    best_score = score\n",
    "        if origin_score - best_score < tolS:\n",
    "            # Do not split if the (variance) reduction is low\n",
    "            return None, None\n",
    "        else:\n",
    "            # return the feature and the value used for the split\n",
    "            return best_feat, best_split\n",
    "\n",
    "    def create_tree(self, X, y):\n",
    "        '''\n",
    "        build the decision tree regressor in a recursive manner.\n",
    "        use a dictionary to represent tree node:\n",
    "            if the tree node is an internal node, the dictionary will have the following five items:\n",
    "                - tree[\"is_leaf\"] stores whether the tree node is a leaf node or not.\n",
    "                - tree[\"split_feat\"] stores the feature used to split on for this node\n",
    "                - tree[\"split_point\"] stores the value of the feature used to split\n",
    "                - tree[\"left\"] is a (nested) dictionary used to store the left subtree of this node.\n",
    "                - tree[\"right\"] is a (nested) dictionary used to store the right subtree of this node.\n",
    "            if the tree node is a leaf node, the dictionary will have the following two items:\n",
    "                - tree[\"is_leaf\"] stores whether the tree node is a leaf node or not.\n",
    "                - tree[\"value\"] stores the prediction at the leaf node\n",
    "        returns a nested dictionary used to store the tree structure.\n",
    "        '''\n",
    "        Tree = {}\n",
    "        # create a leaf node if all values are equal\n",
    "        y_values, y_counts = np.unique(y, return_counts=True)\n",
    "        if len(y_counts) == 1:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            Tree[\"value\"] = y_values[0]\n",
    "            return Tree\n",
    "        # create a leaf node if feature set is empty (a low error reduction)\n",
    "        feat, split = self.choose_best_split(X, y, self.criterion, self.tolS)\n",
    "        if feat is None:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            Tree[\"value\"] = y.mean()\n",
    "            return Tree\n",
    "        # otherwise, create an internal node\n",
    "        Tree[\"is_leaf\"] = False\n",
    "        Tree[\"split_feat\"] = feat\n",
    "        Tree[\"split_point\"] = split\n",
    "        # divide the dataset (X, y) into two parts according to tree split\n",
    "        # build the left subtree\n",
    "        idx = X[feat] < split\n",
    "        Tree[\"left\"] = self.create_tree(X.loc[idx], y.loc[idx])\n",
    "        # build the right subtree\n",
    "        idx = X[feat] >= split\n",
    "        Tree[\"right\"] = self.create_tree(X.loc[idx], y.loc[idx])\n",
    "        return Tree\n",
    "\n",
    "    @staticmethod\n",
    "    def predict_each(x, tree):\n",
    "        '''\n",
    "        for each sample, get the prediction of decision tree regressor in a recursive manner.\n",
    "\n",
    "        Args:\n",
    "            x - features of a sample, a pandas Series with shape (d,)\n",
    "            tree - a nested dictionary representing the decision tree structure.\n",
    "\n",
    "        Returns:\n",
    "            the prediction of the sample `x`\n",
    "        '''\n",
    "        if tree[\"is_leaf\"] is True:\n",
    "            # if the `tree` is a leaf node, get the prediction at the leaf node\n",
    "            return tree[\"value\"]\n",
    "        else:\n",
    "            # the 'tree' is a nested dictionary with the following four items:\n",
    "            #     `split_feat`, `split_point`, `left`, 'right`.\n",
    "            # get the feature used to split on for this tree\n",
    "            feat = tree[\"split_feat\"]\n",
    "            # get the value of the feature used to split\n",
    "            split = tree[\"split_point\"]\n",
    "            # get the value of the feature for the sample `x`\n",
    "            value = x[feat]\n",
    "            if value < split:\n",
    "                # search for the left subtree\n",
    "                return DecisionTreeRegressor.predict_each(x, tree[\"left\"])\n",
    "            else:\n",
    "                # search for the right subtree\n",
    "                return DecisionTreeRegressor.predict_each(x, tree[\"right\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of Random Regression Tree is: 0.011446887551729325\n"
     ]
    }
   ],
   "source": [
    "# dataset preparation\n",
    "X, y = load_dataset()\n",
    "# initialize the decision tree regressor\n",
    "model = DecisionTreeRegressor(criterion=sum_squared_distance_to_mean, tolS=0.1)\n",
    "# fit the regression tree\n",
    "model.fit(X, y)\n",
    "# get the prediction of samples\n",
    "y_hat = model.predict(X)\n",
    "# calculate the mean squared error of samples\n",
    "print(\"The MSE of Random Regression Tree is:\", ((y - y_hat)**2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification Tree\n",
    "## 2.1 Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    color            root    sound          texture        umbilicus surface\n",
      "0   green           curly  muffled            clear           hollow    hard\n",
      "1    dark           curly     dull            clear           hollow    hard\n",
      "2    dark           curly  muffled            clear           hollow    hard\n",
      "3   green           curly     dull            clear           hollow    hard\n",
      "4   light           curly  muffled            clear           hollow    hard\n",
      "5   green  slightly curly  muffled            clear  slightly hollow    soft\n",
      "6    dark  slightly curly  muffled  slightly blurry  slightly hollow    soft\n",
      "7    dark  slightly curly  muffled            clear  slightly hollow    hard\n",
      "8    dark  slightly curly     dull  slightly blurry  slightly hollow    hard\n",
      "9   green        straight    crisp            clear             flat    soft\n",
      "10  light        straight    crisp           blurry             flat    hard\n",
      "11  light           curly  muffled           blurry             flat    soft\n",
      "12  green  slightly curly  muffled  slightly blurry           hollow    hard\n",
      "13  light  slightly curly     dull  slightly blurry           hollow    hard\n",
      "14   dark  slightly curly  muffled            clear  slightly hollow    soft\n",
      "15  light           curly  muffled           blurry             flat    hard\n",
      "16  green           curly     dull  slightly blurry  slightly hollow    hard\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    '''\n",
    "    function used for dataset preparation\n",
    "\n",
    "    Returns:\n",
    "        X - features of training samples, a pandas dataframe with shape (n, d), where\n",
    "            X.columns is the column name of X, and we can use X['feat'] to index all\n",
    "            values of the feature named `feat`. All features are discrete.\n",
    "        y - labels of training samples, a pandas series with shape (n,)\n",
    "    '''\n",
    "    data = [[\"green\", \"curly\", \"muffled\", \"clear\", \"hollow\", \"hard\", \"true\"],\n",
    "            [\"dark\", \"curly\", \"dull\", \"clear\", \"hollow\", \"hard\", \"true\"],\n",
    "            [\"dark\", \"curly\", \"muffled\", \"clear\", \"hollow\", \"hard\", \"true\"],\n",
    "            [\"green\", \"curly\", \"dull\", \"clear\", \"hollow\", \"hard\", \"true\"],\n",
    "            [\"light\", \"curly\", \"muffled\", \"clear\", \"hollow\", \"hard\", \"true\"],\n",
    "            [\n",
    "                \"green\", \"slightly curly\", \"muffled\", \"clear\",\n",
    "                \"slightly hollow\", \"soft\", \"true\"\n",
    "            ],\n",
    "            [\n",
    "                \"dark\", \"slightly curly\", \"muffled\", \"slightly blurry\",\n",
    "                \"slightly hollow\", \"soft\", \"true\"\n",
    "            ],\n",
    "            [\n",
    "                \"dark\", \"slightly curly\", \"muffled\", \"clear\",\n",
    "                \"slightly hollow\", \"hard\", \"true\"\n",
    "            ],\n",
    "            [\n",
    "                \"dark\", \"slightly curly\", \"dull\", \"slightly blurry\",\n",
    "                \"slightly hollow\", \"hard\", \"false\"\n",
    "            ],\n",
    "            [\"green\", \"straight\", \"crisp\", \"clear\", \"flat\", \"soft\", \"false\"],\n",
    "            [\"light\", \"straight\", \"crisp\", \"blurry\", \"flat\", \"hard\", \"false\"],\n",
    "            [\"light\", \"curly\", \"muffled\", \"blurry\", \"flat\", \"soft\", \"false\"],\n",
    "            [\n",
    "                \"green\", \"slightly curly\", \"muffled\", \"slightly blurry\",\n",
    "                \"hollow\", \"hard\", \"false\"\n",
    "            ],\n",
    "            [\n",
    "                \"light\", \"slightly curly\", \"dull\", \"slightly blurry\", \"hollow\",\n",
    "                \"hard\", \"false\"\n",
    "            ],\n",
    "            [\n",
    "                \"dark\", \"slightly curly\", \"muffled\", \"clear\",\n",
    "                \"slightly hollow\", \"soft\", \"false\"\n",
    "            ],\n",
    "            [\"light\", \"curly\", \"muffled\", \"blurry\", \"flat\", \"hard\", \"false\"],\n",
    "            [\n",
    "                \"green\", \"curly\", \"dull\", \"slightly blurry\", \"slightly hollow\",\n",
    "                \"hard\", \"false\"\n",
    "            ]]\n",
    "    data = pd.DataFrame(data)\n",
    "    data.columns = [\n",
    "        \"color\", \"root\", \"sound\", \"texture\", \"umbilicus\", \"surface\", \"ripe\"\n",
    "    ]\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1]\n",
    "    return X, y\n",
    "X, y = load_dataset()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Criteria\n",
    "\n",
    "In the following functions, we define the entropy, information gain, gain_ratio, gini and gini index.\n",
    "\n",
    "### 2.2.1 Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(X, y):\n",
    "    ### calculate the entropy Ent(D) where D = (X, y)\n",
    "    n = X.shape[0]\n",
    "    pk = np.unique(y, return_counts=True)[1] / n\n",
    "    return np.sum(-pk * np.log2(pk))\n",
    "\n",
    "\n",
    "def information_gain(X, y, feat_values, feat):\n",
    "    ### calculate the information gain as the guideline for splitting the data set $D = (X, y)$ with feature $feat$.\n",
    "    n = X.shape[0]\n",
    "    S = entropy(X, y)\n",
    "    values = feat_values[feat]\n",
    "    new_S = 0\n",
    "    for value in values:\n",
    "        idx = X[feat] == value\n",
    "        nv = idx.sum()\n",
    "        if nv > 0:\n",
    "            new_S += nv / n * entropy(X.loc[idx], y.loc[idx])\n",
    "        else:\n",
    "            # In the calculation of entropy, plog_2(p) = 0 when p = 0.\n",
    "            new_S += 0\n",
    "    return S - new_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Gain Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_ratio(X, y, feat_values, feat):\n",
    "    n = X.shape[0]\n",
    "    S = entropy(X, y)\n",
    "    values = feat_values[feat]\n",
    "    new_S = 0\n",
    "    iv = 0\n",
    "    for value in values:\n",
    "        idx = X[feat] == value\n",
    "        nv = idx.sum()\n",
    "        if nv > 0:\n",
    "            # ignore when nv = 0\n",
    "            new_S += nv / n * entropy(X.loc[idx], y.loc[idx])\n",
    "            iv += -(nv / n) * np.log2(nv / n)\n",
    "    return (S - new_S) / iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(X, y):\n",
    "    n = X.shape[0]\n",
    "    pk = np.unique(y, return_counts=True)[1] / n\n",
    "    return 1 - np.sum(pk**2)\n",
    "\n",
    "\n",
    "def gini_index(X, y, feat_values, feat):\n",
    "    n = X.shape[0]\n",
    "    new_S = 0\n",
    "    values = feat_values[feat]\n",
    "    for value in values:\n",
    "        idx = X[feat] == value\n",
    "        nv = idx.sum()\n",
    "        if nv > 0:\n",
    "            new_S += nv / n * gini(X.loc[idx], y.loc[idx])\n",
    "    return new_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implement the class of classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(object):\n",
    "    '''\n",
    "    This class is for classification tree\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of classification tree\n",
    "        - tree: a nested dictionary representing the decision tree structure.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, criterion):\n",
    "        # Initialization\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n, d = X.shape\n",
    "        # for each discrete feature, store all possibie values\n",
    "        self.feat_values = {}\n",
    "        for feat in X.columns:\n",
    "            self.feat_values[feat] = X[feat].unique()\n",
    "        # build the decision tree\n",
    "        self.tree = self.create_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to fit the decision tree classifier\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "\n",
    "        Returns:\n",
    "            y - predictions of test samples, a pandas series with shape (n,)\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y = []\n",
    "        for i in range(n):\n",
    "            y.append(self.predict_each(X.iloc[i], self.tree))\n",
    "        y = pd.Series(y)\n",
    "        y.index = X.index\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def choose_best_split(X, y, feat_values, criterion):\n",
    "        '''\n",
    "        function used to choose the best split feature\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "            y - target values of training samples, a pandas series with shape (n,)\n",
    "            feat_values - a dict to store all possibie values of each discrete feature\n",
    "            criterion - function used to measure the quality of a split\n",
    "\n",
    "        Returns:\n",
    "            best_feat: the feature used to split on for this node\n",
    "        '''\n",
    "        # initialization\n",
    "        best_feat = None\n",
    "        best_score = np.inf if criterion == gini_index else -np.inf\n",
    "        # search for each feature\n",
    "        for feat in X.columns:\n",
    "            # if all values of this feature are equal, do not split this feature\n",
    "            if len(X[feat].unique()) == 1:\n",
    "                continue\n",
    "            # otherwise, measure the quality of the split of this discrete feature\n",
    "            score = criterion(X, y, feat_values, feat)\n",
    "            if criterion == gini_index:\n",
    "                # select the feature with the lowest Gini index as the splitting feature\n",
    "                if score < best_score:\n",
    "                    best_feat = feat\n",
    "                    best_score = score\n",
    "            else:\n",
    "                # select the feature with the highest information gain / gain ratio\n",
    "                if score > best_score:\n",
    "                    best_feat = feat\n",
    "                    best_score = score\n",
    "        return best_feat\n",
    "\n",
    "    def create_tree(self, X, y):\n",
    "        '''\n",
    "        build the decision tree classifier in a recursive manner.\n",
    "        use a dictionary to represent tree node:\n",
    "            if the tree node is an internal node, the dictionary will have the following items:\n",
    "                - tree[\"is_leaf\"] stores whether the tree node is a leaf node or not.\n",
    "                - tree[\"split_feat\"] stores the feature used to split on for this node\n",
    "                - tree[\"feat\"] is a (nested) dictionary used to store the subtree of this node\n",
    "                    (i.e., tree[\"split_feat\"] == \"feat\").\n",
    "            if the tree node is a leaf node, the dictionary will have the following two items:\n",
    "                - tree[\"is_leaf\"] stores whether the tree node is a leaf node or not.\n",
    "                - tree[\"value\"] stores the prediction at the leaf node\n",
    "        returns a nested dictionary used to store the tree structure.\n",
    "        '''\n",
    "        tree = {}\n",
    "        # create a leaf node if all samples belong to the same class\n",
    "        classes, class_counts = np.unique(y, return_counts=True)\n",
    "        if len(classes) == 1:\n",
    "            tree[\"is_leaf\"] = True\n",
    "            tree[\"value\"] = classes[0]\n",
    "            return tree\n",
    "        # create a leaf node if feature set is empty (i.e., all feature values are the same).\n",
    "        feat = self.choose_best_split(X, y, self.feat_values, self.criterion)\n",
    "        if feat is None:\n",
    "            tree[\"is_leaf\"] = True\n",
    "            # using the majority vote to get the prediction at the leaf node\n",
    "            majority_class = classes[np.argmax(class_counts)]\n",
    "            tree[\"value\"] = majority_class\n",
    "            return tree\n",
    "        # otherwise, create an internal node and store the optimal splitting feature `feat`\n",
    "        tree[\"is_leaf\"] = False\n",
    "        tree[\"split_feat\"] = feat\n",
    "        values = self.feat_values[feat]\n",
    "        # divide the dataset (X, y) into multiple parts according to tree split\n",
    "        # search for each possible value of the splitting feature, and build child nodes.\n",
    "        for value in values:\n",
    "            # calculate the subset of samples whose value of splitting feature is `value`\n",
    "            idx = X[feat] == value\n",
    "            if idx.sum() > 0:\n",
    "                # if there exists sample whose value of splitting feature is `value`,\n",
    "                # continue to build the subtree.\n",
    "                tree[value] = self.create_tree(X.loc[idx], y.loc[idx])\n",
    "            else:\n",
    "                # Otherwise, mark the child node as a leaf node,\n",
    "                # and label the child node with the majority class in this tree node.\n",
    "                majority_class = classes[np.argmax(class_counts)]\n",
    "                tree[value] = {\"is_leaf\": True, \"value\": majority_class}\n",
    "        return tree\n",
    "\n",
    "    @staticmethod\n",
    "    def predict_each(x, tree):\n",
    "        '''\n",
    "        for each sample, get the prediction of decision tree classifier in a recursive manner.\n",
    "\n",
    "        Args:\n",
    "            x - features of a sample, a pandas Series with shape (d,)\n",
    "            tree - a nested dictionary representing the decision tree structure.\n",
    "\n",
    "        Returns:\n",
    "            the prediction of the sample `x`\n",
    "        '''\n",
    "        if tree[\"is_leaf\"] is True:\n",
    "            # if the `tree` is a leaf node, get the prediction at the leaf node\n",
    "            return tree[\"value\"]\n",
    "        else:\n",
    "            # the 'tree' is a nested dictionary\n",
    "            # get the value of the feature used to split\n",
    "            feat = tree[\"split_feat\"]\n",
    "            # get the value of the feature for the sample `x`\n",
    "            value = x[feat]\n",
    "            # search for the corresponding subtree\n",
    "            return DecisionTreeClassifier.predict_each(x, tree[value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of classfication tree with the information gain is: 1.0\n",
      "The accuracy of classfication tree with the gain ratio is: 1.0\n",
      "The accuracy of classfication tree with the gini index is: 1.0\n"
     ]
    }
   ],
   "source": [
    "X, y = load_dataset()\n",
    "\n",
    "# initialize the decision tree with different criterion\n",
    "model = DecisionTreeClassifier(criterion=information_gain)\n",
    "model.fit(X, y)\n",
    "y_hat = model.predict(X)\n",
    "print(\"The accuracy of classfication tree with the information gain is:\", (y == y_hat).mean())\n",
    "\n",
    "model = DecisionTreeClassifier(criterion=gain_ratio)\n",
    "model.fit(X, y)\n",
    "y_hat = model.predict(X)\n",
    "print(\"The accuracy of classfication tree with the gain ratio is:\", (y == y_hat).mean())\n",
    "\n",
    "model = DecisionTreeClassifier(criterion=gini_index)\n",
    "model.fit(X, y)\n",
    "y_hat = model.predict(X)\n",
    "print(\"The accuracy of classfication tree with the gini index is:\", (y == y_hat).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
