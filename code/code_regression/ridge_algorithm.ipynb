{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I aim to implement the ridge regression algorithm including the functions to fit the model with training data and predict the output for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression():\n",
    "    '''\n",
    "    This is a class for ridge regression algorithm.\n",
    "    \n",
    "    The class contains the hyper parameters of the ridge regression algorithm as attributes, such as the regurization \n",
    "    parameter(Lambda).\n",
    "    It also contains the functions for initializing the class, calculating the loss, fitting the ridge regression \n",
    "    model and use the fitted model to predict test samples.\n",
    "    \n",
    "    Attributes:\n",
    "        lr:        learning rate of gradient descent\n",
    "        Lambda:    regularization parameter for L_2 penalty\n",
    "        max_itr:   maximum number of iteration for gradient descent\n",
    "        tol:       if the change in loss is smaller than tol, then we stop iteration\n",
    "        W:         concatenation of weight w and bias b\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, lr, Lambda, max_itr, tol):\n",
    "        '''\n",
    "        Initialize the RidgeRegression class\n",
    "        '''\n",
    "        self.lr = lr\n",
    "        self.Lambda = Lambda\n",
    "        self.max_itr = max_itr\n",
    "        self.tol = tol\n",
    "        \n",
    "    def _loss_ridge(self, X, y, W):\n",
    "        '''\n",
    "        Calculating the regularized empirical loss\n",
    "        '''\n",
    "        return ((y-X@W).T@(y-X@W))[0,0] + self.Lambda * np.sum(W[:X.shape[1]-1,0]**2)\n",
    "    \n",
    "    \n",
    "    def fit(self,x,y):  \n",
    "        '''\n",
    "        estimate the weight and bias in the ridge regression model by gradient descent\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_train*num_variables): input of training samples\n",
    "            y (matrix, num_test*1): output of training samples\n",
    "            \n",
    "        Returns:\n",
    "            self.W (matrix, (num_variables+1)*1): estimation of weight w and bias b\n",
    "        ''' \n",
    "        m = x.shape[0]\n",
    "        ### Add the all-one vector to the last column \n",
    "        X = np.concatenate((x,np.ones((m,1))),axis=1)\n",
    "        d = X.shape[1]\n",
    "        self.W = np.ones((d,1))\n",
    "        \n",
    "        ### Use the gradient descent to update W\n",
    "        previous_loss = self._loss_ridge(X, y, self.W)\n",
    "        for i in range(self.max_itr):\n",
    "            L_2_der_W = np.zeros((d,1))\n",
    "            L_2_der_W[:d,0] = self.W[:d,0]\n",
    "            gradient = X.T@(X@self.W-y)/m + self.Lambda * L_2_der_W\n",
    "            self.W = self.W - self.lr * gradient\n",
    "            current_loss = self._loss_ridge(X, y, self.W)\n",
    "            if previous_loss - current_loss < self.tol:\n",
    "                print(f'Converged after {i} iterations')\n",
    "                break\n",
    "            else:\n",
    "                previous_loss = current_loss\n",
    "        return self.W\n",
    "    \n",
    "    def predict(self,x): \n",
    "        '''\n",
    "        predict the output of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted outputs of test samples\n",
    "        ''' \n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x, np.ones((m,1))),axis=1)\n",
    "        return np.dot(X, self.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the class \"RidgeRegression\" class to fit, predict and evaluate the ridge regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 9928 iterations\n",
      "MSE of Ridge Regression is 0.010401064104879443\n",
      "The weight w of Ridge Regression is \n",
      " [ 1.00703274e+00  2.95993559e+00  9.41954206e-04 -5.41866994e-03\n",
      "  1.90221845e-03  4.70667448e-04].\n",
      "The bias b of Ridge Regression is 1.9876498728411585.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "### Initial the class RidgeRegression by assigning values to the parameters.\n",
    "model = RidgeRegression(lr=0.01, Lambda=0.002, max_itr = 20000, tol = 1e-5)\n",
    "### Fit model with training data\n",
    "W = model.fit(X_train, y_train)\n",
    "### Predict the output of test samples\n",
    "y_pred = model.predict(X_test)\n",
    "### Evaluate the model by calculating the MSE of test samples.\n",
    "mse = mean_squared_error(y_pred, y_test)\n",
    "### Print MSE \n",
    "print(\"MSE of Ridge Regression is {}\".format(mse))\n",
    "### Print the estimated w and b\n",
    "print(\"The weight w of Ridge Regression is \\n {}.\".format(W[:X_test.shape[1],0].T))\n",
    "print(\"The bias b of Ridge Regression is {}.\".format(W[X_test.shape[1],0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
