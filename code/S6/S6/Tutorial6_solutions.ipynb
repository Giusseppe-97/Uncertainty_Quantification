{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "def loadDataSet():\n",
    "    train = pd.read_table(\"horseColicTraining.txt\", header=None)\n",
    "    X_train, y_train = train.iloc[:, :-1], train.iloc[:, -1]\n",
    "    test = pd.read_table(\"horseColicTest.txt\", header=None)\n",
    "    X_test, y_test = test.iloc[:, :-1], test.iloc[:, -1]\n",
    "    X_train = np.array(X_train, dtype=np.float64)\n",
    "    X_test = np.array(X_test, dtype=np.float64)\n",
    "    y_train = np.array(y_train, dtype=np.int32)\n",
    "    y_test = np.array(y_test, dtype=np.int32)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Criterion -- Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_counts(y, sample_weight, classes_):\n",
    "    '''\n",
    "    the function used to calculate the summation of weights of samples from each class. Generally speaking,\n",
    "    the weights are all set as one. But for Adaboost, each sample has different values.\n",
    "    '''\n",
    "    class_counts = np.zeros(shape=classes_.shape[0], dtype=np.float64)\n",
    "    for i, label in enumerate(classes_):\n",
    "        idx = y == label\n",
    "        if idx.sum() > 0:\n",
    "            class_counts[i] = sample_weight[idx].sum()\n",
    "        else:\n",
    "            class_counts[i] = 0\n",
    "    return class_counts\n",
    "\n",
    "def gini(y, sample_weight):\n",
    "    classes_ = np.unique(y)\n",
    "    class_counts = calculate_weighted_counts(y, sample_weight, classes_)\n",
    "    if class_counts.sum() > 0:\n",
    "        pk = class_counts / class_counts.sum()\n",
    "        pk = pk[pk > 0]\n",
    "        return 1 - np.sum(pk**2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def gini_index(X, y, feat, point, sample_weight):\n",
    "    '''\n",
    "    calculate the difference of gini index before and after splitting\n",
    "    '''\n",
    "    S = gini(y, sample_weight)\n",
    "    new_S = 0\n",
    "    n = sample_weight.sum()\n",
    "    assert n > 0\n",
    "    idx1 = X[:, feat] < point\n",
    "    nv = sample_weight[idx1].sum()\n",
    "    if nv > 0:\n",
    "        new_S += nv / n * gini(y[idx1], sample_weight[idx1])\n",
    "    idx2 = X[:, feat] >= point\n",
    "    nv = sample_weight[idx2].sum()\n",
    "    if nv > 0:\n",
    "        new_S += nv / n * gini(y[idx2], sample_weight[idx2])\n",
    "    return S - new_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Trees for Classification\n",
    "\n",
    "Different from the classification tree implemented in the last tutorial:\n",
    "1. Each internal node has two child nodes regardless of values of the splitting feature are continuous or discrete.\n",
    "2. Add the parameter `max_depth` for providing another condition to stop splitting procedures.\n",
    "3. Add the parameter `max_features` to use the subset of features to build decision tree.\n",
    "\n",
    "Options 2&3 are designed for constructing trees in the random forest implemented in Section 5. \n",
    "\n",
    "If you'd like to build the classification tree, you can ignore options 2&3 by setting `max_depth = None` and `max_features = None`. Then we can combine it with the pre-pruning or post-pruning technique implemented in Section 4 to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(object):\n",
    "    '''\n",
    "    This class is for classification tree\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of classification tree\n",
    "        - tree: a nested dictionary representing the decision tree structure.\n",
    "        - max_depth: the parameter to control the depth of tree. If the depth is larger than max_depth, we will stop splitting.\n",
    "        - max_feature: the number of selected features to build decision tree\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 criterion=gini_index,\n",
    "                 max_depth=None,\n",
    "                 max_features=None,\n",
    "                 random_seed=None):\n",
    "        self.f_criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        if self.max_depth is None:\n",
    "            self.max_depth = 2**10\n",
    "        self.max_features = max_features\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        np.random.seed(self.random_seed)\n",
    "        num_samples, num_features = X.shape\n",
    "        if self.max_features is None:\n",
    "            self.max_features = num_features\n",
    "        elif self.max_features == \"sqrt\":\n",
    "            self.max_features = np.int(np.round(np.sqrt(num_features)))\n",
    "        self.classes_ = np.unique(y)\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(num_samples, dtype=np.float64)\n",
    "        # build the decision tree\n",
    "        self.tree = self.create_tree(X, y, sample_weight, depth=0)\n",
    "\n",
    "    def create_tree(self, X, y, sample_weight, depth):\n",
    "        Tree = {}\n",
    "        Tree[\"depth\"] = depth\n",
    "        class_counts = calculate_weighted_counts(y, sample_weight, self.classes_)\n",
    "        # create a leaf node if all samples belong to the same class\n",
    "        if (class_counts != 0).sum() == 1:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            Tree[\"pred\"] = self.classes_[class_counts != 0]\n",
    "        # using the majority vote to get the prediction at each node\n",
    "        majority_class = self.classes_[np.argmax(class_counts)]\n",
    "        Tree[\"pred\"] = majority_class\n",
    "        # create a leaf node if feature set is empty\n",
    "        feat, point = self.choose_best_split(X, y, sample_weight)\n",
    "        if feat is None or depth == self.max_depth:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            return Tree\n",
    "        # otherwise, create an internal node\n",
    "        Tree[\"is_leaf\"] = False\n",
    "        Tree[\"split_feat\"] = feat\n",
    "        Tree[\"split_point\"] = point\n",
    "        # build the left subtree\n",
    "        idx = X[:, feat] < point\n",
    "        Tree[\"left\"] = self.create_tree(X[idx], y[idx], sample_weight[idx],\n",
    "                                        depth + 1)\n",
    "        # build the right subtree\n",
    "        idx = X[:, feat] >= point\n",
    "        Tree[\"right\"] = self.create_tree(X[idx], y[idx], sample_weight[idx],\n",
    "                                         depth + 1)\n",
    "        return Tree\n",
    "\n",
    "    def choose_best_split(self, X, y, sample_weight):\n",
    "        # initialization\n",
    "        best_feat, best_point = None, None\n",
    "        best_score = 0.0\n",
    "        # search for each candidate feature\n",
    "        num_features = X.shape[1]\n",
    "        if self.max_features < num_features:\n",
    "            candidate_feat = np.random.permutation(\n",
    "                num_features)[:self.max_features]\n",
    "        else:\n",
    "            candidate_feat = np.arange(num_features)\n",
    "        for feat in candidate_feat:\n",
    "            # if all values of this feature are equal, do not split this feature\n",
    "            X_feat_value = np.unique(X[:, feat])\n",
    "            if len(X_feat_value) == 1:\n",
    "                continue\n",
    "            # search for each possible split point\n",
    "            for i in range(len(X_feat_value) - 1):\n",
    "                # divide the dataset into two parts according to the split\n",
    "                point = (X_feat_value[i] + X_feat_value[i + 1]) / 2.0\n",
    "                # calculate score to evaluate the quality of a split\n",
    "                score = self.f_criterion(X, y, feat, point, sample_weight)\n",
    "                if score > best_score:\n",
    "                    best_feat = feat\n",
    "                    best_point = point\n",
    "                    best_score = score\n",
    "        return best_feat, best_point\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to fit the decision tree classifier\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "\n",
    "        Returns:\n",
    "            y - predictions of test samples, a pandas series with shape (n,)\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y = []\n",
    "        for i in range(n):\n",
    "            y.append(DecisionTreeClassifier.predict_each(X[i], self.tree))\n",
    "        y = np.array(y, dtype=np.int32)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def predict_each(x, tree):\n",
    "        '''\n",
    "        for each sample, get the prediction of decision tree classifier in a recursive manner.\n",
    "\n",
    "        Args:\n",
    "            x - features of a sample, a pandas Series with shape (d,)\n",
    "            tree - a nested dictionary representing the decision tree structure.\n",
    "\n",
    "        Returns:\n",
    "            the prediction of the sample `x`\n",
    "        '''\n",
    "        if tree[\"is_leaf\"] is True:\n",
    "            # if the `tree` is a leaf node, get the prediction at the leaf node\n",
    "            return tree[\"pred\"]\n",
    "        else:\n",
    "            # the 'tree' is a nested dictionary\n",
    "            # get the value of the feature used to split\n",
    "            feat = tree[\"split_feat\"]\n",
    "            point = tree[\"split_point\"]\n",
    "            # get the value of the feature for the sample `x`\n",
    "            value = x[feat]\n",
    "            if value < point:\n",
    "                return DecisionTreeClassifier.predict_each(x, tree[\"left\"])\n",
    "            else:\n",
    "                return DecisionTreeClassifier.predict_each(x, tree[\"right\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training data is: 0.9966555183946488\n",
      "The accuracy of test data is: 0.6268656716417911\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = loadDataSet()\n",
    "model = DecisionTreeClassifier(criterion=gini_index,\n",
    "                                   max_depth=None,\n",
    "                                   max_features=None,\n",
    "                                   random_seed=None)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"The accuracy of training data is:\", acc_train)\n",
    "print(\"The accuracy of test data is:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You maybe find that the accuracy of training data is almost one but the accuracy of test data is low. The reason is that the decision tree overfits to the training data. To prevent overfitting, we introduce the post-pruning technique in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification Tree with the Pruning Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning(tree, classes_, X_valid, y_valid):\n",
    "    '''\n",
    "    the function used to post-prune the decision tee\n",
    "\n",
    "    Args:\n",
    "        tree - a nested dictionary representing the decision tree structure.\n",
    "        classes_ - names of all classes \n",
    "        X_valid - the features of the validation samples\n",
    "        y_valid - the labels of the validation samples\n",
    "    Returns:\n",
    "        the tree structure after pruning\n",
    "   '''\n",
    "    if X_valid.shape[0] == 0:\n",
    "        new_tree = {}\n",
    "        new_tree[\"is_leaf\"] = True\n",
    "        new_tree[\"pred\"] = tree[\"pred\"]\n",
    "        return new_tree\n",
    "    if tree[\"is_leaf\"] is True:\n",
    "        return tree\n",
    "    feat = tree[\"split_feat\"]\n",
    "    point = tree[\"split_point\"]\n",
    "    idx1 = X_valid[:, feat] < point\n",
    "    tree[\"left\"] = pruning(tree[\"left\"], classes_, X_valid[idx1],\n",
    "                           y_valid[idx1])\n",
    "    idx2 = X_valid[:, feat] >= point\n",
    "    tree[\"right\"] = pruning(tree[\"right\"], classes_, X_valid[idx2],\n",
    "                            y_valid[idx2])\n",
    "    if tree[\"left\"][\"is_leaf\"] is True and tree[\"right\"][\"is_leaf\"] is True:\n",
    "        FLAG = True\n",
    "    else:\n",
    "        FLAG = False\n",
    "    if FLAG:\n",
    "        # check validation accuracy gap\n",
    "        valid_y_true = []\n",
    "        valid_y_pred = []\n",
    "        # make prediction and calculate validation accuracy of the tree before merging\n",
    "        child_majority_class = tree[\"left\"][\"pred\"]\n",
    "        idx1 = X_valid[:, feat] < point\n",
    "        if idx1.sum() > 0:\n",
    "            valid_y_true.append(y_valid[idx1])\n",
    "            valid_y_pred.append([child_majority_class] * idx1.sum())\n",
    "        child_majority_class = tree[\"right\"][\"pred\"]\n",
    "        idx2 = X_valid[:, feat] >= point\n",
    "        if idx2.sum() > 0:\n",
    "            valid_y_true.append(y_valid[idx2])\n",
    "            valid_y_pred.append([child_majority_class] * idx2.sum())\n",
    "        valid_y_true = np.concatenate(valid_y_true)\n",
    "        valid_y_pred = np.concatenate(valid_y_pred)\n",
    "        valid_acc_before = np.mean(valid_y_true == valid_y_pred)\n",
    "        # make prediction and calculate validation accuracy of the tree after merging\n",
    "        majority_class = tree[\"pred\"]\n",
    "        valid_y_pred = np.array([majority_class] * X_valid.shape[0])\n",
    "        valid_acc_after = np.mean(valid_y_true == valid_y_pred)\n",
    "        # if the validation accuracy after merging is larger, we will prune\n",
    "        if valid_acc_after > valid_acc_before:\n",
    "            new_tree = {}\n",
    "            new_tree[\"is_leaf\"] = True\n",
    "            new_tree[\"pred\"] = tree[\"pred\"]\n",
    "            return new_tree\n",
    "        else:\n",
    "            return tree\n",
    "    else:\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of the classification tree Without pruning is: 1.0\n",
      "Validation accuracy of the classification tree Without pruning is: 0.6111111111111112\n",
      "Testing accuracy of the classification tree Without pruning is: 0.6716417910447762 \n",
      "\n",
      "Training accuracy of the classification tree With pruning is: 0.9521531100478469\n",
      "Validation accuracy of the classification tree With pruning is: 0.6777777777777778\n",
      "Testing accuracy of the classification tree With pruning is: 0.7164179104477612\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = loadDataSet()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train,\n",
    "                                                      y_train,\n",
    "                                                      test_size=0.3,\n",
    "                                                      stratify=y_train,\n",
    "                                                      random_state=3147)\n",
    "model = DecisionTreeClassifier(criterion=gini_index,\n",
    "                                   max_depth=None,\n",
    "                                   max_features=None,\n",
    "                                   random_seed=None)\n",
    "model.fit(X_train, y_train)\n",
    "# without pruning\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_valid_hat = model.predict(X_valid)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_valid = (y_valid == y_valid_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"Training accuracy of the classification tree Without pruning is:\", acc_train)\n",
    "print(\"Validation accuracy of the classification tree Without pruning is:\", acc_valid)\n",
    "print(\"Testing accuracy of the classification tree Without pruning is:\", acc_test, '\\n')\n",
    "# with pruning\n",
    "model.tree = pruning(model.tree, model.classes_, X_valid, y_valid)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_valid_hat = model.predict(X_valid)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_valid = (y_valid == y_valid_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"Training accuracy of the classification tree With pruning is:\", acc_train)\n",
    "print(\"Validation accuracy of the classification tree With pruning is:\", acc_valid)\n",
    "print(\"Testing accuracy of the classification tree With pruning is:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Random Forest for Classification\n",
    "\n",
    "In this section, we implement the random forest where each tree is built with the class DecisionTreeClassifier(). In our model, the values of parameters are listed below. \n",
    "1. The number of trees $T$ is set as ``num_estimators = 20``\n",
    "2. the number of subsampled features for each tree is $k =\\sqrt{d}$, which corresponds to ``max_features = \"sqrt\"`` in the code.\n",
    "3. The maximum depth of each tree is ``max_depth = 6``.\n",
    "\n",
    "We will not use the pruning technique for each tree in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier(object):\n",
    "    '''\n",
    "    This class is for random forest classification\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of classification tree\n",
    "        - num_estimators: the number of trees in the random forest \n",
    "        - tree: a nested dictionary representing the decision tree structure\n",
    "        - max_depth: the parameter to control the depth of tree. If the depth is larger than max_depth, we will stop splitting.\n",
    "        - max_feature: the number of selected features to build decision tree\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 num_estimators,\n",
    "                 random_state,\n",
    "                 criterion=gini_index,\n",
    "                 max_depth=None,\n",
    "                 max_features=\"sqrt\"):\n",
    "        self.num_estimators = num_estimators\n",
    "        self.random_state = random_state\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        function used to fit all trees in the random forest\n",
    "        \n",
    "        Args:\n",
    "            X - the features of the training samples\n",
    "            y - the labels of the training samples\n",
    "        Returns:\n",
    "            self.model_list - the model list containing `num_estimators` tree models\n",
    "        '''\n",
    "        n, d = X.shape\n",
    "        RandomState = np.random.RandomState(self.random_state)\n",
    "        self.model_list = []\n",
    "        for t in range(self.num_estimators):\n",
    "            random_seed = RandomState.randint(0, np.iinfo(np.int32).max)\n",
    "            ### draw a bootstrapped dataset from X\n",
    "            sample_index = RandomState.choice(np.arange(n), size=n, replace=True)\n",
    "            X_sampled = X[sample_index, :]\n",
    "            y_sampled = y[sample_index]\n",
    "            ### initialize the tree model by using DecisionTreeClassifier()\n",
    "            model = DecisionTreeClassifier(criterion=self.criterion,\n",
    "                                           max_depth=self.max_depth,\n",
    "                                           max_features=self.max_features,\n",
    "                                           random_seed=random_seed)\n",
    "            ### fit the tree model to the bootstrapped dataset by using DecisionTreeClassifier.fit()\n",
    "            model.fit(X_sampled, y_sampled)\n",
    "            self.model_list.append(model)\n",
    "        return self.model_list\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to predict the labels of X\n",
    "        \n",
    "        Args:\n",
    "            X - the features of the test samples\n",
    "        Returns:\n",
    "            y_pred_label - the predicted labels of test samples\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y_pred = np.zeros([self.num_estimators, n], dtype=np.int32)\n",
    "        y_pred_label = np.zeros(n, dtype=np.int32)\n",
    "        ### use T tree classifiers to make predictions by using DecisionTreeClassifier.predict()\n",
    "        for i in range(self.num_estimators):\n",
    "            model_i = self.model_list[i]\n",
    "            y_pred[i, :] = model_i.predict(X)\n",
    "        ### take the majority vote \n",
    "        for i in range(n):\n",
    "            classes, count = np.unique(y_pred[:, i], return_counts=True)\n",
    "            y_pred_label[i] = classes[np.argmax(count)]\n",
    "        return y_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of the random forest is: 0.9431438127090301\n",
      "Testing accuracy of the random forest is: 0.8059701492537313\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = loadDataSet()\n",
    "model = RandomForestClassifier(num_estimators=20,\n",
    "                                   random_state=101,\n",
    "                                   criterion=gini_index,\n",
    "                                   max_depth=6)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"Training accuracy of the random forest is:\", acc_train)\n",
    "print(\"Testing accuracy of the random forest is:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Adaboost\n",
    "\n",
    "In the Adaboost, we use the same parameter values as in random forest, that is, ``num_estimators = 20`` and ``max_depth = 6``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost(object):\n",
    "    '''\n",
    "    This class is for random forest classification\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of classification tree\n",
    "        - num_estimators: the number of iterations in the Adaboost\n",
    "        - tree: a nested dictionary representing the decision tree structure\n",
    "        - max_depth: the parameter to control the depth of tree. If the depth is larger than max_depth, we will stop splitting.\n",
    "        - model weight : a vector to store the weights of each model\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 num_estimators,\n",
    "                 criterion=gini_index,\n",
    "                 max_depth=None):\n",
    "        self.num_estimators = num_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        the function used to fit the decision tree, calculate the model $\\alpha_t$ and update the sample weights in \n",
    "        each iteration.\n",
    "        \n",
    "        Args:\n",
    "            X - the features of the training samples\n",
    "            y - the labels of the training samples\n",
    "        Returns:\n",
    "            self.model_list - the model list containing `num_estimators` tree models\n",
    "        '''\n",
    "        n, d = X.shape\n",
    "        sample_weight = np.ones(n) / n\n",
    "        self.model_weight = np.ones(self.num_estimators)\n",
    "        self.model_list = []\n",
    "        for t in range(self.num_estimators):\n",
    "            ### initialize the tree model by using DecisionTreeClassifier()\n",
    "            model_t = DecisionTreeClassifier(criterion=self.criterion,\n",
    "                                             max_depth=self.max_depth)\n",
    "            ### fit the tree model to the weighted samples by using DecisionTreeClassifier.fit()\n",
    "            model_t.fit(X, y, sample_weight=sample_weight)\n",
    "            ### make predictions by using DecisionTreeClassifier.predict()\n",
    "            y_pred_t = model_t.predict(X)\n",
    "            ### add the fitted model to the \"model_list\"\n",
    "            self.model_list.append(model_t)\n",
    "            ### calculate the weighted misclassification error $\\epsilon_t$\n",
    "            mis_classify_flag = np.where(y != y_pred_t)[0]\n",
    "            epsilon_t = np.sum(sample_weight[mis_classify_flag]) / np.sum(sample_weight)\n",
    "            ### calculate the model weight $\\alpha_t$\n",
    "            self.model_weight[t] = 0.5 * np.log(1 / epsilon_t - 1)\n",
    "            ### update the sample weight w_i\n",
    "            mis_classify_vec = np.zeros(n)\n",
    "            mis_classify_vec[mis_classify_flag] = 1\n",
    "            sample_weight = sample_weight * np.exp(self.model_weight[t] * mis_classify_vec)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to predict the labels of X\n",
    "        \n",
    "        Args:\n",
    "            X - the features of the test samples\n",
    "        Returns:\n",
    "            y_pred_label - the predicted labels of test samples\n",
    "        '''\n",
    "        y_pred = np.zeros([self.num_estimators, X.shape[0]])\n",
    "        for t in range(self.num_estimators):\n",
    "            model = self.model_list[t]\n",
    "            y_pred_temp = model.predict(X)\n",
    "            y_pred_temp = y_pred_temp * self.model_weight[t]\n",
    "            y_pred[t, :] = y_pred_temp\n",
    "        y_pred_res = np.sum(y_pred, axis=0)\n",
    "        y_pred_label = -np.ones(X.shape[0])\n",
    "        y_pred_label[np.where(y_pred_res > 0)[0]] = 1\n",
    "        return y_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of Adaboost is: 0.9966555183946488\n",
      "Testing accuracy of Adaboost is: 0.7761194029850746\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = loadDataSet()\n",
    "y_train[np.where(y_train == 0)[0]] = -1\n",
    "y_test[np.where(y_test == 0)[0]] = -1\n",
    "model = Adaboost(num_estimators=20, criterion=gini_index, max_depth=6)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"Training accuracy of Adaboost is:\", acc_train)\n",
    "print(\"Testing accuracy of Adaboost is:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "4165d9adae938a85345a555bbdca328977ee674e0c3469f8b91cfe3e114ee9d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
