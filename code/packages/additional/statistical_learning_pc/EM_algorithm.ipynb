{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EM algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate samples from the mixture Gaussian Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    " \n",
    "def generate_data(num_samples, alpha, mu_list, sigma_list):    \n",
    "    '''\n",
    "    Generate tbe synthetic dataset from the mixture-of-Gaussian distribution\n",
    "    Args:\n",
    "        num_samples (positive integer)                  : number of samples\n",
    "        alpha  (vector, num_classes)                    : prior probability vetor\n",
    "        mu_list (list of length num_classes)            : the $i$-th element is the mean vector of $i$-th class\n",
    "        sigma_list (list of length num_classes)         : the $i$-th element is the covariance matrix of $i$-th class\n",
    "    Returns:\n",
    "        X (matrix, num_samples * num_features)        : generated data\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    num_components = len(mu_list)\n",
    "    num_features = len(mu_list[0])\n",
    "    X = np.zeros((num_samples, num_features))       \n",
    "    # Generate random numbers in [0,1]\n",
    "    random_numbers = np.random.random(num_samples)\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_components):\n",
    "            if random_numbers[i] < sum(alpha[:j+1]):  \n",
    "                X[i,:]  = np.random.multivariate_normal(mu_list[j], sigma_list[j], 1) \n",
    "                break\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM_for_MG():\n",
    "    '''\n",
    "    This class is for using EM algorithm to estimate the parameters of mixture-of-Gaussian distribution.\n",
    "    \n",
    "    The class contains the parameters of EM iteration, including the number of classes $N$, the prior probabilities \n",
    "    $\\alpha_i$, the mean vectors $\\mu_i$ and covariance matrix $\\Sigma_i$ for each class $i$.\n",
    "    \n",
    "    It also contains the functions for initializing the class, updating parameters in E-step and M-step, iterate over \n",
    "    the two steps until convergence.\n",
    "    \n",
    "    Attributes:\n",
    "        num_classes (positive integer)           : the number of Gaussian components\n",
    "        hat_alpha (list of length num_classes)   : the prior probability of each component\n",
    "        hat_mu (list of length num_classes)      : the mean vector of each Gaussian component\n",
    "        hat_sigma (list of length num_classes)   : the covariance matrix of each Gaussian component\n",
    "        posterior_prob (matrix, num_samples * num_classes) : the posterior probability matrix and the $(j,i)$-th entry\n",
    "            represents the posterior probability that the sample X[j,:] is from the $i$-th Gaussian component.\n",
    "                                                   \n",
    "        \n",
    "    '''\n",
    "    def __init__(self, num_classes=2, num_iteration=1000):\n",
    "        '''\n",
    "        Initialize the class for using EM algorithm to estimate the parameters in the Mixture-of-Gaussian model. \n",
    "        '''\n",
    "        self.num_classes = num_classes\n",
    "        self.num_iteration = num_iteration\n",
    "        self.hat_alpha = []\n",
    "        self.hat_mu = []\n",
    "        self.hat_sigma = []\n",
    "        self.posterior_prob = 0\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \n",
    "        self.num_samples, self.num_features = X.shape\n",
    "        ### Initialize parameters\n",
    "        self.hat_alpha = [1/self.num_classes] * self.num_classes\n",
    "        self.hat_mu = [np.min(X,axis=0) + (ell+1) / (self.num_classes+1) * (np.max(X, axis=0) - np.min(X, axis=0)) for ell in range(self.num_classes)]\n",
    "        self.hat_sigma = [np.eye(self.num_features)*np.std(X,axis=0)] * self.num_classes\n",
    "        ### Iteration begins\n",
    "        previous_alpha = self.hat_alpha\n",
    "        previous_mu = self.hat_mu\n",
    "        for t in range(self.num_iteration):   \n",
    "            ### E-step: Update posterior probability $\\gamma_{ji}$\n",
    "            self.E_step(X)    \n",
    "            ### M-step: Update parameters $alpha, mu, sigma$\n",
    "            self.M_step(X)    \n",
    "            ### Judge whether the parameter estimations converge or not\n",
    "            err_mu = np.mean(np.abs(np.array(previous_mu)-np.array(self.hat_mu)))     \n",
    "            err_alpha = np.mean(np.abs(previous_alpha)-np.abs(self.hat_alpha))\n",
    "            if (err_mu <= 0.001) and (err_alpha < 0.001):     \n",
    "                print('Converged after {} iterations'.format(t+1))\n",
    "                break\n",
    "            else:\n",
    "                previous_mu = self.hat_mu\n",
    "                previous_alpha = self.hat_alpha\n",
    "            ### print the result every 20 iterations\n",
    "            if (t % 20 == 0):\n",
    "                print('The number of iterations is:', t+1)\n",
    "                print(\"The estimated mean vectors are:\",self.hat_mu)\n",
    "                print(\"The estimated prior probablilities are:\",self.hat_alpha)\n",
    "        return self.hat_alpha, self.hat_mu, self.hat_sigma\n",
    "        \n",
    "        \n",
    "    def E_step(self, X):\n",
    "        '''\n",
    "        Calculate the posterior probablilty $\\gamma_{ji}$ for each class $i$.\n",
    "        '''\n",
    "        self.posterior_prob = np.zeros((self.num_samples, self.num_classes))\n",
    "        for j in range(self.num_samples):\n",
    "            denom = 0\n",
    "            for i in range(self.num_classes):\n",
    "                denom += self.hat_alpha[i] * np.exp(-(X[j,:]-self.hat_mu[i]).reshape(1,-1)@np.linalg.inv(self.hat_sigma[i])@(X[j,:]-self.hat_mu[i]).reshape(-1,1)/2)[0,0]/np.sqrt(np.linalg.det(self.hat_sigma[i]))\n",
    "            for i in range(self.num_classes):\n",
    "                numer = np.exp(-(X[j,:]-self.hat_mu[i]).reshape(1,-1)@np.linalg.inv(self.hat_sigma[i])@(X[j,:]-self.hat_mu[i]).reshape(-1,1)/2)[0,0]/np.sqrt(np.linalg.det(self.hat_sigma[i]))   \n",
    "                self.posterior_prob[j,i] = self.hat_alpha[i]*numer/denom      \n",
    "\n",
    "    \n",
    "    def M_step(self, X):\n",
    "        '''\n",
    "        Update the parameters $\\alpha_i$, $\\mu_i$ and $\\Sigma_i$\n",
    "        '''\n",
    "        num_features = np.shape(X)[1]\n",
    "        self.hat_mu, self.hat_alpha, self.hat_sigma = [], [], []\n",
    "        for i in range(self.num_classes):\n",
    "            denom=0   \n",
    "            numer=0   \n",
    "            for j in range(self.num_samples):\n",
    "                numer += self.posterior_prob[j,i]*X[j,:]\n",
    "                denom += self.posterior_prob[j,i]\n",
    "            self.hat_mu.append(numer/denom)    \n",
    "            self.hat_alpha.append(denom/self.num_samples)     \n",
    "        for i in range(self.num_classes):\n",
    "            cov_matrix = np.zeros((self.num_features,self.num_features))\n",
    "            for j in range(self.num_samples):\n",
    "                cov_matrix += self.posterior_prob[j,i] * np.dot((X[j,:] - self.hat_mu[i]).reshape(-1,1),(X[j,:] - self.hat_mu[i]).reshape(1,-1))\n",
    "            self.hat_sigma.append(cov_matrix/np.sum(self.posterior_prob[:,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of iterations is: 1\n",
      "The estimated mean vectors are: [array([5.54740373, 6.39989141]), array([14.97425697, 16.1360408 ]), array([26.04041944, 21.09775304]), array([45.14208248, 30.00284033])]\n",
      "The estimated prior probablilities are: [0.14781162761653793, 0.23755184520641426, 0.2139882530246035, 0.4006482741524441]\n",
      "The number of iterations is: 21\n",
      "The estimated mean vectors are: [array([4.86511192, 4.48628506]), array([10.04122609, 14.78472996]), array([24.72561463, 19.92091405]), array([45.08480489, 29.91543767])]\n",
      "The estimated prior probablilities are: [0.10966891235083012, 0.18915836036094022, 0.29517846619104515, 0.4059942610971841]\n",
      "Converged after 36 iterations\n",
      "The mean vectors converge to: [array([4.71381662, 4.2074898 ]), array([ 9.96878205, 14.62347069]), array([24.72846409, 19.9208832 ]), array([45.08480369, 29.91543702])]\n",
      "The prior probablilities converge to: [0.10374668320673734, 0.19519354885398635, 0.2950654422147265, 0.4059943257245499]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000         \n",
    "num_components = 4            \n",
    "alpha = [0.1,0.2,0.3,0.4]  \n",
    "mu1 = [5,5]\n",
    "mu2 = [10,15]\n",
    "mu3 = [25,20]\n",
    "mu4 = [45,30]\n",
    "mu_list = [mu1, mu2, mu3, mu4]\n",
    "sigma_list = [np.array([[10, 0], [0, 10]])]*4\n",
    "dataset = generate_data(num_samples, alpha, mu_list, sigma_list) \n",
    "num_iteration = 1000\n",
    "model = EM_for_MG(num_components, num_iteration)\n",
    "hat_alpha, hat_mu, hat_sigma = model.fit(dataset)\n",
    "print(\"The mean vectors converge to:\", hat_mu)\n",
    "print(\"The prior probablilities converge to:\", hat_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
