{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Naive Bayes\n",
    "\n",
    "We are going to classify sentences with Naive Bayes algorithm. We have some abusive and not abusive sentences in the form of word lists. Our task is to predict whether a new sentence is abusive or not.\n",
    "\n",
    "\n",
    "## 1.1 Prepare: transform sentences into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    ### Each sentence appears in the form of word list.\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],       \n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    ### Labels of sentences. 1 is abusive, 0 not\n",
    "    classVec = [0,1,0,1,0,1]\n",
    "    return postingList,classVec\n",
    "\n",
    "# create a list of the unique words in all sentences.\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])    #Create an empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #Create the union of two sets\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary list is:\n",
      " ['steak', 'is', 'quit', 'to', 'mr', 'problems', 'love', 'ate', 'help', 'cute', 'I', 'licks', 'posting', 'garbage', 'take', 'not', 'park', 'so', 'food', 'worthless', 'stop', 'dog', 'maybe', 'how', 'please', 'flea', 'has', 'my', 'dalmation', 'him', 'stupid', 'buying']\n"
     ]
    }
   ],
   "source": [
    "postingList, classVec = loadDataSet()\n",
    "myVocabList = createVocabList(postingList)\n",
    "print('The vocabulary list is:\\n', myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    '''\n",
    "    According to vocabulary list (vocabList), we convert a word vector (inputSet) to a vector of 1s and 0s of the \n",
    "    same length as the vocabulary list. \n",
    "    The $i$-th element of output vector represents whether the $i$-th word in our vocabulary list is present or not in \n",
    "    the word vector.\n",
    "    \n",
    "    Args:\n",
    "        vocabList - a vocabulary list\n",
    "        inputSet - a word list\n",
    "    Returns:\n",
    "        returnVec - a vector of 1s and 0s of the same length as the vocabulary list\n",
    "    '''\n",
    "    returnVec = [0] * len(vocabList)                               #Create a vector of all 0s\n",
    "    for word in inputSet:                                          \n",
    "        if word in vocabList:                                      #If the word is in the vocabulary list，then we set its value to 1 in the output vector.\n",
    "            returnVec[vocabList.index(word)] += 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 0-1 vector of the first sentence is: [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "trainMat = []\n",
    "for postinDoc in postingList:\n",
    "    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "print('The 0-1 vector of the first sentence is:', trainMat[0])\n",
    "print(len(trainMat[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Implement the class of Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class NaiveBayes():\n",
    "    '''\n",
    "    This is a class for Naive Bayes classification.\n",
    "    \n",
    "    The class contains the arrays of conditional probabilities for abusive class and not abusive class.\n",
    "    \n",
    "    It also contains the functions for initializing the class, fitting the Naive Bayes classifier model and use \n",
    "    the fitted model to predict test samples.\n",
    "    \n",
    "    Attributes:\n",
    "        p0Vect (vector, num_sentences)   - array of conditional probabilities for abusive class\n",
    "        p1Vect (vector, num_sentences) - array of conditional probabilities for not abusive class\n",
    "        pAbusive (number in [0,1])  - the probability that the document belongs to the abusive class\n",
    "        \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.p1Vect = 0\n",
    "        self.p0Vect = 0\n",
    "        self.pAbusive = 0\n",
    "        \n",
    "    def fit(self, trainMatrix, classVec):\n",
    "        '''\n",
    "        fit the naive Bayes classifier to the training data. To be specific, we calculate the class-conditional \n",
    "        probability $p(x_j|c)$ and $p(c)$.\n",
    "\n",
    "        Args:\n",
    "            trainMatrix (matrix, num_sentences * num_vocablist) : sentence matrix, returned by the function setOfWords2Vec()\n",
    "            classVec (vector, num_sentences)                    : label vector，returned by the function loadDataSet()\n",
    "        Returns:\n",
    "            p0Vect (vector, num_sentences)   - array of conditional probabilities for abusive class\n",
    "            p1Vect (vector, num_sentences) - array of conditional probabilities for not abusive class\n",
    "            pAbusive (number in [0,1])  - the probability that the document belongs to the abusive class\n",
    "        '''\n",
    "        numTrainDocs = len(trainMatrix)                       \n",
    "        numWords = len(trainMatrix[0])                        \n",
    "        self.pAbusive = sum(classVec)/numTrainDocs     \n",
    "        ### Create numpy.ones array, the number of appearance of all words is initialized to 1 due to Laplacian smoothing\n",
    "        p0Num = np.ones(numWords); p1Num = np.ones(numWords)  \n",
    "        ### The denominator is initialized to 2 due to Laplacian smoothing.\n",
    "        p0Denom = 2.0; p1Denom = 2.0                          \n",
    "        ### Calculate the probablities of appearance of all vocabulary words for the abusive and non-abusive class.\n",
    "        for i in range(numTrainDocs):\n",
    "            ### Update p1Num, p1Denom, p0Num, p0Denom\n",
    "            if classVec[i] == 1:   \n",
    "                p1Num += trainMatrix[i]\n",
    "                p1Denom += sum(trainMatrix[i])\n",
    "            else:                      \n",
    "                p0Num += trainMatrix[i]\n",
    "                p0Denom += sum(trainMatrix[i])\n",
    "        self.p1Vect = p1Num/p1Denom\n",
    "        self.p0Vect = p0Num/p0Denom\n",
    "        return self.p0Vect, self.p1Vect, self.pAbusive\n",
    "    \n",
    "\n",
    "    def predict(self, vec2Classify):\n",
    "        '''\n",
    "        Args:\n",
    "            vec2Classify - the word list (or sentence) to be classfied\n",
    "        Returns:\n",
    "            0/1 - classified as not abusive/abusive\n",
    "        '''\n",
    "        logp1Vect = np.log(self.p1Vect)                     \n",
    "        logp0Vect = np.log(self.p0Vect)\n",
    "        p1 = np.sum(vec2Classify * logp1Vect) + np.log(self.pAbusive)       \n",
    "        p0 = np.sum(vec2Classify * logp0Vect) + np.log(1.0 - self.pAbusive)\n",
    "        if p1 > p0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0V:\n",
      " [0.07692308 0.07692308 0.07692308 0.03846154 0.07692308 0.07692308\n",
      " 0.07692308 0.07692308 0.03846154 0.03846154 0.07692308 0.15384615\n",
      " 0.07692308 0.03846154 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308 0.03846154 0.11538462 0.03846154 0.07692308 0.07692308\n",
      " 0.03846154 0.07692308 0.07692308 0.03846154 0.03846154 0.03846154\n",
      " 0.07692308 0.03846154]\n",
      "p1V:\n",
      " [0.14285714 0.04761905 0.04761905 0.0952381  0.0952381  0.04761905\n",
      " 0.04761905 0.04761905 0.0952381  0.0952381  0.04761905 0.04761905\n",
      " 0.04761905 0.0952381  0.04761905 0.04761905 0.0952381  0.04761905\n",
      " 0.04761905 0.0952381  0.0952381  0.0952381  0.04761905 0.04761905\n",
      " 0.19047619 0.04761905 0.04761905 0.0952381  0.0952381  0.14285714\n",
      " 0.04761905 0.0952381 ]\n",
      "pAbusive:\n",
      " 0.5\n"
     ]
    }
   ],
   "source": [
    "NBmodel = NaiveBayes()\n",
    "p0V, p1V, pAb = NBmodel.fit(trainMat, classVec)\n",
    "print('p0V:\\n', p0V)\n",
    "print('p1V:\\n', p1V)\n",
    "print('pAbusive:\\n', pAb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Predict the new sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] is classified as: 0\n",
      "['stupid', 'garbage'] is classified as: 1\n"
     ]
    }
   ],
   "source": [
    "testEntry = ['love', 'my', 'dalmation']\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "print('{} is classified as: {}'.format(testEntry, NBmodel.predict(thisDoc)))\n",
    "\n",
    "testEntry = ['stupid', 'garbage']\n",
    "thisDoc = np.array(setOfWords2Vec(myVocabList, testEntry))\n",
    "print('{} is classified as: {}'.format(testEntry, NBmodel.predict(thisDoc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Discriminant Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(dataset_path, file_type=\"txt\"):\n",
    "    if file_type == \"txt\":\n",
    "        X = []                                                       ### create feature matrix\n",
    "        y = []                                                       ### create label matrix\n",
    "        fr = open(dataset_path)                                      ### open file\n",
    "        for line in fr.readlines():                                  ### read datum\n",
    "            lineArr = line.strip().split()                           ### remove the `\\n` and obtain the data from string\n",
    "            X.append([float(x) for x in lineArr[:-1]])               ### add to the feature matrix\n",
    "            y.append(float(lineArr[-1]))                             ### add to the label matrix\n",
    "        fr.close()                                                   ### close file\n",
    "        return X, y \n",
    "\n",
    "# read the data\n",
    "import numpy as np\n",
    "X_train, y_train = loadDataSet(\"horseColicTraining.txt\")\n",
    "X_test, y_test = loadDataSet(\"horseColicTest.txt\")\n",
    "\n",
    "# transform the data from list to np.array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# normalize\n",
    "X = np.vstack([X_train, X_test])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA(object):\n",
    "    '''\n",
    "    This class is for linear discriminant analysis classification.\n",
    "    \n",
    "    The class contains the parameters of LDA, including the number of classes and the prior probability p(i) of \n",
    "    each class $i$, where $i=1,2,\\ldots,num_classes$. Moreover, the class contains the the mean vectors $\\mu_i$ \n",
    "    and covariance matrix $\\Sigma$ of probability distributions $p(x|i)$ for the class $i$.\n",
    "    \n",
    "    It also contains the functions for initializing the class, fitting the LDA classifier model, use \n",
    "    the fitted model to calculate the linear discriminant functions $\\delta_i(x)$ and decision function $h^*(x)$.\n",
    "    \n",
    "    Attributes:\n",
    "        mu (matrix, num_classes*num_features)    : mean vectors of distributions $p(x|i)$. The $i$-th row represents $\\mu_i$.\n",
    "        Sigma (matrix, num_features*num_features): covariance matrix\n",
    "        num_classes (positive integer)           : the number of classes\n",
    "        priorProbs (vector, num_classes)         : the prior probability vector and its $i$-th element is $p(i)$\n",
    "        \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the class by just assigning zero to all atrributes. \n",
    "        '''\n",
    "        self.mu = 0 \n",
    "        self.Sigma = 0\n",
    "        self.num_classes = 0\n",
    "        self.priorProbs = 0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        estimate the mean vector and covariance matrix of each class in the LDA model\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_train*num_features): features of training samples\n",
    "            y (matrix, num_train): label of training samples\n",
    "            \n",
    "        Returns:\n",
    "            mu (matrix, num_classes*num_features)    : mean vectors of distributions $p(x|i)$. The $i$-th row represents $\\mu_i$.\n",
    "            Sigma (matrix, num_features*num_features): covariance matrix\n",
    "        ''' \n",
    "        num_samples, num_features = X.shape\n",
    "        values, counts = np.unique(y, return_counts = True)\n",
    "        num_classes = len(values)\n",
    "        ### calculate the prior probability $p(i)$\n",
    "        self.priorProbs = counts / num_samples\n",
    "        ### calculate the mean vector of each class $\\mu_i$\n",
    "        self.mu = np.zeros((num_classes, num_features))\n",
    "        for k in range(num_samples):\n",
    "            self.mu[int(y[k]),:] += X[k,:]\n",
    "        self.mu = self.mu / np.expand_dims(counts, 1) \n",
    "        ### calculate the covariance matrix $\\Sigma$\n",
    "        Sigma_i = [np.cov(X[y == i].T)*(X[y == i].shape[0]-1) for i in range(num_classes)] \n",
    "        self.Sigma = sum(Sigma_i) / (X.shape[0]-num_classes)\n",
    "        return self.mu, self.Sigma\n",
    "    \n",
    "    def linear_discriminant_func(self, X):\n",
    "        '''\n",
    "        calculate the linear discriminant functions $\\delta_i(X)$\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_samples*num_features): features of samples\n",
    "            \n",
    "        Returns:\n",
    "            value (matrix, num_samples*num_classes): the linear discriminant function values. \n",
    "            The $(j,i)$-th entry of value represents $\\delta_i(X[j,:])$, which is the linear discriminant function value for the class $i$ of the sample at row $j$.\n",
    "        '''\n",
    "        ### calculate the inverse matrix of the covariance matrix $\\Sigma$\n",
    "        U, S, V = np.linalg.svd(self.Sigma)\n",
    "        Sn = np.linalg.inv(np.diag(S))\n",
    "        Sigma_inv = np.dot(np.dot(V.T, Sn), U.T)\n",
    "        ### calculate the linear discriminant function values of X\n",
    "        value = np.dot(np.dot(X, Sigma_inv), self.mu.T) - \\\n",
    "                0.5 * np.multiply(np.dot(self.mu, Sigma_inv).T, self.mu.T).sum(axis = 0).reshape(1, -1) + \\\n",
    "                np.log(np.expand_dims(self.priorProbs, axis = 0))\n",
    "        return value\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        calculate the linear discriminant functions\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_samples*num_features): features of samples\n",
    "            \n",
    "        Returns:\n",
    "            pred_label (vector, num_samples): the predicted labels of samples. The $j$-th entry represents the predicted label of the sample at row $j$.\n",
    "        '''\n",
    "        pred_value = self.linear_discriminant_func(X)\n",
    "        pred_label = np.argmax(pred_value, axis = 1)\n",
    "        return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA on the test dataset is 0.7313432835820896.\n"
     ]
    }
   ],
   "source": [
    "### initiate the LDA model\n",
    "model = LDA()\n",
    "### fit the model with training data and get the estimation of mu and Sigma\n",
    "mu, Sigma = model.fit(X_train, y_train)\n",
    "### predict the label of test data\n",
    "y_pred = model.predict(X_test)\n",
    "### calculate the accuracy of the fitted LDA model on test data\n",
    "accuracy = np.sum(y_pred == y_test)/len(y_test)\n",
    "print(\"Accuracy of LDA on the test dataset is {}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. EM algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate samples from the mixture Gaussian Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    " \n",
    "def generate_data(num_samples, alpha, mu_list, sigma_list):    \n",
    "    '''\n",
    "    Generate tbe synthetic dataset from the mixture-of-Gaussian distribution\n",
    "    Args:\n",
    "        num_samples (positive integer)                  : number of samples\n",
    "        alpha  (vector, num_classes)                    : prior probability vetor\n",
    "        mu_list (list of length num_classes)            : the $i$-th element is the mean vector of $i$-th class\n",
    "        sigma_list (list of length num_classes)         : the $i$-th element is the covariance matrix of $i$-th class\n",
    "    Returns:\n",
    "        X (matrix, num_samples * num_features)        : generated data\n",
    "      \n",
    "    '''\n",
    "    \n",
    "    num_components = len(mu_list)\n",
    "    num_features = len(mu_list[0])\n",
    "    X = np.zeros((num_samples, num_features))       \n",
    "    # Generate random numbers in [0,1]\n",
    "    random_numbers = np.random.random(num_samples)\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_components):\n",
    "            if random_numbers[i] < sum(alpha[:j+1]):  \n",
    "                X[i,:]  = np.random.multivariate_normal(mu_list[j], sigma_list[j], 1) \n",
    "                break\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM_for_MG():\n",
    "    '''\n",
    "    This class is for using EM algorithm to estimate the parameters of mixture-of-Gaussian distribution.\n",
    "    \n",
    "    The class contains the parameters of EM iteration, including the number of classes $N$, the prior probabilities \n",
    "    $\\alpha_i$, the mean vectors $\\mu_i$ and covariance matrix $\\Sigma_i$ for each class $i$.\n",
    "    \n",
    "    It also contains the functions for initializing the class, updating parameters in E-step and M-step, iterate over \n",
    "    the two steps until convergence.\n",
    "    \n",
    "    Attributes:\n",
    "        num_classes (positive integer)           : the number of Gaussian components\n",
    "        hat_alpha (list of length num_classes)   : the prior probability of each component\n",
    "        hat_mu (list of length num_classes)      : the mean vector of each Gaussian component\n",
    "        hat_sigma (list of length num_classes)   : the covariance matrix of each Gaussian component\n",
    "        posterior_prob (matrix, num_samples * num_classes) : the posterior probability matrix and the $(j,i)$-th entry\n",
    "            represents the posterior probability that the sample X[j,:] is from the $i$-th Gaussian component.\n",
    "                                                   \n",
    "        \n",
    "    '''\n",
    "    def __init__(self, num_classes=2, num_iteration=1000):\n",
    "        '''\n",
    "        Initialize the class for using EM algorithm to estimate the parameters in the Mixture-of-Gaussian model. \n",
    "        '''\n",
    "        self.num_classes = num_classes\n",
    "        self.num_iteration = num_iteration\n",
    "        self.hat_alpha = []\n",
    "        self.hat_mu = []\n",
    "        self.hat_sigma = []\n",
    "        self.posterior_prob = 0\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \n",
    "        self.num_samples, self.num_features = X.shape\n",
    "        ### Initialize parameters\n",
    "        self.hat_alpha = [1/self.num_classes] * self.num_classes\n",
    "        self.hat_mu = [np.min(X,axis=0) + (ell+1) / (self.num_classes+1) * (np.max(X, axis=0) - np.min(X, axis=0)) for ell in range(self.num_classes)]\n",
    "        self.hat_sigma = [np.eye(self.num_features)*np.std(X,axis=0)] * self.num_classes\n",
    "        ### Iteration begins\n",
    "        previous_alpha = self.hat_alpha\n",
    "        previous_mu = self.hat_mu\n",
    "        for t in range(self.num_iteration):   \n",
    "            ### E-step: Update posterior probability $\\gamma_{ji}$\n",
    "            self.E_step(X)    \n",
    "            ### M-step: Update parameters $alpha, mu, sigma$\n",
    "            self.M_step(X)    \n",
    "            ### Judge whether the parameter estimations converge or not\n",
    "            err_mu = np.mean(np.abs(np.array(previous_mu)-np.array(self.hat_mu)))     \n",
    "            err_alpha = np.mean(np.abs(previous_alpha)-np.abs(self.hat_alpha))\n",
    "            if (err_mu <= 0.001) and (err_alpha < 0.001):     \n",
    "                print('Converged after {} iterations'.format(t+1))\n",
    "                break\n",
    "            else:\n",
    "                previous_mu = self.hat_mu\n",
    "                previous_alpha = self.hat_alpha\n",
    "            ### print the result every 20 iterations\n",
    "            if (t % 20 == 0):\n",
    "                print('The number of iterations is:', t+1)\n",
    "                print(\"The estimated mean vectors are:\",self.hat_mu)\n",
    "                print(\"The estimated prior probablilities are:\",self.hat_alpha)\n",
    "        return self.hat_alpha, self.hat_mu, self.hat_sigma\n",
    "        \n",
    "        \n",
    "    def E_step(self, X):\n",
    "        '''\n",
    "        Calculate the posterior probablilty $\\gamma_{ji}$ for each class $i$.\n",
    "        '''\n",
    "        self.posterior_prob = np.zeros((self.num_samples, self.num_classes))\n",
    "        for j in range(self.num_samples):\n",
    "            denom = 0\n",
    "            for i in range(self.num_classes):\n",
    "                denom += self.hat_alpha[i] * np.exp(-(X[j,:]-self.hat_mu[i]).reshape(1,-1)@np.linalg.inv(self.hat_sigma[i])@(X[j,:]-self.hat_mu[i]).reshape(-1,1)/2)[0,0]/np.sqrt(np.linalg.det(self.hat_sigma[i]))\n",
    "            for i in range(self.num_classes):\n",
    "                numer = np.exp(-(X[j,:]-self.hat_mu[i]).reshape(1,-1)@np.linalg.inv(self.hat_sigma[i])@(X[j,:]-self.hat_mu[i]).reshape(-1,1)/2)[0,0]/np.sqrt(np.linalg.det(self.hat_sigma[i]))   \n",
    "                self.posterior_prob[j,i] = self.hat_alpha[i]*numer/denom      \n",
    "\n",
    "    \n",
    "    def M_step(self, X):\n",
    "        '''\n",
    "        Update the parameters $\\alpha_i$, $\\mu_i$ and $\\Sigma_i$\n",
    "        '''\n",
    "        num_features = np.shape(X)[1]\n",
    "        self.hat_mu, self.hat_alpha, self.hat_sigma = [], [], []\n",
    "        for i in range(self.num_classes):\n",
    "            denom=0   \n",
    "            numer=0   \n",
    "            for j in range(self.num_samples):\n",
    "                numer += self.posterior_prob[j,i]*X[j,:]\n",
    "                denom += self.posterior_prob[j,i]\n",
    "            self.hat_mu.append(numer/denom)    \n",
    "            self.hat_alpha.append(denom/self.num_samples)     \n",
    "        for i in range(self.num_classes):\n",
    "            cov_matrix = np.zeros((self.num_features,self.num_features))\n",
    "            for j in range(self.num_samples):\n",
    "                cov_matrix += self.posterior_prob[j,i] * np.dot((X[j,:] - self.hat_mu[i]).reshape(-1,1),(X[j,:] - self.hat_mu[i]).reshape(1,-1))\n",
    "            self.hat_sigma.append(cov_matrix/np.sum(self.posterior_prob[:,i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of iterations is: 1\n",
      "The estimated mean vectors are: [array([5.54740373, 6.39989141]), array([14.97425697, 16.1360408 ]), array([26.04041944, 21.09775304]), array([45.14208248, 30.00284033])]\n",
      "The estimated prior probablilities are: [0.14781162761653793, 0.23755184520641426, 0.2139882530246035, 0.4006482741524441]\n",
      "The number of iterations is: 21\n",
      "The estimated mean vectors are: [array([4.86511192, 4.48628506]), array([10.04122609, 14.78472996]), array([24.72561463, 19.92091405]), array([45.08480489, 29.91543767])]\n",
      "The estimated prior probablilities are: [0.10966891235083012, 0.18915836036094022, 0.29517846619104515, 0.4059942610971841]\n",
      "Converged after 36 iterations\n",
      "The mean vectors converge to: [array([4.71381662, 4.2074898 ]), array([ 9.96878205, 14.62347069]), array([24.72846409, 19.9208832 ]), array([45.08480369, 29.91543702])]\n",
      "The prior probablilities converge to: [0.10374668320673734, 0.19519354885398635, 0.2950654422147265, 0.4059943257245499]\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1000         \n",
    "num_components = 4            \n",
    "alpha = [0.1,0.2,0.3,0.4]  \n",
    "mu1 = [5,5]\n",
    "mu2 = [10,15]\n",
    "mu3 = [25,20]\n",
    "mu4 = [45,30]\n",
    "mu_list = [mu1, mu2, mu3, mu4]\n",
    "sigma_list = [np.array([[10, 0], [0, 10]])]*4\n",
    "dataset = generate_data(num_samples, alpha, mu_list, sigma_list) \n",
    "num_iteration = 1000\n",
    "model = EM_for_MG(num_components, num_iteration)\n",
    "hat_alpha, hat_mu, hat_sigma = model.fit(dataset)\n",
    "print(\"The mean vectors converge to:\", hat_mu)\n",
    "print(\"The prior probablilities converge to:\", hat_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
