{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Regression\n",
    "\n",
    "## 1.1 Generate Synthetic Data\n",
    "\n",
    "We generate a synthetic data sampled from a sparse linear regression model. To be specific, the input $x = (x_1, \\ldots, x_6)$ has six variables but the output $y$ is the linear combination of only the first two variables $x_1$ and $x_2$. Mathematically speaking, the sparse linear regression model is given by\n",
    "\\begin{align*}\n",
    "y = x_1 + 3x_2 + 2 + \\varepsilon,\n",
    "\\end{align*}\n",
    "where $\\varepsilon$ is the noise from the normal distribution $\\mathcal{N}(0,0.1)$. To represent it in the matrix form, we denote as $w = (1,3,0,0,0,0)^{\\top}$ and $b = 2$. Then we get\n",
    "\\begin{align*}\n",
    "y = w^{\\top} x + b + \\varepsilon,\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def simulation(m):\n",
    "    \"\"\"\n",
    "    Generate a specified number of samples according to the sparse linear model.\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    m : num_samples\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    x (matrix, m*num_variables) : Input or features \n",
    "    y (matrix, m*1): Output or labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate independent and identically distributed samples as inputs.\n",
    "    x1 = np.random.normal(3,1,[m,1])\n",
    "    x2 = np.random.uniform(0,1,[m,1])\n",
    "    x3 = np.random.normal(1,4,[m,1])\n",
    "    x4 = np.random.normal(-1,1,[m,1])\n",
    "    x5 = np.random.normal(0,1,[m,1])\n",
    "    x6 = np.random.uniform(-1,1,[m,1])\n",
    "    # Generate the true outputs according to the sparse linear model.\n",
    "    y = x1 + 3*x2 + 2 + np.random.normal(0,0.1,[m,1])\n",
    "    return np.hstack([x1,x2,x3,x4,x5,x6]), y\n",
    "\n",
    "# Generate 5000 samples and split them into training and test dataset.\n",
    "X, y = simulation(5000)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Simulation Experiments\n",
    "\n",
    "Next we will fit the ridge regression and lasso to the simulation data.\n",
    "To be specific, we will follow the paradiam as belows:\n",
    "\n",
    "1. Implement the ridge regression and lasso algorithm in the form of ``class``. \n",
    "2. Fit the models with training samples, (i.e. ``X_train`` and ``y_train``)\n",
    "3. Prediect the outputs of test data ``X_test``.\n",
    "4. Evaluate the perfomance of the fitted model by calculating the MSE between the predictions and true outputs ``y_test``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2.1 Ridge Regression \n",
    "\n",
    "In the following ``RidgeRegression`` class, we aim to implement the ridge regression algorithm including the functions to fit the model with training data and predict the output for test data.\n",
    "\n",
    "Please type in the codes in the specified space to complete the construction of the class ``Lasso``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression():\n",
    "    '''\n",
    "    This is a class for ridge regression algorithm.\n",
    "    \n",
    "    The class contains the hyper parameters of the ridge regression algorithm as attributes, such as the regurization \n",
    "    parameter(Lambda).\n",
    "    It also contains the functions for initializing the class, calculating the loss, fitting the ridge regression \n",
    "    model and use the fitted model to predict test samples.\n",
    "    \n",
    "    Attributes:\n",
    "        lr:        learning rate of gradient descent\n",
    "        Lambda:    regularization parameter for L_2 penalty\n",
    "        max_itr:   maximum number of iteration for gradient descent\n",
    "        tol:       if the change in loss is smaller than tol, then we stop iteration\n",
    "        W:         concatenation of weight w and bias b\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, lr, Lambda, max_itr, tol):\n",
    "        '''\n",
    "        Initialize the RidgeRegression class\n",
    "        '''\n",
    "        self.lr = lr\n",
    "        self.Lambda = Lambda\n",
    "        self.max_itr = max_itr\n",
    "        self.tol = tol\n",
    "        \n",
    "    def _loss_ridge(self, X, y, W):\n",
    "        '''\n",
    "        Calculating the regularized empirical loss\n",
    "        '''\n",
    "        return ((y-X@W).T@(y-X@W))[0,0] + self.Lambda * np.sum(W[:X.shape[1]-1,0]**2)\n",
    "    \n",
    "    \n",
    "    def fit(self,x,y):  \n",
    "        '''\n",
    "        estimate the weight and bias in the ridge regression model by gradient descent\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_train*num_variables): input of training samples\n",
    "            y (matrix, num_test*1): output of training samples\n",
    "            \n",
    "        Returns:\n",
    "            self.W (matrix, (num_variables+1)*1): estimation of weight w and bias b\n",
    "        ''' \n",
    "        m = x.shape[0]\n",
    "        ### Add the all-one vector to the last column \n",
    "        X = np.concatenate((x,np.ones((m,1))),axis=1)\n",
    "        d = X.shape[1]\n",
    "        self.W = np.ones((d,1))\n",
    "        \n",
    "        ### Use the gradient descent to update W\n",
    "        previous_loss = self._loss_ridge(X, y, self.W)\n",
    "        for i in range(self.max_itr):\n",
    "            L_2_der_W = np.zeros((d,1))\n",
    "            L_2_der_W[:d,0] = self.W[:d,0]\n",
    "            gradient = X.T@(X@self.W-y)/m + self.Lambda * L_2_der_W\n",
    "            self.W = self.W - self.lr * gradient\n",
    "            current_loss = self._loss_ridge(X, y, self.W)\n",
    "            if previous_loss - current_loss < self.tol:\n",
    "                print(f'Converged after {i} iterations')\n",
    "                break\n",
    "            else:\n",
    "                previous_loss = current_loss\n",
    "        return self.W\n",
    "    \n",
    "    def predict(self,x): \n",
    "        '''\n",
    "        predict the output of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted outputs of test samples\n",
    "        ''' \n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x, np.ones((m,1))),axis=1)\n",
    "        return np.dot(X, self.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use the class ``RidgeRegression`` to fit, predict and evaluate the ridge regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 9928 iterations\n",
      "MSE of Ridge Regression is 0.010401064104879443\n",
      "The weight w of Ridge Regression is \n",
      " [ 1.00703274e+00  2.95993559e+00  9.41954206e-04 -5.41866994e-03\n",
      "  1.90221845e-03  4.70667448e-04].\n",
      "The bias b of Ridge Regression is 1.9876498728411585.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "### Initial the class RidgeRegression by assigning values to the parameters.\n",
    "model = RidgeRegression(lr=0.01, Lambda=0.002, max_itr = 20000, tol = 1e-5)\n",
    "### Fit model with training data\n",
    "W = model.fit(X_train, y_train)\n",
    "### Predict the output of test samples\n",
    "y_pred = model.predict(X_test)\n",
    "### Evaluate the model by calculating the MSE of test samples.\n",
    "mse = mean_squared_error(y_pred, y_test)\n",
    "### Print MSE \n",
    "print(\"MSE of Ridge Regression is {}\".format(mse))\n",
    "### Print the estimated w and b\n",
    "print(\"The weight w of Ridge Regression is \\n {}.\".format(W[:X_test.shape[1],0].T))\n",
    "print(\"The bias b of Ridge Regression is {}.\".format(W[X_test.shape[1],0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Lasso\n",
    "Similar to the ``RidgeRegression`` class, we will implement the Lasso algorithm.\n",
    "\n",
    "Please type in the codes in the specified space to complete the construction of the class ``Lasso``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso():\n",
    "    '''\n",
    "    This is a class for Lasso algorithm.\n",
    "    \n",
    "    The class contains the hyper parameters of the lasso algorithm as attributes, such as the regurization \n",
    "    parameter(Lambda) of L_1 penality.\n",
    "    It also contains the functions for initializing the class, fitting the lasso model and use the fitted \n",
    "    model to predict test samples.\n",
    "    \n",
    "    Attributes:\n",
    "        Lambda:    regularization parameter for L_1 penalty\n",
    "        max_itr:   maximum number of iteration for gradient descent\n",
    "        tol:       if the change in loss is smaller than tol, then we stop iteration\n",
    "        W:         concatenation of weight w and bias b\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, Lambda=0.5, max_itr=100, tol=0.0001):\n",
    "        '''\n",
    "        Initialize the RidgeRegression class\n",
    "        '''\n",
    "        self.Lambda = Lambda\n",
    "        self.max_itr = max_itr\n",
    "        self.tol = tol  \n",
    "    \n",
    "    def _loss_lasso(self, X, y, W):\n",
    "        '''\n",
    "        Calculating the regularized empirical loss\n",
    "        '''\n",
    "        return ((y-X@W).T@(y-X@W))[0,0] + self.Lambda * np.sum(np.abs(W[:X.shape[1]-1,0]))\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        estimate the weight and bias in the lasso model by coordinate gradient descent\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_train*num_variables): input of training samples\n",
    "            y (matrix, num_test*1): output of training samples\n",
    "            \n",
    "        Returns:\n",
    "            self.W (matrix, (num_variables+1)*1): estimation of weight w and bias b\n",
    "        '''\n",
    "        m = x.shape[0]\n",
    "        ### Add the all-one vector to the last column \n",
    "        X = np.concatenate((x,np.ones((m,1))),axis=1)\n",
    "        # weight and bias initialization\n",
    "        d = X.shape[1]\n",
    "        self.W = np.zeros((d,1))\n",
    "        \n",
    "        ### Use the cooridinate gradient descent to update W\n",
    "        previous_loss = self._loss_lasso(X, y, self.W)\n",
    "        for i in range(self.max_itr):\n",
    "            ### Update bias\n",
    "            self.W[-1,0] = np.mean(y.T-x@self.W[:-1,0])\n",
    "            ### Update W_j, j=0,...,d-2\n",
    "            for j in range(d-1):\n",
    "                # Calculate r_j = Y - X@W, with W[j,0]=0 and other elements in W unchanged\n",
    "                copy_W = self.W.copy()\n",
    "                copy_W[j,0] = 0\n",
    "                rj = y - X@copy_W\n",
    "                # Calculate X[:,j]@r_j and X[:,j].T@X[:,j]\n",
    "                aj = X[:,j].T@X[:,j]\n",
    "                bj = 2 * X[:,j]@rj / m \n",
    "                if bj <= -self.Lambda:\n",
    "                    self.W[j,0] = (bj + self.Lambda)/(2*aj)*m\n",
    "                elif bj >= self.Lambda:\n",
    "                    self.W[j,0] = (bj - self.Lambda)/(2*aj)*m\n",
    "                else:\n",
    "                    self.W[j,0] = 0\n",
    "            current_loss = self._loss_lasso(X, y, self.W)\n",
    "            if previous_loss - current_loss < self.tol:\n",
    "                print(f'Converged after {i} iterations')\n",
    "                break\n",
    "            else:\n",
    "                previous_loss = current_loss\n",
    "        return self.W\n",
    "    \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        predict the output of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted outputs of test samples\n",
    "        ''' \n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x,np.ones((m,1))),axis=1)\n",
    "        return  X@self.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use the class ``Lasso`` to fit, predict and evaluate the lasso model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 137 iterations\n",
      "MSE of Lasso is 0.010946914858139166\n",
      "The weight w of Lasso is \n",
      " [0.99483344 2.90448037 0.         0.         0.         0.        ].\n",
      "The bias b of Lasso is 2.0631135396663183.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "### Initial the class Lasso by assigning values to the parameters.\n",
    "model = Lasso(Lambda = 0.015, max_itr=10000, tol=1e-5)\n",
    "### Fit model with training data\n",
    "W = model.fit(X_train, y_train)\n",
    "### Predict the output of test samples\n",
    "y_pred = model.predict(X_test)\n",
    "### Evaluate the model by calculating the MSE of test samples.\n",
    "mse = mean_squared_error(y_pred, y_test)\n",
    "### Print MSE \n",
    "print(\"MSE of Lasso is {}\".format(mse))\n",
    "### Print the estimated w and b\n",
    "print(\"The weight w of Lasso is \\n {}.\".format(W[:X_test.shape[1],0].T))\n",
    "print(\"The bias b of Lasso is {}.\".format(W[X_test.shape[1],0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Real-world Experiments -- UCI dataset\n",
    "In order to apply the estimator `class` in real-world problems, we first learn about the sources of machine learning datasets for evaluating algorithms. For example, the `uci machine learning repository` is a well-known online sources with thousands of real-world datasets. \n",
    "\n",
    "https://archive.ics.uci.edu/ml/index.php\n",
    "\n",
    "### 1.3.1 Energy dataset \n",
    "In this tutorial, we select ``Energy`` (https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction) as our target regression dataset. Usually, before we use the dataset downloaded from the website, we are supposed to trim them first, e.g.~Make sure that the features and labels are in their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for ridge regression and lasso\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df = pd.read_csv(\"energydata_rv1.csv\")\n",
    "X = np.array(df.iloc[:,:-1])\n",
    "y = np.array(df.iloc[:,-1])\n",
    "# normalize\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "# split the train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Ridge Regression is 211.40297345443952\n",
      "The weight w of Ridge Regression is \n",
      " [ 0.16910642 -0.09258337  1.08079126  0.75970807  0.23066261  2.57910311\n",
      "  0.52520182  1.24506884  1.76542622 -0.47429644 -0.33042693  0.40706676\n",
      "  1.72303559  3.26209764  1.17317072 -1.27751672  1.97334509  0.0124819\n",
      "  1.03872677 -0.558929    1.59767475  4.16813434  4.00899188  2.14984311\n",
      "  1.44358676 -0.99561108].\n",
      "The bias b of Ridge Regression is 10.163365369387115.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "### Initial the class RidgeRegression by assigning values to the parameters.\n",
    "model = RidgeRegression(lr=0.01, Lambda=0.02, max_itr = 20000, tol = 1e-5)\n",
    "### Fit model with training data\n",
    "W = model.fit(X_train, y_train)\n",
    "### Predict the output of test samples\n",
    "y_pred = model.predict(X_test)\n",
    "### Evaluate the model by calculating the MSE of test samples.\n",
    "mse = mean_squared_error(y_pred, y_test)\n",
    "### Print MSE \n",
    "print(\"MSE of Ridge Regression is {}\".format(mse))\n",
    "### Print the estimated w and b\n",
    "print(\"The weight w of Ridge Regression is \\n {}.\".format(W[:X_test.shape[1],0].T))\n",
    "print(\"The bias b of Ridge Regression is {}.\".format(W[X_test.shape[1],0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Lasso is 209.8119576560742\n",
      "The weight w of Lasso is \n",
      " [-0.75791663  0.          0.          0.91385972  0.          0.25992675\n",
      "  0.          0.          0.2034837   0.          0.         -1.41598724\n",
      " -0.36242924  0.54596499  0.         -0.03448298  0.          0.73379924\n",
      "  0.         -1.57685701  0.          0.          0.98427781  0.\n",
      " -1.07493677  0.18704327].\n",
      "The bias b of Lasso is 25.133698679123533.\n"
     ]
    }
   ],
   "source": [
    "### Initial the class Lasso by assigning values to the parameters.\n",
    "model = Lasso(Lambda = 0.015, max_itr=100, tol=1e-5)\n",
    "### Fit model with training data\n",
    "W = model.fit(X_train, y_train)\n",
    "### Predict the output of test samples\n",
    "y_pred = model.predict(X_test)\n",
    "### Evaluate the model by calculating the MSE of test samples.\n",
    "mse = mean_squared_error(y_pred, y_test)\n",
    "### Print MSE \n",
    "print(\"MSE of Lasso is {}\".format(mse))\n",
    "### Print the estimated w and b\n",
    "print(\"The weight w of Lasso is \\n {}.\".format(W[:X_test.shape[1],0].T))\n",
    "print(\"The bias b of Lasso is {}.\".format(W[X_test.shape[1],0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classification\n",
    "\n",
    "## 2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    '''\n",
    "    This is a class for Logistic Regression algorithm.\n",
    "    \n",
    "    The class contains the hyper parameters of the logistic regression algorithm as attributes.\n",
    "    It also contains the functions for initializing the class, fitting the ridge regression model and use the fitted \n",
    "    model to predict test samples.\n",
    "    \n",
    "    Attributes:\n",
    "        lr:        learning rate of gradient descent\n",
    "        max_itr:   maximum number of iteration for gradient descent\n",
    "        tol:       if the change in loss is smaller than tol, then we stop iteration\n",
    "        W:         concatenation of weight w and bias b\n",
    "        verbose:   whether or not print the value of logitic loss every 1000 iterations\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, lr=0.01, max_itr=100000, tol = 1e-5, verbose = False):\n",
    "        self.lr = lr\n",
    "        self.max_itr = max_itr\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    " \n",
    "    def __sigmoid(self, z):\n",
    "        '''\n",
    "        Define the Sigmoid function to convert from real value to [0,1]\n",
    "        \n",
    "        Args: \n",
    "            z (matrix, num_samples*1): scores or real value\n",
    "            \n",
    "        Returns:\n",
    "            A matrix (num_variables+1)*1: a value in the interval [0,1]\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __logistic_loss(self, h, y):\n",
    "        '''\n",
    "        Calculate the logistic loss\n",
    "        '''\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        estimate the weight and bias in the logistic regression model by gradient descent\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_train*num_variables): input of training samples\n",
    "            y (matrix, num_test*1): labels of training samples, 0 or 1\n",
    "            \n",
    "        Returns:\n",
    "            self.W (matrix, (num_variables+1)*1): estimation of weight and bias, i.e (w,b)\n",
    "        '''\n",
    "        ### Add the all-one vector to the last column \n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x, np.ones((m, 1))), axis=1)\n",
    "        y = y.reshape(-1,1)\n",
    "        # weight and bias initialization\n",
    "        d = X.shape[1]\n",
    "        self.W = np.zeros((d,1))\n",
    "        \n",
    "        z = np.dot(X, self.W)\n",
    "        h = self.__sigmoid(z)\n",
    "        previous_loss = self.__logistic_loss(h, y)\n",
    "        for i in range(self.max_itr):\n",
    "            #Calculate the gradient and update w and b\n",
    "            z = np.dot(X, self.W)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / m\n",
    "            self.W -= self.lr * gradient\n",
    "            \n",
    "            #Calculate the new logistic loss\n",
    "            z = np.dot(X, self.W)\n",
    "            h = self.__sigmoid(z)\n",
    "            current_loss = self.__logistic_loss(h, y)\n",
    "            if previous_loss - current_loss < self.tol:\n",
    "                print('Converged after {} iterations'.format(i+1))\n",
    "                print('Logistic loss after {} iterations is {}'.format(i+1,current_loss))\n",
    "                break\n",
    "            else:\n",
    "                previous_loss = current_loss\n",
    "            if(self.verbose == True and i % 10000 == 0):\n",
    "                print('Logistic loss after {} iterations is {}'.format(i+1,current_loss))\n",
    "        return self.W\n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        '''\n",
    "        predict the posterior probability p_1(x; W) of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted posterior probability p_1(x; W) of test samples\n",
    "        ''' \n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x, np.ones((m, 1))), axis=1)\n",
    "        return self.__sigmoid(np.dot(X, self.W))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        predict the label of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted labels of test samples, 0 or 1\n",
    "        ''' \n",
    "        return self.predict_prob(x).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(dataset_path, file_type=\"txt\"):\n",
    "    if file_type == \"txt\":\n",
    "        X = []                                                        #create feature matrix\n",
    "        y = []                                                       # create label matrix\n",
    "        fr = open(dataset_path)                                            #open file\n",
    "        for line in fr.readlines():                                         #read datum\n",
    "            lineArr = line.strip().split()                                  #remove the `\\n` and obtain the data from string\n",
    "            X.append([float(x) for x in lineArr[:-1]])     # add to the feature matrix\n",
    "            y.append(float(lineArr[-1]))                                # add to the label matrix\n",
    "        fr.close()                                                          # close file\n",
    "        return X, y    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "import numpy as np\n",
    "X_train, y_train = loadDataSet(\"horseColicTraining.txt\")\n",
    "X_test, y_test = loadDataSet(\"horseColicTest.txt\")\n",
    "\n",
    "# transform the data from list to np.array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# normalize\n",
    "X = np.vstack([X_train, X_test])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss after 1 iterations is 0.6898762822641373\n",
      "Logistic loss after 10001 iterations is 0.5217484321994517\n",
      "Converged after 11802 iterations\n",
      "Logistic loss after 11802 iterations is 0.52171924670628\n",
      "[[ 0.7611359  -0.2002035   1.00541571 -2.51425541  0.82184505 -0.60528775\n",
      "  -0.36426046 -1.38660266 -0.16129892 -1.17278023  1.47758935 -0.60870321\n",
      "   1.39102436 -0.31464443 -0.98595971  0.58698705 -0.70142135 -0.4994333\n",
      "   1.04153942  0.06140696 -1.07582939  0.95021878]]\n",
      "The weight w of LR is \n",
      " [ 0.7611359  -0.2002035   1.00541571 -2.51425541  0.82184505 -0.60528775\n",
      " -0.36426046 -1.38660266 -0.16129892 -1.17278023  1.47758935 -0.60870321\n",
      "  1.39102436 -0.31464443 -0.98595971  0.58698705 -0.70142135 -0.4994333\n",
      "  1.04153942  0.06140696 -1.07582939].\n",
      "The bias b of LR is 0.9502187772809589.\n"
     ]
    }
   ],
   "source": [
    "### initiate the logistic regressor\n",
    "model = LogisticRegression(lr=0.1, max_itr=100000, tol = 1e-8, verbose=True)\n",
    "### fit the model with training data and get the estimation of parameters (w & b)\n",
    "W = model.fit(X_train, y_train)\n",
    "### Print the estimated w and b\n",
    "print(W.T)\n",
    "### Print the estimated w and b\n",
    "print(\"The weight w of LR is \\n {}.\".format(W[:X_test.shape[1],0].T))\n",
    "print(\"The bias b of LR is {}.\".format(W[X_test.shape[1],0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LR on the test dataset is 0.7164179104477612.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = np.sum(y_pred[:,0] == y_test)/len(y_test)\n",
    "print(\"Accuracy of LR on the test dataset is {}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
