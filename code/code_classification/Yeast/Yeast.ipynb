{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yeast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from encodings import search_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import necessary tools from the sklearn library\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import sklearn library tools used ONLY for validating my results\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import randint as sp_randint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # read dataset from csv file\n",
    "    data_name = \"yeast_classification\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "\n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.20, random_state=2200)\n",
    "    return train_X, test_X, train_y, test_y, data_X, data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLDA(object):\n",
    "    '''\n",
    "    This class is for linear discriminant analysis classification.\n",
    "    \n",
    "    The class contains the parameters of LDA, including the number of classes and the prior probability p(i) of \n",
    "    each class $i$, where $i=1,2,\\ldots,num_classes$. Moreover, the class contains the the mean vectors $\\mu_i$ \n",
    "    and covariance matrix $\\Sigma$ of probability distributions $p(x|i)$ for the class $i$.\n",
    "    \n",
    "    It also contains the functions for initializing the class, fitting the LDA classifier model, use \n",
    "    the fitted model to calculate the linear discriminant functions $\\delta_i(x)$ and decision function $h^*(x)$.\n",
    "    \n",
    "    Attributes:\n",
    "        mu (matrix, num_classes*num_features)    : mean vectors of distributions $p(x|i)$. The $i$-th row represents $\\mu_i$.\n",
    "        Sigma (matrix, num_features*num_features): covariance matrix\n",
    "        num_classes (positive integer)           : the number of classes\n",
    "        priorProbs (vector, num_classes)         : the prior probability vector and its $i$-th element is $p(i)$\n",
    "        \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the class by just assigning zero to all atrributes. \n",
    "        '''\n",
    "        self.mu = 0 \n",
    "        self.Sigma = 0\n",
    "        self.num_classes = 0\n",
    "        self.priorProbs = 0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        estimate the mean vector and covariance matrix of each class in the LDA model\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_train*num_features): features of training samples\n",
    "            y (matrix, num_train): label of training samples\n",
    "            \n",
    "        Returns:\n",
    "            mu (matrix, num_classes*num_features)    : mean vectors of distributions $p(x|i)$. The $i$-th row represents $\\mu_i$.\n",
    "            Sigma (matrix, num_features*num_features): covariance matrix\n",
    "        ''' \n",
    "        num_samples, num_features = X.shape\n",
    "        values, counts = np.unique(y, return_counts = True)\n",
    "        num_classes = len(values)\n",
    "        ### calculate the prior probability $p(i)$\n",
    "        self.priorProbs = counts / num_samples\n",
    "        ### calculate the mean vector of each class $\\mu_i$\n",
    "        self.mu = np.zeros((num_classes, num_features))\n",
    "        for k in range(num_samples):\n",
    "            self.mu[int(y[k]),:] += X[k,:]\n",
    "        self.mu = self.mu / np.expand_dims(counts, 1) \n",
    "        ### calculate the covariance matrix $\\Sigma$\n",
    "        Sigma_i = [np.cov(X[y == i].T)*(X[y == i].shape[0]-1) for i in range(num_classes)] \n",
    "        self.Sigma = sum(Sigma_i) / (X.shape[0]-num_classes)\n",
    "        return self.mu, self.Sigma\n",
    "    \n",
    "    def linear_discriminant_func(self, X):\n",
    "        '''\n",
    "        calculate the linear discriminant functions $\\delta_i(X)$\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_samples*num_features): features of samples\n",
    "            \n",
    "        Returns:\n",
    "            value (matrix, num_samples*num_classes): the linear discriminant function values. \n",
    "            The $(j,i)$-th entry of value represents $\\delta_i(X[j,:])$, which is the linear discriminant function value for the class $i$ of the sample at row $j$.\n",
    "        '''\n",
    "        ### calculate the inverse matrix of the covariance matrix $\\Sigma$\n",
    "        U, S, V = np.linalg.svd(self.Sigma)\n",
    "        Sn = np.linalg.inv(np.diag(S))\n",
    "        Sigma_inv = np.dot(np.dot(V.T, Sn), U.T)\n",
    "        ### calculate the linear discriminant function values of X\n",
    "        value = np.dot(np.dot(X, Sigma_inv), self.mu.T) - \\\n",
    "                0.5 * np.multiply(np.dot(self.mu, Sigma_inv).T, self.mu.T).sum(axis = 0).reshape(1, -1) + \\\n",
    "                np.log(np.expand_dims(self.priorProbs, axis = 0))\n",
    "        return value\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        calculate the linear discriminant functions\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_samples*num_features): features of samples\n",
    "            \n",
    "        Returns:\n",
    "            pred_label (vector, num_samples): the predicted labels of samples. The $j$-th entry represents the predicted label of the sample at row $j$.\n",
    "        '''\n",
    "        pred_value = self.linear_discriminant_func(X)\n",
    "        pred_label = np.argmax(pred_value, axis = 1)\n",
    "        return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA on the test dataset is 0.5488215488215489.\n"
     ]
    }
   ],
   "source": [
    "### initiate the LDA model\n",
    "model = myLDA()\n",
    "### fit the model with training data and get the estimation of mu and Sigma\n",
    "mu, Sigma = model.fit(main()[0], main()[2])\n",
    "### predict the label of test data\n",
    "y_pred = model.predict(main()[1])\n",
    "### calculate the accuracy of the fitted LDA model on test data\n",
    "accuracy = np.sum(y_pred == main()[3])/len(main()[3])\n",
    "print(\"Accuracy of LDA on the test dataset is {}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I runned my model and obtained a 0.56% accuracy. Not bad for having selected random hyper-paramenters, but maybe with a better understanding of my dataset, some data-cleaning, and sklearn tool-kit can make the difference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am loading the full dataset and renaming the columns to keep better track of each attribute\n",
    "data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "data_path = os.path.join(data_dir, \"yeast_classification.csv\")\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "col_names = ['col_1','col_2','col_3','col_4','col_5','col_6','col_7','col_8','name']\n",
    "df.columns = col_names\n",
    "\n",
    "df.head(3)\n",
    "df =df.drop(columns=['col_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>col_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.163951</td>\n",
       "      <td>0.158175</td>\n",
       "      <td>0.064922</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.075043</td>\n",
       "      <td>-0.124540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_3</th>\n",
       "      <td>-0.163951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059668</td>\n",
       "      <td>-0.008083</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>-0.185805</td>\n",
       "      <td>-0.022043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_4</th>\n",
       "      <td>0.158175</td>\n",
       "      <td>0.059668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005931</td>\n",
       "      <td>-0.009040</td>\n",
       "      <td>-0.103591</td>\n",
       "      <td>-0.054797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_5</th>\n",
       "      <td>0.064922</td>\n",
       "      <td>-0.008083</td>\n",
       "      <td>-0.005931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.002829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_6</th>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>-0.009040</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>-0.035659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_7</th>\n",
       "      <td>0.075043</td>\n",
       "      <td>-0.185805</td>\n",
       "      <td>-0.103591</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_8</th>\n",
       "      <td>-0.124540</td>\n",
       "      <td>-0.022043</td>\n",
       "      <td>-0.054797</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>-0.035659</td>\n",
       "      <td>0.089690</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          col_1     col_3     col_4     col_5     col_6     col_7     col_8\n",
       "col_1  1.000000 -0.163951  0.158175  0.064922  0.005597  0.075043 -0.124540\n",
       "col_3 -0.163951  1.000000  0.059668 -0.008083  0.009378 -0.185805 -0.022043\n",
       "col_4  0.158175  0.059668  1.000000 -0.005931 -0.009040 -0.103591 -0.054797\n",
       "col_5  0.064922 -0.008083 -0.005931  1.000000 -0.009674  0.043627  0.002829\n",
       "col_6  0.005597  0.009378 -0.009040 -0.009674  1.000000  0.020900 -0.035659\n",
       "col_7  0.075043 -0.185805 -0.103591  0.043627  0.020900  1.000000  0.089690\n",
       "col_8 -0.124540 -0.022043 -0.054797  0.002829 -0.035659  0.089690  1.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for high correlation among features\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500121</td>\n",
       "      <td>0.500034</td>\n",
       "      <td>0.261186</td>\n",
       "      <td>0.504717</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.499885</td>\n",
       "      <td>0.276199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.137299</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>0.137098</td>\n",
       "      <td>0.048351</td>\n",
       "      <td>0.075683</td>\n",
       "      <td>0.057797</td>\n",
       "      <td>0.106491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             col_1        col_3        col_4        col_5        col_6  \\\n",
       "count  1484.000000  1484.000000  1484.000000  1484.000000  1484.000000   \n",
       "mean      0.500121     0.500034     0.261186     0.504717     0.007500   \n",
       "std       0.137299     0.086670     0.137098     0.048351     0.075683   \n",
       "min       0.110000     0.210000     0.000000     0.500000     0.000000   \n",
       "25%       0.410000     0.460000     0.170000     0.500000     0.000000   \n",
       "50%       0.490000     0.510000     0.220000     0.500000     0.000000   \n",
       "75%       0.580000     0.550000     0.320000     0.500000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.830000   \n",
       "\n",
       "             col_7        col_8  \n",
       "count  1484.000000  1484.000000  \n",
       "mean      0.499885     0.276199  \n",
       "std       0.057797     0.106491  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.480000     0.220000  \n",
       "50%       0.510000     0.220000  \n",
       "75%       0.530000     0.300000  \n",
       "max       0.730000     1.000000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYT    463\n",
      "NUC    429\n",
      "MIT    244\n",
      "ME3    163\n",
      "ME2     51\n",
      "ME1     44\n",
      "EXC     35\n",
      "VAC     30\n",
      "POX     20\n",
      "ERL      5\n",
      "Name: name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.name.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x3600 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr                             = df.corr()\n",
    "mask                             = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax                            = plt.subplots(figsize=(10,50))\n",
    "cmap                             = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Matix\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.1, cbar_kws={\"shrink\": .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found a correlation between the different measurements of Hold, Pressure and Finger-Area with their averages. I also found a correlation with the different measurements of Hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA on the test dataset is 0.5993265993265994.\n"
     ]
    }
   ],
   "source": [
    "### initiate the LDA model\n",
    "data_X = df.iloc[:,:-1]\n",
    "data_y = df.iloc[:,-1]\n",
    "scaler_X = StandardScaler()\n",
    "data_X = scaler_X.fit_transform(data_X)\n",
    "data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "# Randomly assingning a train and test set\n",
    "train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.20, random_state=13)\n",
    "\n",
    "model = myLDA()\n",
    "### fit the model with training data and get the estimation of mu and Sigma\n",
    "mu, Sigma = model.fit(train_X, train_y)\n",
    "### predict the label of test data\n",
    "y_pred = model.predict(test_X)\n",
    "### calculate the accuracy of the fitted LDA model on test data\n",
    "accuracy = np.sum(y_pred == test_y)/len(test_y)\n",
    "print(\"Accuracy of LDA on the test dataset is {}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_f = RandomForestClassifier()\n",
    "algo_lda = LinearDiscriminantAnalysis()\n",
    "algo_dt = DecisionTreeClassifier()\n",
    "model_forrest = algo_f.fit(main()[0], main()[2])\n",
    "model_LDA = algo_lda.fit(main()[0], main()[2])\n",
    "model_DT = algo_dt.fit(main()[0], main()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 48.485 for the DT model\n",
      " Accuracy: 62.626 for the forrest model\n",
      " Accuracy: 54.882 for the LDA model\n"
     ]
    }
   ],
   "source": [
    "print(\" Accuracy: %.3f for the DT model\" % (model_DT.score(main()[1], main()[3])*100.0))\n",
    "print(\" Accuracy: %.3f for the forrest model\" % (model_forrest.score(main()[1], main()[3])*100.0))\n",
    "print(\" Accuracy: %.3f for the LDA model\" % (model_LDA.score(main()[1], main()[3])*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I again use the processed data from the logistic regression but this time I specify SVC\n",
    "\n",
    "SVM_best_scores = {}\n",
    "tree_param = {'criterion':['gini','entropy'],'max_depth':np.arange(3, 15)}\n",
    "search_svm = GridSearchCV(estimator = algo_dt ,param_grid= tree_param,\n",
    "                    cv = 5, return_train_score = True,\n",
    "                    n_jobs = -1)\n",
    "\n",
    "search_svm.fit(main()[0], main()[2])\n",
    "SVM_best_scores = {'model':search_svm, 'best_params':search_svm.best_params_,\n",
    "                        'best_score':search_svm.best_score_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 5}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_best_scores['best_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(DecisionTreeClassifier(), tree_param, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "              param_grid={'bootstrap': [False], 'criterion': ['gini', 'entropy'],\n",
       "                          'max_depth': range(1, 10),\n",
       "                          'max_features': ['auto', 'sqrt'],\n",
       "                          'min_samples_split': [3, 7, 10],\n",
       "                          'n_estimators': [10, 20, 71]}),\n",
       " 'best_params': {'bootstrap': False,\n",
       "  'criterion': 'entropy',\n",
       "  'max_depth': 9,\n",
       "  'max_features': 'auto',\n",
       "  'min_samples_split': 10,\n",
       "  'n_estimators': 71},\n",
       " 'best_score': 0.626809204694536}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# utilisation d'une grille complete avec toutes les parametres jugés nécessaires\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10,20,71],\n",
    "    \"bootstrap\": [False],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_split\" : [3,7, 10],\n",
    "    \"max_depth\" :range(10)[1:],\n",
    "    \"max_features\": ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Effectuer grid search\n",
    "grid_search = GridSearchCV(algo_f, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(main()[0], main()[2])\n",
    "forrest_best_scores = {'model':grid_search, 'best_params':grid_search.best_params_,\n",
    "                        'best_score':grid_search.best_score_}\n",
    "forrest_best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found some improved paramenters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path, header=1)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    # read dataset from csv file\n",
    "    data_name = \"yeast_classification\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion -- Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_counts(y, sample_weight, classes_):\n",
    "    '''\n",
    "    the function used to calculate the summation of weights of samples from each class. Generally speaking,\n",
    "    the weights are all set as one. But for Adaboost, each sample has different values.\n",
    "    '''\n",
    "    class_counts = np.zeros(shape=classes_.shape[0], dtype=np.float64)\n",
    "    for i, label in enumerate(classes_):\n",
    "        idx = y == label\n",
    "        if idx.sum() > 0:\n",
    "            class_counts[i] = sample_weight[idx].sum()\n",
    "        else:\n",
    "            class_counts[i] = 0\n",
    "    return class_counts\n",
    "\n",
    "def gini(y, sample_weight):\n",
    "    classes_ = np.unique(y)\n",
    "    class_counts = calculate_weighted_counts(y, sample_weight, classes_)\n",
    "    if class_counts.sum() > 0:\n",
    "        pk = class_counts / class_counts.sum()\n",
    "        pk = pk[pk > 0]\n",
    "        return 1 - np.sum(pk**2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def gini_index(X, y, feat, point, sample_weight):\n",
    "    '''\n",
    "    calculate the difference of gini index before and after splitting\n",
    "    '''\n",
    "    S = gini(y, sample_weight)\n",
    "    new_S = 0\n",
    "    n = sample_weight.sum()\n",
    "    assert n > 0\n",
    "    idx1 = X[:, feat] < point\n",
    "    nv = sample_weight[idx1].sum()\n",
    "    if nv > 0:\n",
    "        new_S += nv / n * gini(y[idx1], sample_weight[idx1])\n",
    "    idx2 = X[:, feat] >= point\n",
    "    nv = sample_weight[idx2].sum()\n",
    "    if nv > 0:\n",
    "        new_S += nv / n * gini(y[idx2], sample_weight[idx2])\n",
    "    return S - new_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Trees for Classification\n",
    "\n",
    "Different from the classification tree implemented in the last tutorial:\n",
    "1. Each internal node has two child nodes regardless of values of the splitting feature are continuous or discrete.\n",
    "2. Add the parameter `max_depth` for providing another condition to stop splitting procedures.\n",
    "3. Add the parameter `max_features` to use the subset of features to build decision tree.\n",
    "\n",
    "Options 2&3 are designed for constructing trees in the random forest implemented in Section 5. \n",
    "\n",
    "If you'd like to build the classification tree, you can ignore options 2&3 by setting `max_depth = None` and `max_features = None`. Then we can combine it with the pre-pruning or post-pruning technique implemented in Section 4 to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(object):\n",
    "    '''\n",
    "    This class is for classification tree\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of classification tree\n",
    "        - tree: a nested dictionary representing the decision tree structure.\n",
    "        - max_depth: the parameter to control the depth of tree. If the depth is larger than max_depth, we will stop splitting.\n",
    "        - max_feature: the number of selected features to build decision tree\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 criterion=gini_index,\n",
    "                 max_depth=None,\n",
    "                 max_features=None,\n",
    "                 random_seed=None):\n",
    "        self.f_criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        if self.max_depth is None:\n",
    "            self.max_depth = 2**10\n",
    "        self.max_features = max_features\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        np.random.seed(self.random_seed)\n",
    "        num_samples, num_features = X.shape\n",
    "        if self.max_features is None:\n",
    "            self.max_features = num_features\n",
    "        elif self.max_features == \"sqrt\":\n",
    "            self.max_features = np.int(np.round(np.sqrt(num_features)))\n",
    "        self.classes_ = np.unique(y)\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(num_samples, dtype=np.float64)\n",
    "        # build the decision tree\n",
    "        self.tree = self.create_tree(X, y, sample_weight, depth=0)\n",
    "\n",
    "    def create_tree(self, X, y, sample_weight, depth):\n",
    "        Tree = {}\n",
    "        Tree[\"depth\"] = depth\n",
    "        class_counts = calculate_weighted_counts(y, sample_weight, self.classes_)\n",
    "        # create a leaf node if all samples belong to the same class\n",
    "        if (class_counts != 0).sum() == 1:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            Tree[\"pred\"] = self.classes_[class_counts != 0]\n",
    "        # using the majority vote to get the prediction at each node\n",
    "        majority_class = self.classes_[np.argmax(class_counts)]\n",
    "        Tree[\"pred\"] = majority_class\n",
    "        # create a leaf node if feature set is empty\n",
    "        feat, point = self.choose_best_split(X, y, sample_weight)\n",
    "        if feat is None or depth == self.max_depth:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            return Tree\n",
    "        # otherwise, create an internal node\n",
    "        Tree[\"is_leaf\"] = False\n",
    "        Tree[\"split_feat\"] = feat\n",
    "        Tree[\"split_point\"] = point\n",
    "        # build the left subtree\n",
    "        idx = X[:, feat] < point\n",
    "        Tree[\"left\"] = self.create_tree(X[idx], y[idx], sample_weight[idx],\n",
    "                                        depth + 1)\n",
    "        # build the right subtree\n",
    "        idx = X[:, feat] >= point\n",
    "        Tree[\"right\"] = self.create_tree(X[idx], y[idx], sample_weight[idx],\n",
    "                                         depth + 1)\n",
    "        return Tree\n",
    "\n",
    "    def choose_best_split(self, X, y, sample_weight):\n",
    "        # initialization\n",
    "        best_feat, best_point = None, None\n",
    "        best_score = 0.0\n",
    "        # search for each candidate feature\n",
    "        num_features = X.shape[1]\n",
    "        if self.max_features < num_features:\n",
    "            candidate_feat = np.random.permutation(\n",
    "                num_features)[:self.max_features]\n",
    "        else:\n",
    "            candidate_feat = np.arange(num_features)\n",
    "        for feat in candidate_feat:\n",
    "            # if all values of this feature are equal, do not split this feature\n",
    "            X_feat_value = np.unique(X[:, feat])\n",
    "            if len(X_feat_value) == 1:\n",
    "                continue\n",
    "            # search for each possible split point\n",
    "            for i in range(len(X_feat_value) - 1):\n",
    "                # divide the dataset into two parts according to the split\n",
    "                point = (X_feat_value[i] + X_feat_value[i + 1]) / 2.0\n",
    "                # calculate score to evaluate the quality of a split\n",
    "                score = self.f_criterion(X, y, feat, point, sample_weight)\n",
    "                if score > best_score:\n",
    "                    best_feat = feat\n",
    "                    best_point = point\n",
    "                    best_score = score\n",
    "        return best_feat, best_point\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to fit the decision tree classifier\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "\n",
    "        Returns:\n",
    "            y - predictions of test samples, a pandas series with shape (n,)\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y = []\n",
    "        for i in range(n):\n",
    "            y.append(DecisionTreeClassifier.predict_each(X[i], self.tree))\n",
    "        y = np.array(y, dtype=np.int32)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def predict_each(x, tree):\n",
    "        '''\n",
    "        for each sample, get the prediction of decision tree classifier in a recursive manner.\n",
    "\n",
    "        Args:\n",
    "            x - features of a sample, a pandas Series with shape (d,)\n",
    "            tree - a nested dictionary representing the decision tree structure.\n",
    "\n",
    "        Returns:\n",
    "            the prediction of the sample `x`\n",
    "        '''\n",
    "        if tree[\"is_leaf\"] is True:\n",
    "            # if the `tree` is a leaf node, get the prediction at the leaf node\n",
    "            return tree[\"pred\"]\n",
    "        else:\n",
    "            # the 'tree' is a nested dictionary\n",
    "            # get the value of the feature used to split\n",
    "            feat = tree[\"split_feat\"]\n",
    "            point = tree[\"split_point\"]\n",
    "            # get the value of the feature for the sample `x`\n",
    "            value = x[feat]\n",
    "            if value < point:\n",
    "                return DecisionTreeClassifier.predict_each(x, tree[\"left\"])\n",
    "            else:\n",
    "                return DecisionTreeClassifier.predict_each(x, tree[\"right\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_t():\n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(main()[0], main()[1], test_size=0.33, random_state=100)\n",
    "    return train_X, test_X, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training data is: 1.0\n",
      "The accuracy of test data is: 0.4959183673469388\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = t_t()\n",
    "model = DecisionTreeClassifier(criterion=gini_index,\n",
    "                                   max_depth=None,\n",
    "                                   max_features=None,\n",
    "                                   random_seed=None)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"The accuracy of training data is:\", acc_train)\n",
    "print(\"The accuracy of test data is:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You maybe find that the accuracy of training data is almost one but the accuracy of test data is low. The reason is that the decision tree overfits to the training data. To prevent overfitting, we introduce the post-pruning technique in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Classification Tree with the Pruning Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pruning(tree, classes_, X_valid, y_valid):\n",
    "    '''\n",
    "    the function used to post-prune the decision tee\n",
    "\n",
    "    Args:\n",
    "        tree - a nested dictionary representing the decision tree structure.\n",
    "        classes_ - names of all classes \n",
    "        X_valid - the features of the validation samples\n",
    "        y_valid - the labels of the validation samples\n",
    "    Returns:\n",
    "        the tree structure after pruning\n",
    "   '''\n",
    "    if X_valid.shape[0] == 0:\n",
    "        new_tree = {}\n",
    "        new_tree[\"is_leaf\"] = True\n",
    "        new_tree[\"pred\"] = tree[\"pred\"]\n",
    "        return new_tree\n",
    "    if tree[\"is_leaf\"] is True:\n",
    "        return tree\n",
    "    feat = tree[\"split_feat\"]\n",
    "    point = tree[\"split_point\"]\n",
    "    idx1 = X_valid[:, feat] < point\n",
    "    tree[\"left\"] = pruning(tree[\"left\"], classes_, X_valid[idx1],\n",
    "                           y_valid[idx1])\n",
    "    idx2 = X_valid[:, feat] >= point\n",
    "    tree[\"right\"] = pruning(tree[\"right\"], classes_, X_valid[idx2],\n",
    "                            y_valid[idx2])\n",
    "    if tree[\"left\"][\"is_leaf\"] is True and tree[\"right\"][\"is_leaf\"] is True:\n",
    "        FLAG = True\n",
    "    else:\n",
    "        FLAG = False\n",
    "    if FLAG:\n",
    "        # check validation accuracy gap\n",
    "        valid_y_true = []\n",
    "        valid_y_pred = []\n",
    "        # make prediction and calculate validation accuracy of the tree before merging\n",
    "        child_majority_class = tree[\"left\"][\"pred\"]\n",
    "        idx1 = X_valid[:, feat] < point\n",
    "        if idx1.sum() > 0:\n",
    "            valid_y_true.append(y_valid[idx1])\n",
    "            valid_y_pred.append([child_majority_class] * idx1.sum())\n",
    "        child_majority_class = tree[\"right\"][\"pred\"]\n",
    "        idx2 = X_valid[:, feat] >= point\n",
    "        if idx2.sum() > 0:\n",
    "            valid_y_true.append(y_valid[idx2])\n",
    "            valid_y_pred.append([child_majority_class] * idx2.sum())\n",
    "        valid_y_true = np.concatenate(valid_y_true)\n",
    "        valid_y_pred = np.concatenate(valid_y_pred)\n",
    "        valid_acc_before = np.mean(valid_y_true == valid_y_pred)\n",
    "        # make prediction and calculate validation accuracy of the tree after merging\n",
    "        majority_class = tree[\"pred\"]\n",
    "        valid_y_pred = np.array([majority_class] * X_valid.shape[0])\n",
    "        valid_acc_after = np.mean(valid_y_true == valid_y_pred)\n",
    "        # if the validation accuracy after merging is larger, we will prune\n",
    "        if valid_acc_after > valid_acc_before:\n",
    "            new_tree = {}\n",
    "            new_tree[\"is_leaf\"] = True\n",
    "            new_tree[\"pred\"] = tree[\"pred\"]\n",
    "            return new_tree\n",
    "        else:\n",
    "            return tree\n",
    "    else:\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of the classification tree Without pruning is: 1.0\n",
      "Validation accuracy of the classification tree Without pruning is: 0.4865771812080537\n",
      "Testing accuracy of the classification tree Without pruning is: 0.44693877551020406 \n",
      "\n",
      "Training accuracy of the classification tree With pruning is: 0.9309352517985612\n",
      "Validation accuracy of the classification tree With pruning is: 0.5570469798657718\n",
      "Testing accuracy of the classification tree With pruning is: 0.46938775510204084\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = t_t()\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train,\n",
    "                                                      y_train,\n",
    "                                                      test_size=0.3,\n",
    "                                                      stratify=y_train,\n",
    "                                                      random_state=3147)\n",
    "model = DecisionTreeClassifier(criterion=gini_index,\n",
    "                                   max_depth=None,\n",
    "                                   max_features=None,\n",
    "                                   random_seed=None)\n",
    "model.fit(X_train, y_train)\n",
    "# without pruning\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_valid_hat = model.predict(X_valid)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_valid = (y_valid == y_valid_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"Training accuracy of the classification tree Without pruning is:\", acc_train)\n",
    "print(\"Validation accuracy of the classification tree Without pruning is:\", acc_valid)\n",
    "print(\"Testing accuracy of the classification tree Without pruning is:\", acc_test, '\\n')\n",
    "# with pruning\n",
    "model.tree = pruning(model.tree, model.classes_, X_valid, y_valid)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_valid_hat = model.predict(X_valid)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_valid = (y_valid == y_valid_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"Training accuracy of the classification tree With pruning is:\", acc_train)\n",
    "print(\"Validation accuracy of the classification tree With pruning is:\", acc_valid)\n",
    "print(\"Testing accuracy of the classification tree With pruning is:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Random Forest for Classification\n",
    "\n",
    "In this section, we implement the random forest where each tree is built with the class DecisionTreeClassifier(). In our model, the values of parameters are listed below. \n",
    "1. The number of trees $T$ is set as ``num_estimators = 20``\n",
    "2. the number of subsampled features for each tree is $k =\\sqrt{d}$, which corresponds to ``max_features = \"sqrt\"`` in the code.\n",
    "3. The maximum depth of each tree is ``max_depth = 6``.\n",
    "\n",
    "We will not use the pruning technique for each tree in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier(object):\n",
    "    '''\n",
    "    This class is for random forest classification\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of classification tree\n",
    "        - num_estimators: the number of trees in the random forest \n",
    "        - tree: a nested dictionary representing the decision tree structure\n",
    "        - max_depth: the parameter to control the depth of tree. If the depth is larger than max_depth, we will stop splitting.\n",
    "        - max_feature: the number of selected features to build decision tree\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 num_estimators,\n",
    "                 random_state,\n",
    "                 criterion=gini_index,\n",
    "                 max_depth=None,\n",
    "                 max_features=\"sqrt\"):\n",
    "        self.num_estimators = num_estimators\n",
    "        self.random_state = random_state\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        function used to fit all trees in the random forest\n",
    "        \n",
    "        Args:\n",
    "            X - the features of the training samples\n",
    "            y - the labels of the training samples\n",
    "        Returns:\n",
    "            self.model_list - the model list containing `num_estimators` tree models\n",
    "        '''\n",
    "        n, d = X.shape\n",
    "        RandomState = np.random.RandomState(self.random_state)\n",
    "        self.model_list = []\n",
    "        for t in range(self.num_estimators):\n",
    "            random_seed = RandomState.randint(0, np.iinfo(np.int32).max)\n",
    "            ### draw a bootstrapped dataset from X\n",
    "            sample_index = RandomState.choice(np.arange(n), size=n, replace=True)\n",
    "            X_sampled = X[sample_index, :]\n",
    "            y_sampled = y[sample_index]\n",
    "            ### initialize the tree model by using DecisionTreeClassifier()\n",
    "            model = DecisionTreeClassifier(criterion=self.criterion,\n",
    "                                           max_depth=self.max_depth,\n",
    "                                           max_features=self.max_features,\n",
    "                                           random_seed=random_seed)\n",
    "            ### fit the tree model to the bootstrapped dataset by using DecisionTreeClassifier.fit()\n",
    "            model.fit(X_sampled, y_sampled)\n",
    "            self.model_list.append(model)\n",
    "        return self.model_list\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to predict the labels of X\n",
    "        \n",
    "        Args:\n",
    "            X - the features of the test samples\n",
    "        Returns:\n",
    "            y_pred_label - the predicted labels of test samples\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y_pred = np.zeros([self.num_estimators, n], dtype=np.int32)\n",
    "        y_pred_label = np.zeros(n, dtype=np.int32)\n",
    "        ### use T tree classifiers to make predictions by using DecisionTreeClassifier.predict()\n",
    "        for i in range(self.num_estimators):\n",
    "            model_i = self.model_list[i]\n",
    "            y_pred[i, :] = model_i.predict(X)\n",
    "        ### take the majority vote \n",
    "        for i in range(n):\n",
    "            classes, count = np.unique(y_pred[:, i], return_counts=True)\n",
    "            y_pred_label[i] = classes[np.argmax(count)]\n",
    "        return y_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of the random forest is: 0.6626384692849949\n",
      "Testing accuracy of the random forest is: 0.6163265306122448\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = t_t()\n",
    "model = RandomForestClassifier(num_estimators=20,\n",
    "                                   random_state=101,\n",
    "                                   criterion=gini_index,\n",
    "                                   max_depth=6)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"Training accuracy of the random forest is:\", acc_train)\n",
    "print(\"Testing accuracy of the random forest is:\", acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
