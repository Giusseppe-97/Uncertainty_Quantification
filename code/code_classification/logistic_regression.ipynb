{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    '''\n",
    "    This is a class for Logistic Regression algorithm.\n",
    "    \n",
    "    The class contains the hyper parameters of the logistic regression algorithm as attributes.\n",
    "    It also contains the functions for initializing the class, fitting the ridge regression model and use the fitted \n",
    "    model to predict test samples.\n",
    "    \n",
    "    Attributes:\n",
    "        lr:        learning rate of gradient descent\n",
    "        max_itr:   maximum number of iteration for gradient descent\n",
    "        tol:       if the change in loss is smaller than tol, then we stop iteration\n",
    "        W:         concatenation of weight w and bias b\n",
    "        verbose:   whether or not print the value of logitic loss every 1000 iterations\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, lr=0.01, max_itr=100000, tol = 1e-5, verbose = False):\n",
    "        self.lr = lr\n",
    "        self.max_itr = max_itr\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    " \n",
    "    def __sigmoid(self, z):\n",
    "        '''\n",
    "        Define the Sigmoid function to convert from real value to [0,1]\n",
    "        \n",
    "        Args: \n",
    "            z (matrix, num_samples*1): scores or real value\n",
    "            \n",
    "        Returns:\n",
    "            A matrix (num_variables+1)*1: a value in the interval [0,1]\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __logistic_loss(self, h, y):\n",
    "        '''\n",
    "        Calculate the logistic loss\n",
    "        '''\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        estimate the weight and bias in the logistic regression model by gradient descent\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_train*num_variables): input of training samples\n",
    "            y (matrix, num_test*1): labels of training samples, 0 or 1\n",
    "            \n",
    "        Returns:\n",
    "            self.W (matrix, (num_variables+1)*1): estimation of weight and bias, i.e (w,b)\n",
    "        '''\n",
    "        ### Add the all-one vector to the last column \n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x, np.ones((m, 1))), axis=1)\n",
    "        y = y.reshape(-1,1)\n",
    "        # weight and bias initialization\n",
    "        d = X.shape[1]\n",
    "        self.W = np.zeros((d,1))\n",
    "        \n",
    "        z = np.dot(X, self.W)\n",
    "        h = self.__sigmoid(z)\n",
    "        previous_loss = self.__logistic_loss(h, y)\n",
    "        for i in range(self.max_itr):\n",
    "            #Calculate the gradient and update w and b\n",
    "            z = np.dot(X, self.W)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / m\n",
    "            self.W -= self.lr * gradient\n",
    "            \n",
    "            #Calculate the new logistic loss\n",
    "            z = np.dot(X, self.W)\n",
    "            h = self.__sigmoid(z)\n",
    "            current_loss = self.__logistic_loss(h, y)\n",
    "            if previous_loss - current_loss < self.tol:\n",
    "                print('Converged after {} iterations'.format(i+1))\n",
    "                print('Logistic loss after {} iterations is {}'.format(i+1,current_loss))\n",
    "                break\n",
    "            else:\n",
    "                previous_loss = current_loss\n",
    "            if(self.verbose == True and i % 10000 == 0):\n",
    "                print('Logistic loss after {} iterations is {}'.format(i+1,current_loss))\n",
    "        return self.W\n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        '''\n",
    "        predict the posterior probability p_1(x; W) of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted posterior probability p_1(x; W) of test samples\n",
    "        ''' \n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x, np.ones((m, 1))), axis=1)\n",
    "        return self.__sigmoid(np.dot(X, self.W))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        predict the label of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted labels of test samples, 0 or 1\n",
    "        ''' \n",
    "        return self.predict_prob(x).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet(dataset_path, file_type=\"txt\"):\n",
    "    if file_type == \"txt\":\n",
    "        X = []                                                        #create feature matrix\n",
    "        y = []                                                       # create label matrix\n",
    "        fr = open(dataset_path)                                            #open file\n",
    "        for line in fr.readlines():                                         #read datum\n",
    "            lineArr = line.strip().split()                                  #remove the `\\n` and obtain the data from string\n",
    "            X.append([float(x) for x in lineArr[:-1]])     # add to the feature matrix\n",
    "            y.append(float(lineArr[-1]))                                # add to the label matrix\n",
    "        fr.close()                                                          # close file\n",
    "        return X, y    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "import numpy as np\n",
    "X_train, y_train = loadDataSet(\"horseColicTraining.txt\")\n",
    "X_test, y_test = loadDataSet(\"horseColicTest.txt\")\n",
    "\n",
    "# transform the data from list to np.array\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# normalize\n",
    "X = np.vstack([X_train, X_test])\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss after 1 iterations is 0.6898762822641373\n",
      "Logistic loss after 10001 iterations is 0.5217484321994517\n",
      "Converged after 11802 iterations\n",
      "Logistic loss after 11802 iterations is 0.52171924670628\n",
      "[[ 0.7611359  -0.2002035   1.00541571 -2.51425541  0.82184505 -0.60528775\n",
      "  -0.36426046 -1.38660266 -0.16129892 -1.17278023  1.47758935 -0.60870321\n",
      "   1.39102436 -0.31464443 -0.98595971  0.58698705 -0.70142135 -0.4994333\n",
      "   1.04153942  0.06140696 -1.07582939  0.95021878]]\n",
      "The weight w of LR is \n",
      " [ 0.7611359  -0.2002035   1.00541571 -2.51425541  0.82184505 -0.60528775\n",
      " -0.36426046 -1.38660266 -0.16129892 -1.17278023  1.47758935 -0.60870321\n",
      "  1.39102436 -0.31464443 -0.98595971  0.58698705 -0.70142135 -0.4994333\n",
      "  1.04153942  0.06140696 -1.07582939].\n",
      "The bias b of LR is 0.9502187772809589.\n"
     ]
    }
   ],
   "source": [
    "### initiate the logistic regressor\n",
    "model = LogisticRegression(lr=0.1, max_itr=100000, tol = 1e-8, verbose=True)\n",
    "### fit the model with training data and get the estimation of parameters (w & b)\n",
    "W = model.fit(X_train, y_train)\n",
    "### Print the estimated w and b\n",
    "print(W.T)\n",
    "### Print the estimated w and b\n",
    "print(\"The weight w of LR is \\n {}.\".format(W[:X_test.shape[1],0].T))\n",
    "print(\"The bias b of LR is {}.\".format(W[X_test.shape[1],0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LR on the test dataset is 0.7164179104477612.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy = np.sum(y_pred[:,0] == y_test)/len(y_test)\n",
    "print(\"Accuracy of LR on the test dataset is {}.\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
