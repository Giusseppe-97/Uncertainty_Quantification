{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    '''\n",
    "    This is a class for Logistic Regression algorithm.\n",
    "    \n",
    "    The class contains the hyper parameters of the logistic regression algorithm as attributes.\n",
    "    It also contains the functions for initializing the class, fitting the ridge regression model and use the fitted \n",
    "    model to predict test samples.\n",
    "    \n",
    "    Attributes:\n",
    "        lr:        learning rate of gradient descent\n",
    "        max_itr:   maximum number of iteration for gradient descent\n",
    "        tol:       if the change in loss is smaller than tol, then we stop iteration\n",
    "        W:         concatenation of weight w and bias b\n",
    "        verbose:   whether or not print the value of logitic loss every 1000 iterations\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, lr=0.01, max_itr=100000, tol = 1e-5, verbose = False):\n",
    "        self.lr = lr\n",
    "        self.max_itr = max_itr\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    " \n",
    "    def __sigmoid(self, z):\n",
    "        '''\n",
    "        Define the Sigmoid function to convert from real value to [0,1]\n",
    "        \n",
    "        Args: \n",
    "            z (matrix, num_samples*1): scores or real value\n",
    "            \n",
    "        Returns:\n",
    "            A matrix (num_variables+1)*1: a value in the interval [0,1]\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __logistic_loss(self, h, y):\n",
    "        '''\n",
    "        Calculate the logistic loss\n",
    "        '''\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        estimate the weight and bias in the logistic regression model by gradient descent\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_train*num_variables): input of training samples\n",
    "            y (matrix, num_test*1): labels of training samples, 0 or 1\n",
    "            \n",
    "        Returns:\n",
    "            self.W (matrix, (num_variables+1)*1): estimation of weight and bias, i.e (w,b)\n",
    "        '''\n",
    "        ### Add the all-one vector to the last column \n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x, np.ones((m, 1))), axis=1)\n",
    "        y = y.reshape(-1,1)\n",
    "        # weight and bias initialization\n",
    "        d = X.shape[1]\n",
    "        self.W = np.zeros((d,1))\n",
    "        \n",
    "        z = np.dot(X, self.W)\n",
    "        h = self.__sigmoid(z)\n",
    "        previous_loss = self.__logistic_loss(h, y)\n",
    "        for i in range(self.max_itr):\n",
    "            #Calculate the gradient and update w and b\n",
    "            z = np.dot(X, self.W)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / m\n",
    "            self.W -= self.lr * gradient\n",
    "            \n",
    "            #Calculate the new logistic loss\n",
    "            z = np.dot(X, self.W)\n",
    "            h = self.__sigmoid(z)\n",
    "            current_loss = self.__logistic_loss(h, y)\n",
    "            if previous_loss - current_loss < self.tol:\n",
    "                print('Converged after {} iterations'.format(i+1))\n",
    "                print('Logistic loss after {} iterations is {}'.format(i+1,current_loss))\n",
    "                break\n",
    "            else:\n",
    "                previous_loss = current_loss\n",
    "            if(self.verbose == True and i % 10000 == 0):\n",
    "                print('Logistic loss after {} iterations is {}'.format(i+1,current_loss))\n",
    "        return self.W\n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        '''\n",
    "        predict the posterior probability p_1(x; W) of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted posterior probability p_1(x; W) of test samples\n",
    "        ''' \n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x, np.ones((m, 1))), axis=1)\n",
    "        return self.__sigmoid(np.dot(X, self.W))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        predict the label of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted labels of test samples, 0 or 1\n",
    "        ''' \n",
    "        return self.predict_prob(x).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\data\\data_classification\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    print(data_path)\n",
    "    df = pd.read_csv(data_path)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return data_X, data_y\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # model-specified parameters\n",
    "    n_estimators = 5\n",
    "\n",
    "    # read dataset from csv file\n",
    "    data_name = \"messidor_classification\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "    pd.csv.split(5)\n",
    "\n",
    "    # CrossValidation with 5 splits\n",
    "    kf = KFold(n_splits=5)\n",
    "    res_list = []\n",
    "    for train_index, test_index in kf.split(data_X):\n",
    "        train_X, train_y = data_X[train_index,:], data_y[train_index]\n",
    "        test_X, test_y = data_X[test_index,:], data_y[test_index]\n",
    "    \n",
    "    return train_X, train_y,test_X, test_y\n",
    "\n",
    "def run():\n",
    "    ### initiate the logistic regressor\n",
    "    model = LogisticRegression(lr=0.1, max_itr=100000, tol = 1e-8, verbose=True)\n",
    "    ### fit the model with training data and get the estimation of parameters (w & b)\n",
    "    W = model.fit(main()[0], main()[1])\n",
    "    ### Print the estimated w and b\n",
    "    print(W.T)\n",
    "    ### Print the estimated w and b\n",
    "    print(\"The weight w of LR is \\n {}.\".format(W[:main()[2].shape[1],0].T))\n",
    "    print(\"The bias b of LR is {}.\".format(W[main()[2].shape[1],0]))\n",
    "\n",
    "    y_pred = model.predict(main()[2])\n",
    "    accuracy = np.sum(y_pred[:,0] == main()[3])/len(main()[3])\n",
    "    print(\"Accuracy of LR on the test dataset is {}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\..\\data\\data_classification\\messidor_classification.csv\n",
      "..\\..\\data\\data_classification\\messidor_classification.csv\n",
      "Logistic loss after 1 iterations is 0.6850519586171843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_20392\\853448875.py:39: RuntimeWarning: divide by zero encountered in log\n",
      "  return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_20392\\853448875.py:39: RuntimeWarning: invalid value encountered in multiply\n",
      "  return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss after 10001 iterations is nan\n",
      "Logistic loss after 20001 iterations is nan\n",
      "Logistic loss after 30001 iterations is nan\n",
      "Logistic loss after 40001 iterations is nan\n",
      "Logistic loss after 50001 iterations is nan\n",
      "Logistic loss after 60001 iterations is nan\n",
      "Logistic loss after 70001 iterations is nan\n",
      "Logistic loss after 80001 iterations is nan\n",
      "Logistic loss after 90001 iterations is nan\n",
      "[[ 5.39305776e-01 -3.02237216e-01  1.73272888e+01 -3.86641401e+00\n",
      "  -8.11513173e+00 -3.18147228e+00 -1.20821519e+00  8.77367275e-01\n",
      "   5.83633193e-01 -4.78478074e-01  1.07891459e-01 -6.28442453e-01\n",
      "   4.30643900e-01 -2.14016203e+00  3.99311985e+00  4.64742622e-01\n",
      "   4.98400131e-03 -4.80248669e-02 -5.19470564e-02  1.39062746e+00]]\n",
      "..\\..\\data\\data_classification\\messidor_classification.csv\n",
      "The weight w of LR is \n",
      " [ 5.39305776e-01 -3.02237216e-01  1.73272888e+01 -3.86641401e+00\n",
      " -8.11513173e+00 -3.18147228e+00 -1.20821519e+00  8.77367275e-01\n",
      "  5.83633193e-01 -4.78478074e-01  1.07891459e-01 -6.28442453e-01\n",
      "  4.30643900e-01 -2.14016203e+00  3.99311985e+00  4.64742622e-01\n",
      "  4.98400131e-03 -4.80248669e-02 -5.19470564e-02].\n",
      "..\\..\\data\\data_classification\\messidor_classification.csv\n",
      "The bias b of LR is 1.3906274552613553.\n",
      "..\\..\\data\\data_classification\\messidor_classification.csv\n",
      "..\\..\\data\\data_classification\\messidor_classification.csv\n",
      "..\\..\\data\\data_classification\\messidor_classification.csv\n",
      "Accuracy of LR on the test dataset is 0.7391304347826086.\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
