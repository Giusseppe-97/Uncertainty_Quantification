{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodings import search_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    '''\n",
    "    This is a class for Logistic Regression algorithm.\n",
    "    \n",
    "    The class contains the hyper parameters of the logistic regression algorithm as attributes.\n",
    "    It also contains the functions for initializing the class, fitting the ridge regression model and use the fitted \n",
    "    model to predict test samples.\n",
    "    \n",
    "    Attributes:\n",
    "        lr:        learning rate of gradient descent\n",
    "        max_itr:   maximum number of iteration for gradient descent\n",
    "        tol:       if the change in loss is smaller than tol, then we stop iteration\n",
    "        W:         concatenation of weight w and bias b\n",
    "        verbose:   whether or not print the value of logitic loss every 1000 iterations\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, lr=0.01, max_itr=100000, tol = 1e-5, verbose = False):\n",
    "        self.lr = lr\n",
    "        self.max_itr = max_itr\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    " \n",
    "    def __sigmoid(self, z):\n",
    "        '''\n",
    "        Define the Sigmoid function to convert from real value to [0,1]\n",
    "        \n",
    "        Args: \n",
    "            z (matrix, num_samples*1): scores or real value\n",
    "            \n",
    "        Returns:\n",
    "            A matrix (num_variables+1)*1: a value in the interval [0,1]\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __logistic_loss(self, h, y):\n",
    "        '''\n",
    "        Calculate the logistic loss\n",
    "        '''\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        estimate the weight and bias in the logistic regression model by gradient descent\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_train*num_variables): input of training samples\n",
    "            y (matrix, num_test*1): labels of training samples, 0 or 1\n",
    "            \n",
    "        Returns:\n",
    "            self.W (matrix, (num_variables+1)*1): estimation of weight and bias, i.e (w,b)\n",
    "        '''\n",
    "        ### Add the all-one vector to the last column \n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x, np.ones((m, 1))), axis=1)\n",
    "        y = y.reshape(-1,1)\n",
    "        # weight and bias initialization\n",
    "        d = X.shape[1]\n",
    "        self.W = np.zeros((d,1))\n",
    "        \n",
    "        z = np.dot(X, self.W)\n",
    "        h = self.__sigmoid(z)\n",
    "        previous_loss = self.__logistic_loss(h, y)\n",
    "        for i in range(self.max_itr):\n",
    "            ######################################\n",
    "            ######################################\n",
    "            ####### Write your codes below #######\n",
    "            ### Calculate the gradient and update self.W\n",
    "            p1 = np.divide(np.exp(np.dot(X, self.W)),(1+np.exp(np.dot(X,self.W))))\n",
    "            \n",
    "            grad = - 1/m * (np.dot(np.transpose(X),y-p1))\n",
    "\n",
    "            self.W = self.W - self.lr * grad\n",
    "\n",
    "            ######################################\n",
    "            ######################################\n",
    "            \n",
    "            #Calculate the new logistic loss\n",
    "            z = np.dot(X, self.W)\n",
    "            h = self.__sigmoid(z)\n",
    "            current_loss = self.__logistic_loss(h, y)\n",
    "            if previous_loss - current_loss < self.tol:\n",
    "                print('Converged after {} iterations'.format(i+1))\n",
    "                print('Logistic loss after {} iterations is {}'.format(i+1,current_loss))\n",
    "                break\n",
    "            else:\n",
    "                previous_loss = current_loss\n",
    "            if(self.verbose == True and i % 10000 == 0):\n",
    "                print('Logistic loss after {} iterations is {}'.format(i+1,current_loss))\n",
    "        return self.W\n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        '''\n",
    "        predict the posterior probability p_1(x; W) of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted posterior probability p_1(x; W) of test samples\n",
    "        ''' \n",
    "        ######################################\n",
    "        ######################################\n",
    "        ####### Write your codes below #######\n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x,np.ones((m,1))),axis=1)    \n",
    "        y = np.divide(np.exp(np.dot(X, self.W)),(1+np.exp(np.dot(X,self.W))))\n",
    "        \n",
    "        return y\n",
    "        ######################################\n",
    "        ######################################\n",
    "    \n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        predict the label of the test samples\n",
    "        \n",
    "        Args: \n",
    "            x (matrix, num_test*num_variables): input of test samples\n",
    "            \n",
    "        Returns:\n",
    "            y (matrix, num_test*1): predicted labels of test samples, 0 or 1\n",
    "        ''' \n",
    "        ######################################\n",
    "        ######################################\n",
    "        ####### Write your codes below #######\n",
    "        m = x.shape[0]\n",
    "        X = np.concatenate((x,np.ones((m,1))),axis=1)\n",
    "        y = 1 / (1 + np.exp(-np.dot(X,self.W)))\n",
    "        y = np.where(y>0.5,1,0)\n",
    "        \n",
    "        return y\n",
    "        ######################################\n",
    "        ######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return data_X, data_y\n",
    "\n",
    "def main():\n",
    "\n",
    "    # read dataset from csv file\n",
    "    data_name = \"messidor_classification\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "\n",
    "    # # CrossValidation with 5 splits\n",
    "    # # model-specified parameters\n",
    "    # n_estimators = 5\n",
    "    # kf = KFold(n_splits=5)\n",
    "    # res_list = []\n",
    "    # for train_index, test_index in kf.split(data_X):\n",
    "    #     train_X, train_y = data_X[train_index,:], data_y[train_index]\n",
    "    #     test_X, test_y = data_X[test_index,:], data_y[test_index]\n",
    "\n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.33, random_state=2200)\n",
    "    \n",
    "    ### initiate the logistic regressor\n",
    "    model = LogisticRegression(lr=1, max_itr=100000, tol = 1e-8, verbose=True)\n",
    "    ### fit the model with training data and get the estimation of parameters (w & b)\n",
    "    W = model.fit(train_X, train_y)\n",
    "    ### Print the estimated w and b\n",
    "    print(W.T)\n",
    "    ### Print the estimated w and b\n",
    "    print(\"The weight w of LR is \\n {}.\".format(W[:test_X.shape[1],0].T))\n",
    "    print(\"The bias b of LR is {}.\".format(W[test_X.shape[1],0]))\n",
    "\n",
    "    y_pred = model.predict(test_X)\n",
    "    accuracy = np.sum(y_pred[:,0] == test_y)/len(test_y)\n",
    "    print(\"Accuracy of LR on the test dataset is {}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss after 1 iterations is 0.6838623451102486\n",
      "Logistic loss after 10001 iterations is 0.5100447489191648\n",
      "Logistic loss after 20001 iterations is 0.49833145587368816\n",
      "Logistic loss after 30001 iterations is 0.49234880920929286\n",
      "Logistic loss after 40001 iterations is 0.4887060952482046\n",
      "Logistic loss after 50001 iterations is 0.48628621226189894\n",
      "Logistic loss after 60001 iterations is 0.4845822765472486\n",
      "Logistic loss after 70001 iterations is 0.4833318116095802\n",
      "Logistic loss after 80001 iterations is 0.48238539816737697\n",
      "Logistic loss after 90001 iterations is 0.4816515366397042\n",
      "[[ 0.52244598 -0.29337878 17.03643152 -3.30565035 -9.06976625 -3.00483606\n",
      "  -0.21214185  0.3362852   0.56957169 -0.21787326 -0.07485432 -0.59727419\n",
      "   0.59130027 -1.73886879  1.80174032  0.50608556  0.08228581 -0.26506719\n",
      "  -0.2188865   1.08788073]]\n",
      "The weight w of LR is \n",
      " [ 0.52244598 -0.29337878 17.03643152 -3.30565035 -9.06976625 -3.00483606\n",
      " -0.21214185  0.3362852   0.56957169 -0.21787326 -0.07485432 -0.59727419\n",
      "  0.59130027 -1.73886879  1.80174032  0.50608556  0.08228581 -0.26506719\n",
      " -0.2188865 ].\n",
      "The bias b of LR is 1.0878807251338636.\n",
      "Accuracy of LR on the test dataset is 0.7657894736842106.\n"
     ]
    }
   ],
   "source": [
    "# test_size=0.33, random_state=2200, lr=0.1, max_itr=100000, tol = 1e-8, verbose=True\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss after 1 iterations is 0.6921641748267755\n",
      "Logistic loss after 10001 iterations is 0.5555033483898666\n",
      "Logistic loss after 20001 iterations is 0.542446259552765\n",
      "Logistic loss after 30001 iterations is 0.5340283371704706\n",
      "Logistic loss after 40001 iterations is 0.527955115531314\n",
      "Logistic loss after 50001 iterations is 0.5233343289308922\n",
      "Logistic loss after 60001 iterations is 0.5196705066902175\n",
      "Logistic loss after 70001 iterations is 0.5166664586948004\n",
      "Logistic loss after 80001 iterations is 0.5141356876108761\n",
      "Logistic loss after 90001 iterations is 0.5119565769119439\n",
      "[[ 0.39141833 -0.26602959  6.12217218  1.34022129 -2.14053075 -2.90379363\n",
      "  -1.75791881  0.47324419  0.55613116 -0.2982995   0.06378874 -0.64538668\n",
      "   0.54006301 -1.08652891  1.03865021  0.87287042  0.06689074 -0.2864109\n",
      "  -0.20862822  0.69042134]]\n",
      "The weight w of LR is \n",
      " [ 0.39141833 -0.26602959  6.12217218  1.34022129 -2.14053075 -2.90379363\n",
      " -1.75791881  0.47324419  0.55613116 -0.2982995   0.06378874 -0.64538668\n",
      "  0.54006301 -1.08652891  1.03865021  0.87287042  0.06689074 -0.2864109\n",
      " -0.20862822].\n",
      "The bias b of LR is 0.6904213407338207.\n",
      "Accuracy of LR on the test dataset is 0.7421052631578947.\n"
     ]
    }
   ],
   "source": [
    "# test_size=0.33, random_state=2200, lr=0.01, max_itr=100000, tol = 1e-8, verbose=True\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss after 1 iterations is 0.6921641748267755\n",
      "Logistic loss after 10001 iterations is 0.5555033483898666\n",
      "Logistic loss after 20001 iterations is 0.542446259552765\n",
      "Logistic loss after 30001 iterations is 0.5340283371704706\n",
      "Logistic loss after 40001 iterations is 0.527955115531314\n",
      "Logistic loss after 50001 iterations is 0.5233343289308922\n",
      "Logistic loss after 60001 iterations is 0.5196705066902175\n",
      "Logistic loss after 70001 iterations is 0.5166664586948004\n",
      "Logistic loss after 80001 iterations is 0.5141356876108761\n",
      "Logistic loss after 90001 iterations is 0.5119565769119439\n",
      "Logistic loss after 100001 iterations is 0.5100470636570057\n",
      "Logistic loss after 110001 iterations is 0.5083500496768416\n",
      "Logistic loss after 120001 iterations is 0.5068246841432131\n",
      "Logistic loss after 130001 iterations is 0.5054409811540579\n",
      "Logistic loss after 140001 iterations is 0.5041763991833677\n",
      "Logistic loss after 150001 iterations is 0.5030136115342634\n",
      "Logistic loss after 160001 iterations is 0.5019390202560364\n",
      "Logistic loss after 170001 iterations is 0.5009417456651308\n",
      "Logistic loss after 180001 iterations is 0.5000129267333496\n",
      "Logistic loss after 190001 iterations is 0.49914522856053395\n",
      "Logistic loss after 200001 iterations is 0.4983324901471425\n",
      "Logistic loss after 210001 iterations is 0.49756946867916496\n",
      "Logistic loss after 220001 iterations is 0.4968516511370874\n",
      "Logistic loss after 230001 iterations is 0.49617511348381205\n",
      "Logistic loss after 240001 iterations is 0.49553641389659914\n",
      "Logistic loss after 250001 iterations is 0.4949325106526519\n",
      "Logistic loss after 260001 iterations is 0.49436069808047695\n",
      "Logistic loss after 270001 iterations is 0.4938185559066348\n",
      "Logistic loss after 280001 iterations is 0.49330390865351514\n",
      "Logistic loss after 290001 iterations is 0.492814792669799\n",
      "Logistic loss after 300001 iterations is 0.4923494290278777\n",
      "Logistic loss after 310001 iterations is 0.4919062009863907\n",
      "Logistic loss after 320001 iterations is 0.49148363504853926\n",
      "Logistic loss after 330001 iterations is 0.49108038488708117\n",
      "Logistic loss after 340001 iterations is 0.49069521758190754\n",
      "Logistic loss after 350001 iterations is 0.49032700174460614\n",
      "Logistic loss after 360001 iterations is 0.48997469719956416\n",
      "Logistic loss after 370001 iterations is 0.4896373459622163\n",
      "Logistic loss after 380001 iterations is 0.4893140643085645\n",
      "Logistic loss after 390001 iterations is 0.48900403577079865\n",
      "Logistic loss after 400001 iterations is 0.4887065049250685\n",
      "Logistic loss after 410001 iterations is 0.48842077186166577\n",
      "Logistic loss after 420001 iterations is 0.488146187246818\n",
      "Logistic loss after 430001 iterations is 0.48788214790026685\n",
      "Logistic loss after 440001 iterations is 0.48762809282477154\n",
      "Logistic loss after 450001 iterations is 0.48738349963332234\n",
      "Logistic loss after 460001 iterations is 0.48714788132770553\n",
      "Logistic loss after 470001 iterations is 0.4869207833885261\n",
      "Logistic loss after 480001 iterations is 0.48670178114215795\n",
      "Logistic loss after 490001 iterations is 0.48649047737456896\n",
      "Logistic loss after 500001 iterations is 0.48628650016575825\n",
      "Logistic loss after 510001 iterations is 0.48608950092175096\n",
      "Logistic loss after 520001 iterations is 0.4858991525838461\n",
      "Logistic loss after 530001 iterations is 0.4857151479971703\n",
      "Logistic loss after 540001 iterations is 0.4855371984226397\n",
      "Logistic loss after 550001 iterations is 0.48536503217820026\n",
      "Logistic loss after 560001 iterations is 0.48519839339677007\n",
      "Logistic loss after 570001 iterations is 0.4850370408896613\n",
      "Logistic loss after 580001 iterations is 0.4848807471054518\n",
      "Logistic loss after 590001 iterations is 0.48472929717532764\n",
      "Logistic loss after 600001 iterations is 0.4845824880368453\n",
      "Logistic loss after 610001 iterations is 0.48444012762888516\n",
      "Logistic loss after 620001 iterations is 0.4843020341513\n",
      "Logistic loss after 630001 iterations is 0.48416803538340747\n",
      "Logistic loss after 640001 iterations is 0.48403796805605986\n",
      "Logistic loss after 650001 iterations is 0.4839116772725359\n",
      "Logistic loss after 660001 iterations is 0.4837890159739644\n",
      "Logistic loss after 670001 iterations is 0.4836698444454008\n",
      "Logistic loss after 680001 iterations is 0.48355402985904805\n",
      "Logistic loss after 690001 iterations is 0.48344144585144694\n",
      "Logistic loss after 700001 iterations is 0.4833319721317537\n",
      "Logistic loss after 710001 iterations is 0.4832254941185002\n",
      "Logistic loss after 720001 iterations is 0.48312190260246224\n",
      "Converged after 727965 iterations\n",
      "Logistic loss after 727965 iterations is 0.48304139749768904\n",
      "[[ 0.50529622 -0.2919409  15.30617368 -2.04992194 -8.31020546 -3.38982306\n",
      "  -0.23061164  0.37104243  0.56763372 -0.22658371 -0.06201778 -0.59386307\n",
      "   0.58709399 -1.72605181  1.79342159  0.52495256  0.08338623 -0.26986911\n",
      "  -0.21843889  1.03522323]]\n",
      "The weight w of LR is \n",
      " [ 0.50529622 -0.2919409  15.30617368 -2.04992194 -8.31020546 -3.38982306\n",
      " -0.23061164  0.37104243  0.56763372 -0.22658371 -0.06201778 -0.59386307\n",
      "  0.58709399 -1.72605181  1.79342159  0.52495256  0.08338623 -0.26986911\n",
      " -0.21843889].\n",
      "The bias b of LR is 1.0352232329915128.\n",
      "Accuracy of LR on the test dataset is 0.7657894736842106.\n"
     ]
    }
   ],
   "source": [
    "# test_size=0.33, random_state=2200, lr=0.01, max_itr=1000000, tol = 1e-8, verbose=True\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic loss after 1 iterations is 0.6503645821819763\n",
      "Logistic loss after 10001 iterations is 0.481070252852604\n",
      "Logistic loss after 20001 iterations is 0.478701109161681\n",
      "Logistic loss after 30001 iterations is 0.4781392787274469\n",
      "Converged after 39261 iterations\n",
      "Logistic loss after 39261 iterations is 0.4779747259564311\n",
      "[[ 0.5963301  -0.28901531 22.62791265 -9.19124023 -9.01601166 -2.3929928\n",
      "  -0.29851959  0.27779321  0.57521055 -0.19728158 -0.10612059 -0.59066354\n",
      "   0.55910458 -1.63791733  1.60915241  0.57491034  0.07528688 -0.25517917\n",
      "  -0.21667281  1.23362117]]\n",
      "The weight w of LR is \n",
      " [ 0.5963301  -0.28901531 22.62791265 -9.19124023 -9.01601166 -2.3929928\n",
      " -0.29851959  0.27779321  0.57521055 -0.19728158 -0.10612059 -0.59066354\n",
      "  0.55910458 -1.63791733  1.60915241  0.57491034  0.07528688 -0.25517917\n",
      " -0.21667281].\n",
      "The bias b of LR is 1.2336211669052164.\n",
      "Accuracy of LR on the test dataset is 0.7684210526315789.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
