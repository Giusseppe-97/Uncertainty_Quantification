{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "np.random.seed(102)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the Gaussian kernel (RBF kernel) for SVM. For any $x_i, x_j \\in\\mathbb{R}^d$, we define\n",
    "$$\n",
    "\\kappa(x_i, x_j) = \\exp \\bigl( - \\|x_i - x_j\\|^2 / (2 \\sigma^2) \\bigr)\n",
    "$$ \n",
    "where $\\sigma > 0$ is the width of the Gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM():\n",
    "    '''\n",
    "    This is a class for support vector machine.\n",
    "    \n",
    "    The class contains the hyper-parameters such as $C$ and the kernel bandwidth $\\sigma$. It also contains \n",
    "    the alpha vector, the tolerance for prediction error and the maximum number of iteration.\n",
    "    \n",
    "    It contains the functions for calculating the kernel matrix, fitting the model to estimate alpha and b \n",
    "    with SMO algorithm, making predictions and other fundamental functions.\n",
    "    \n",
    "    Attributes:\n",
    "        C (positive number)         - the hyperparameter for SVM algorithm\n",
    "        sigma (positive number)     - the kernel bandwidth $\\sigma$ of Gaussian kernel \n",
    "        toler (positive number)     - the threshold value of prediction error. If the prediction error of \n",
    "                                      a sample is larger than this value, the corresponding alpha_i will be \n",
    "                                      probably updated.\n",
    "        maxIter (positive integer)  - the maximum number of iteration to search a pair of alpha's to update\n",
    "        alphas (vector, num_samples)- the alpha vector in the dual problem \n",
    "        b (number)                  - the bias b\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, C = 1, sigma = 1, toler = 1, maxIter = 10):\n",
    "        self.C = C\n",
    "        self.sigma = sigma\n",
    "        self.toler = toler\n",
    "        self.maxIter = maxIter\n",
    "        self.alphas = 0\n",
    "        self.b = 0\n",
    "        \n",
    "    def rbfkernel(self, X, Y):\n",
    "        '''\n",
    "        Calculate the kernel matrix whose (i,j)-th entry is $k(X[i,:], Y[j,:])$.\n",
    "        '''\n",
    "        m = X.shape[0]\n",
    "        n = Y.shape[0]\n",
    "        K = np.zeros(shape=(m, n))\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                K[i, j] = np.sum((X[i,:] - Y[j,:])**2)\n",
    "        K = np.exp(-K / (2 * self.sigma**2))\n",
    "        return K\n",
    "\n",
    "    def selectJrand(self, i, m):\n",
    "        '''\n",
    "        Randomly choose an index $j\\neq i$ from 0 to m-1\n",
    "        '''\n",
    "        j = i \n",
    "        while (j == i):\n",
    "            j = int(np.random.uniform(0, m))\n",
    "        return j\n",
    "\n",
    "\n",
    "    def clipAlpha(self, aj, H, L):\n",
    "        '''\n",
    "        Clip the vale aj by the lower bound L and upper bound H\n",
    "        '''\n",
    "        if aj > H:\n",
    "            aj = H\n",
    "        if L > aj:\n",
    "            aj = L\n",
    "        return aj\n",
    "\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        estimate the alphas vector and bias in the SVM model\n",
    "        \n",
    "        Args: \n",
    "            X_train (matrix, num_train*num_features): features of training samples\n",
    "            y_train (vector, num_train): label of training samples, each label is either -1 or 1\n",
    "            \n",
    "        Returns:\n",
    "            self.b (a number)                 : the bias\n",
    "            self.alphas (vector, num_features): the alpha vector \n",
    "        ''' \n",
    "        K_train = self.rbfkernel(X_train, X_train)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        m = K_train.shape[0]\n",
    "        self.alphas = np.zeros((m, ))\n",
    "        num_iter = 0\n",
    "        while (num_iter < self.maxIter):\n",
    "            alphaPairsChanged = 0\n",
    "            # optimize for each data vector (with kernel trick)\n",
    "            for i in range(m):   \n",
    "                fXi = (self.alphas * y_train) @ K_train[i, :] + self.b\n",
    "                # if checks if an example violates KKT conditions\n",
    "                Ei = fXi - y_train[i]\n",
    "                if ((y_train[i] * Ei < -self.toler) and (self.alphas[i] < self.C)) \\\n",
    "                       or ((y_train[i] * Ei > self.toler) and (self.alphas[i] > 0)):\n",
    "                    j = self.selectJrand(i, m)\n",
    "                    fXj = (self.alphas * y_train) @ K_train[j, :] + self.b\n",
    "                    Ej = fXj - y_train[j]\n",
    "                    alphaJold = self.alphas[j].copy()\n",
    "                    alphaIold = self.alphas[i].copy()\n",
    "                    if (y_train[j] != y_train[i]):\n",
    "                        L = max(0, self.alphas[i] - self.alphas[j])\n",
    "                        H = min(self.C, self.C + self.alphas[i] - self.alphas[j])\n",
    "                    else:\n",
    "                        L = max(0, self.alphas[i] + self.alphas[j] - self.C)\n",
    "                        H = min(self.C, self.alphas[i] + self.alphas[j])\n",
    "                    if L == H:\n",
    "                        continue\n",
    "                    eta = 2.0 * K_train[j, i] - K_train[j, j] - K_train[i, i]\n",
    "                    if eta >= 0:\n",
    "                        continue\n",
    "                    self.alphas[i] += y_train[i] * (Ei - Ej) / eta\n",
    "                    self.alphas[i] = self.clipAlpha(self.alphas[i], H, L)\n",
    "                    if (abs(self.alphas[i] - alphaIold) < 0.00001):\n",
    "                        continue\n",
    "                    # update i by the same amount as j, the direction depends on y[i] and y[j]\n",
    "                    self.alphas[j] += y_train[i] * y_train[j] * (alphaIold - self.alphas[i])\n",
    "                    # update self.b\n",
    "                    b1 = self.b - Ej - y_train[j] * (self.alphas[j] - alphaJold) * K_train[\n",
    "                        j, j] - y_train[i] * (self.alphas[i] - alphaIold) * K_train[j, i]\n",
    "                    b2 = self.b - Ei - y_train[j] * (self.alphas[j] - alphaJold) * K_train[\n",
    "                        j, i] - y_train[i] * (self.alphas[i] - alphaIold) * K_train[i, i]\n",
    "                    if (0 < self.alphas[j]) and (self.C > self.alphas[j]): self.b = b1\n",
    "                    elif (0 < self.alphas[i]) and (self.C > self.alphas[i]): self.b = b2\n",
    "                    else: self.b = (b1 + b2) / 2.0\n",
    "                    alphaPairsChanged += 1\n",
    "            if (alphaPairsChanged == 0): num_iter += 1\n",
    "            else: num_iter = 0\n",
    "        return self.b, self.alphas\n",
    "\n",
    "\n",
    "    def predict(self,X_test):\n",
    "        '''\n",
    "        predict the label of test samples\n",
    "        Args:\n",
    "            X_test(matrix, num_test*num_features): features of test samples\n",
    "        Returns:\n",
    "            y_hat(vector, num_test): the predicted label of test samples, each label is either -1 or 1\n",
    "        '''\n",
    "        K_test = self.rbfkernel(self.X_train, X_test)\n",
    "        f = K_test.T @ (self.alphas * self.y_train) + self.b\n",
    "        y_hat = np.sign(f)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SVM on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loadDataSet(dataset_path):\n",
    "#     data = pd.read_table(dataset_path, header=None)\n",
    "#     data_X, data_y = data.iloc[:, :-1], data.iloc[:, -1]\n",
    "#     data_X = np.array(data_X, dtype=np.float32)\n",
    "#     data_y = np.array(data_y, dtype=np.int64)\n",
    "#     data_y[data_y == 0] = -1\n",
    "#     return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the data\n",
    "# X_train, y_train = loadDataSet(\"horseColicTraining.txt\") \n",
    "# X_test, y_test = loadDataSet(\"horseColicTest.txt\")\n",
    "# # normalize\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return data_X, data_y\n",
    "\n",
    "def main():\n",
    "\n",
    "    # read dataset from csv file\n",
    "    data_name = \"messidor_classification\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "    # pd.csv.split(5)\n",
    "\n",
    "    # Train and test set\n",
    "    kf = KFold(n_splits=10)\n",
    "    res_list = []\n",
    "    for train_index, test_index in kf.split(data_X):\n",
    "        train_X, train_y = data_X[train_index,:], data_y[train_index]\n",
    "        test_X, test_y = data_X[test_index,:], data_y[test_index]\n",
    "    \n",
    "    return train_X, train_y,test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    # running the model and getting the accuracy\n",
    "    model_SVM = SVM(C = 0.5, sigma = 2, toler=0.001, maxIter=40)\n",
    "    b, alphas = model_SVM.fit(main()[0], main()[1])\n",
    "    y_test_hat = model_SVM.predict(main()[2])\n",
    "    accuracy = np.mean(y_test_hat == main()[3])\n",
    "    print(\"The accuracy of SVM is:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.90538595e-02  2.98212905e-01 -6.41486299e-01 ... -1.29476283e+00\n",
      "  -4.68655678e-01  1.40504812e+00]\n",
      " [ 5.90538595e-02  2.98212905e-01 -5.63391135e-01 ... -8.21678582e-02\n",
      "   2.00605415e+00 -7.11719399e-01]\n",
      " [ 5.90538595e-02  2.98212905e-01  9.20416985e-01 ...  2.74282645e-01\n",
      "   1.12151640e+00 -7.11719399e-01]\n",
      " ...\n",
      " [ 5.90538595e-02  2.98212905e-01 -1.11005728e+00 ...  1.65190892e+00\n",
      "  -1.05576440e+00  1.40504812e+00]\n",
      " [ 5.90538595e-02  2.98212905e-01  4.12798418e-01 ... -1.19858966e+00\n",
      "  -3.96554271e-02 -7.11719399e-01]\n",
      " [ 5.90538595e-02  2.98212905e-01  2.95655672e-01 ...  1.20361803e-03\n",
      "   7.03443253e-01  1.40504812e+00]]\n",
      "The accuracy of SVM is: 0.043478260869565216\n"
     ]
    }
   ],
   "source": [
    "print(main()[0])\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
