{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from encodings import search_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import necessary tools from the sklearn library\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "# Import sklearn library tools used ONLY for validating my results\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score, recall_score, precision_score, auc, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import randint as sp_randint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # read dataset from csv file\n",
    "    data_name = \"abalone_classification\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "\n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.33, random_state=2200)\n",
    "    return train_X, test_X, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-5.834718e-16</td>\n",
       "      <td>-3.027929e-16</td>\n",
       "      <td>3.912493e-16</td>\n",
       "      <td>9.185853e-17</td>\n",
       "      <td>-1.020650e-17</td>\n",
       "      <td>2.704723e-16</td>\n",
       "      <td>2.976897e-16</td>\n",
       "      <td>-4.252710e-17</td>\n",
       "      <td>-7.144552e-17</td>\n",
       "      <td>1.169495e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.739154e+00</td>\n",
       "      <td>-3.556267e+00</td>\n",
       "      <td>-3.335953e+00</td>\n",
       "      <td>-1.686092e+00</td>\n",
       "      <td>-1.614731e+00</td>\n",
       "      <td>-1.643173e+00</td>\n",
       "      <td>-1.705134e+00</td>\n",
       "      <td>-6.748338e-01</td>\n",
       "      <td>-6.880179e-01</td>\n",
       "      <td>-7.594876e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.161975e-01</td>\n",
       "      <td>-5.833158e-01</td>\n",
       "      <td>-5.862075e-01</td>\n",
       "      <td>-7.897577e-01</td>\n",
       "      <td>-7.811585e-01</td>\n",
       "      <td>-7.946415e-01</td>\n",
       "      <td>-7.819095e-01</td>\n",
       "      <td>-6.748338e-01</td>\n",
       "      <td>-6.880179e-01</td>\n",
       "      <td>-7.594876e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.749513e-01</td>\n",
       "      <td>1.725193e-01</td>\n",
       "      <td>1.156329e-02</td>\n",
       "      <td>-5.963767e-02</td>\n",
       "      <td>-1.052891e-01</td>\n",
       "      <td>-8.753202e-02</td>\n",
       "      <td>-3.470794e-02</td>\n",
       "      <td>-6.748338e-01</td>\n",
       "      <td>-6.880179e-01</td>\n",
       "      <td>-7.594876e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.579031e-01</td>\n",
       "      <td>7.267984e-01</td>\n",
       "      <td>6.093341e-01</td>\n",
       "      <td>6.613049e-01</td>\n",
       "      <td>6.426730e-01</td>\n",
       "      <td>6.606355e-01</td>\n",
       "      <td>6.478319e-01</td>\n",
       "      <td>1.481846e+00</td>\n",
       "      <td>1.453451e+00</td>\n",
       "      <td>1.316677e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.423480e+00</td>\n",
       "      <td>2.440025e+00</td>\n",
       "      <td>2.368329e+01</td>\n",
       "      <td>4.072271e+00</td>\n",
       "      <td>5.085388e+00</td>\n",
       "      <td>5.286500e+00</td>\n",
       "      <td>5.504642e+00</td>\n",
       "      <td>1.481846e+00</td>\n",
       "      <td>1.453451e+00</td>\n",
       "      <td>1.316677e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0             1             2             3             4  \\\n",
       "count  4.177000e+03  4.177000e+03  4.177000e+03  4.177000e+03  4.177000e+03   \n",
       "mean  -5.834718e-16 -3.027929e-16  3.912493e-16  9.185853e-17 -1.020650e-17   \n",
       "std    1.000120e+00  1.000120e+00  1.000120e+00  1.000120e+00  1.000120e+00   \n",
       "min   -3.739154e+00 -3.556267e+00 -3.335953e+00 -1.686092e+00 -1.614731e+00   \n",
       "25%   -6.161975e-01 -5.833158e-01 -5.862075e-01 -7.897577e-01 -7.811585e-01   \n",
       "50%    1.749513e-01  1.725193e-01  1.156329e-02 -5.963767e-02 -1.052891e-01   \n",
       "75%    7.579031e-01  7.267984e-01  6.093341e-01  6.613049e-01  6.426730e-01   \n",
       "max    2.423480e+00  2.440025e+00  2.368329e+01  4.072271e+00  5.085388e+00   \n",
       "\n",
       "                  5             6             7             8             9  \n",
       "count  4.177000e+03  4.177000e+03  4.177000e+03  4.177000e+03  4.177000e+03  \n",
       "mean   2.704723e-16  2.976897e-16 -4.252710e-17 -7.144552e-17  1.169495e-17  \n",
       "std    1.000120e+00  1.000120e+00  1.000120e+00  1.000120e+00  1.000120e+00  \n",
       "min   -1.643173e+00 -1.705134e+00 -6.748338e-01 -6.880179e-01 -7.594876e-01  \n",
       "25%   -7.946415e-01 -7.819095e-01 -6.748338e-01 -6.880179e-01 -7.594876e-01  \n",
       "50%   -8.753202e-02 -3.470794e-02 -6.748338e-01 -6.880179e-01 -7.594876e-01  \n",
       "75%    6.606355e-01  6.478319e-01  1.481846e+00  1.453451e+00  1.316677e+00  \n",
       "max    5.286500e+00  5.504642e+00  1.481846e+00  1.453451e+00  1.316677e+00  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name = \"abalone_classification\"\n",
    "data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "dfy = pd.DataFrame(data_y)\n",
    "df = pd.DataFrame(data_X)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986812</td>\n",
       "      <td>0.827554</td>\n",
       "      <td>0.925261</td>\n",
       "      <td>0.897914</td>\n",
       "      <td>0.903018</td>\n",
       "      <td>0.897706</td>\n",
       "      <td>0.309666</td>\n",
       "      <td>-0.551465</td>\n",
       "      <td>0.236543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.986812</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833684</td>\n",
       "      <td>0.925452</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.899724</td>\n",
       "      <td>0.905330</td>\n",
       "      <td>0.318626</td>\n",
       "      <td>-0.564315</td>\n",
       "      <td>0.240376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.827554</td>\n",
       "      <td>0.833684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819221</td>\n",
       "      <td>0.774972</td>\n",
       "      <td>0.798319</td>\n",
       "      <td>0.817338</td>\n",
       "      <td>0.298421</td>\n",
       "      <td>-0.518552</td>\n",
       "      <td>0.215459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.925261</td>\n",
       "      <td>0.925452</td>\n",
       "      <td>0.819221</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969405</td>\n",
       "      <td>0.966375</td>\n",
       "      <td>0.955355</td>\n",
       "      <td>0.299741</td>\n",
       "      <td>-0.557592</td>\n",
       "      <td>0.252038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.897914</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.774972</td>\n",
       "      <td>0.969405</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931961</td>\n",
       "      <td>0.882617</td>\n",
       "      <td>0.263991</td>\n",
       "      <td>-0.521842</td>\n",
       "      <td>0.251793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.903018</td>\n",
       "      <td>0.899724</td>\n",
       "      <td>0.798319</td>\n",
       "      <td>0.966375</td>\n",
       "      <td>0.931961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907656</td>\n",
       "      <td>0.308444</td>\n",
       "      <td>-0.556081</td>\n",
       "      <td>0.242194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.897706</td>\n",
       "      <td>0.905330</td>\n",
       "      <td>0.817338</td>\n",
       "      <td>0.955355</td>\n",
       "      <td>0.882617</td>\n",
       "      <td>0.907656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.306319</td>\n",
       "      <td>-0.546953</td>\n",
       "      <td>0.235391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.309666</td>\n",
       "      <td>0.318626</td>\n",
       "      <td>0.298421</td>\n",
       "      <td>0.299741</td>\n",
       "      <td>0.263991</td>\n",
       "      <td>0.308444</td>\n",
       "      <td>0.306319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.464298</td>\n",
       "      <td>-0.512528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.551465</td>\n",
       "      <td>-0.564315</td>\n",
       "      <td>-0.518552</td>\n",
       "      <td>-0.557592</td>\n",
       "      <td>-0.521842</td>\n",
       "      <td>-0.556081</td>\n",
       "      <td>-0.546953</td>\n",
       "      <td>-0.464298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.522541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.236543</td>\n",
       "      <td>0.240376</td>\n",
       "      <td>0.215459</td>\n",
       "      <td>0.252038</td>\n",
       "      <td>0.251793</td>\n",
       "      <td>0.242194</td>\n",
       "      <td>0.235391</td>\n",
       "      <td>-0.512528</td>\n",
       "      <td>-0.522541</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.000000  0.986812  0.827554  0.925261  0.897914  0.903018  0.897706   \n",
       "1  0.986812  1.000000  0.833684  0.925452  0.893162  0.899724  0.905330   \n",
       "2  0.827554  0.833684  1.000000  0.819221  0.774972  0.798319  0.817338   \n",
       "3  0.925261  0.925452  0.819221  1.000000  0.969405  0.966375  0.955355   \n",
       "4  0.897914  0.893162  0.774972  0.969405  1.000000  0.931961  0.882617   \n",
       "5  0.903018  0.899724  0.798319  0.966375  0.931961  1.000000  0.907656   \n",
       "6  0.897706  0.905330  0.817338  0.955355  0.882617  0.907656  1.000000   \n",
       "7  0.309666  0.318626  0.298421  0.299741  0.263991  0.308444  0.306319   \n",
       "8 -0.551465 -0.564315 -0.518552 -0.557592 -0.521842 -0.556081 -0.546953   \n",
       "9  0.236543  0.240376  0.215459  0.252038  0.251793  0.242194  0.235391   \n",
       "\n",
       "          7         8         9  \n",
       "0  0.309666 -0.551465  0.236543  \n",
       "1  0.318626 -0.564315  0.240376  \n",
       "2  0.298421 -0.518552  0.215459  \n",
       "3  0.299741 -0.557592  0.252038  \n",
       "4  0.263991 -0.521842  0.251793  \n",
       "5  0.308444 -0.556081  0.242194  \n",
       "6  0.306319 -0.546953  0.235391  \n",
       "7  1.000000 -0.464298 -0.512528  \n",
       "8 -0.464298  1.000000 -0.522541  \n",
       "9 -0.512528 -0.522541  1.000000  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for high correlation among features\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting null values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x2520 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convertir les données en float\n",
    "df = df.astype(float)\n",
    "\n",
    "# Mise en place de la matrice de coorélation\n",
    "corr                             = df.corr()\n",
    "mask                             = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax                            = plt.subplots(figsize=(12,35))\n",
    "cmap                             = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Visualisation de la matrice\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.1, cbar_kws={\"shrink\": .1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.574558</td>\n",
       "      <td>-0.432149</td>\n",
       "      <td>-1.064424</td>\n",
       "      <td>-0.641898</td>\n",
       "      <td>-0.607685</td>\n",
       "      <td>-0.726212</td>\n",
       "      <td>-0.638217</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.448986</td>\n",
       "      <td>-1.439929</td>\n",
       "      <td>-1.183978</td>\n",
       "      <td>-1.230277</td>\n",
       "      <td>-1.170910</td>\n",
       "      <td>-1.205221</td>\n",
       "      <td>-1.212987</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050033</td>\n",
       "      <td>0.122130</td>\n",
       "      <td>-0.107991</td>\n",
       "      <td>-0.309469</td>\n",
       "      <td>-0.463500</td>\n",
       "      <td>-0.356690</td>\n",
       "      <td>-0.207139</td>\n",
       "      <td>1.481846</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>-0.759488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.699476</td>\n",
       "      <td>-0.432149</td>\n",
       "      <td>-0.347099</td>\n",
       "      <td>-0.637819</td>\n",
       "      <td>-0.648238</td>\n",
       "      <td>-0.607600</td>\n",
       "      <td>-0.602294</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.615544</td>\n",
       "      <td>-1.540707</td>\n",
       "      <td>-1.423087</td>\n",
       "      <td>-1.272086</td>\n",
       "      <td>-1.215968</td>\n",
       "      <td>-1.287337</td>\n",
       "      <td>-1.320757</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>1.453451</td>\n",
       "      <td>-0.759488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>0.341509</td>\n",
       "      <td>0.424464</td>\n",
       "      <td>0.609334</td>\n",
       "      <td>0.118813</td>\n",
       "      <td>0.047908</td>\n",
       "      <td>0.532900</td>\n",
       "      <td>0.073062</td>\n",
       "      <td>1.481846</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>-0.759488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>0.549706</td>\n",
       "      <td>0.323686</td>\n",
       "      <td>-0.107991</td>\n",
       "      <td>0.279929</td>\n",
       "      <td>0.358808</td>\n",
       "      <td>0.309362</td>\n",
       "      <td>0.155685</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0.632985</td>\n",
       "      <td>0.676409</td>\n",
       "      <td>1.565767</td>\n",
       "      <td>0.708212</td>\n",
       "      <td>0.748559</td>\n",
       "      <td>0.975413</td>\n",
       "      <td>0.496955</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.841182</td>\n",
       "      <td>0.777187</td>\n",
       "      <td>0.250672</td>\n",
       "      <td>0.541998</td>\n",
       "      <td>0.773341</td>\n",
       "      <td>0.733627</td>\n",
       "      <td>0.410739</td>\n",
       "      <td>1.481846</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>-0.759488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>1.549052</td>\n",
       "      <td>1.482634</td>\n",
       "      <td>1.326659</td>\n",
       "      <td>2.283681</td>\n",
       "      <td>2.640993</td>\n",
       "      <td>1.787449</td>\n",
       "      <td>1.840481</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "0    -0.574558 -0.432149 -1.064424 -0.641898 -0.607685 -0.726212 -0.638217   \n",
       "1    -1.448986 -1.439929 -1.183978 -1.230277 -1.170910 -1.205221 -1.212987   \n",
       "2     0.050033  0.122130 -0.107991 -0.309469 -0.463500 -0.356690 -0.207139   \n",
       "3    -0.699476 -0.432149 -0.347099 -0.637819 -0.648238 -0.607600 -0.602294   \n",
       "4    -1.615544 -1.540707 -1.423087 -1.272086 -1.215968 -1.287337 -1.320757   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4172  0.341509  0.424464  0.609334  0.118813  0.047908  0.532900  0.073062   \n",
       "4173  0.549706  0.323686 -0.107991  0.279929  0.358808  0.309362  0.155685   \n",
       "4174  0.632985  0.676409  1.565767  0.708212  0.748559  0.975413  0.496955   \n",
       "4175  0.841182  0.777187  0.250672  0.541998  0.773341  0.733627  0.410739   \n",
       "4176  1.549052  1.482634  1.326659  2.283681  2.640993  1.787449  1.840481   \n",
       "\n",
       "             7         8         9  \n",
       "0    -0.674834 -0.688018  1.316677  \n",
       "1    -0.674834 -0.688018  1.316677  \n",
       "2     1.481846 -0.688018 -0.759488  \n",
       "3    -0.674834 -0.688018  1.316677  \n",
       "4    -0.674834  1.453451 -0.759488  \n",
       "...        ...       ...       ...  \n",
       "4172  1.481846 -0.688018 -0.759488  \n",
       "4173 -0.674834 -0.688018  1.316677  \n",
       "4174 -0.674834 -0.688018  1.316677  \n",
       "4175  1.481846 -0.688018 -0.759488  \n",
       "4176 -0.674834 -0.688018  1.316677  \n",
       "\n",
       "[4177 rows x 10 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns =[0,1,2,3,4,5]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.638217</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.212987</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.207139</td>\n",
       "      <td>1.481846</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>-0.759488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.602294</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.320757</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>1.453451</td>\n",
       "      <td>-0.759488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          6         7         8         9\n",
       "0 -0.638217 -0.674834 -0.688018  1.316677\n",
       "1 -1.212987 -0.674834 -0.688018  1.316677\n",
       "2 -0.207139  1.481846 -0.688018 -0.759488\n",
       "3 -0.602294 -0.674834 -0.688018  1.316677\n",
       "4 -1.320757 -0.674834  1.453451 -0.759488"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x2520 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convertir les données en float\n",
    "df = df.astype(float)\n",
    "\n",
    "# Mise en place de la matrice de coorélation\n",
    "corr                             = df.corr()\n",
    "mask                             = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax                            = plt.subplots(figsize=(12,35))\n",
    "cmap                             = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Visualisation de la matrice\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.1, cbar_kws={\"shrink\": .1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLDA(object):\n",
    "    '''\n",
    "    This class is for linear discriminant analysis classification.\n",
    "    \n",
    "    The class contains the parameters of LDA, including the number of classes and the prior probability p(i) of \n",
    "    each class $i$, where $i=1,2,\\ldots,num_classes$. Moreover, the class contains the the mean vectors $\\mu_i$ \n",
    "    and covariance matrix $\\Sigma$ of probability distributions $p(x|i)$ for the class $i$.\n",
    "    \n",
    "    It also contains the functions for initializing the class, fitting the LDA classifier model, use \n",
    "    the fitted model to calculate the linear discriminant functions $\\delta_i(x)$ and decision function $h^*(x)$.\n",
    "    \n",
    "    Attributes:\n",
    "        mu (matrix, num_classes*num_features)    : mean vectors of distributions $p(x|i)$. The $i$-th row represents $\\mu_i$.\n",
    "        Sigma (matrix, num_features*num_features): covariance matrix\n",
    "        num_classes (positive integer)           : the number of classes\n",
    "        priorProbs (vector, num_classes)         : the prior probability vector and its $i$-th element is $p(i)$\n",
    "        \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the class by just assigning zero to all atrributes. \n",
    "        '''\n",
    "        self.mu = 0 \n",
    "        self.Sigma = 0\n",
    "        self.num_classes = 0\n",
    "        self.priorProbs = 0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        estimate the mean vector and covariance matrix of each class in the LDA model\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_train*num_features): features of training samples\n",
    "            y (matrix, num_train): label of training samples\n",
    "            \n",
    "        Returns:\n",
    "            mu (matrix, num_classes*num_features)    : mean vectors of distributions $p(x|i)$. The $i$-th row represents $\\mu_i$.\n",
    "            Sigma (matrix, num_features*num_features): covariance matrix\n",
    "        ''' \n",
    "        num_samples, num_features = X.shape\n",
    "        values, counts = np.unique(y, return_counts = True)\n",
    "        num_classes = len(values)\n",
    "        ### calculate the prior probability $p(i)$\n",
    "        self.priorProbs = counts / num_samples\n",
    "        ### calculate the mean vector of each class $\\mu_i$\n",
    "        self.mu = np.zeros((num_classes, num_features))\n",
    "        for k in range(num_samples):\n",
    "            self.mu[int(y[k]),:] += X[k,:]\n",
    "        self.mu = self.mu / np.expand_dims(counts, 1) \n",
    "        ### calculate the covariance matrix $\\Sigma$\n",
    "        Sigma_i = [np.cov(X[y == i].T)*(X[y == i].shape[0]-1) for i in range(num_classes)] \n",
    "        self.Sigma = sum(Sigma_i) / (X.shape[0]-num_classes)\n",
    "        return self.mu, self.Sigma\n",
    "    \n",
    "    def linear_discriminant_func(self, X):\n",
    "        '''\n",
    "        calculate the linear discriminant functions $\\delta_i(X)$\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_samples*num_features): features of samples\n",
    "            \n",
    "        Returns:\n",
    "            value (matrix, num_samples*num_classes): the linear discriminant function values. \n",
    "            The $(j,i)$-th entry of value represents $\\delta_i(X[j,:])$, which is the linear discriminant function value for the class $i$ of the sample at row $j$.\n",
    "        '''\n",
    "        ### calculate the inverse matrix of the covariance matrix $\\Sigma$\n",
    "        U, S, V = np.linalg.svd(self.Sigma)\n",
    "        Sn = np.linalg.inv(np.diag(S))\n",
    "        Sigma_inv = np.dot(np.dot(V.T, Sn), U.T)\n",
    "        ### calculate the linear discriminant function values of X\n",
    "        value = np.dot(np.dot(X, Sigma_inv), self.mu.T) - \\\n",
    "                0.5 * np.multiply(np.dot(self.mu, Sigma_inv).T, self.mu.T).sum(axis = 0).reshape(1, -1) + \\\n",
    "                np.log(np.expand_dims(self.priorProbs, axis = 0))\n",
    "        return value\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        calculate the linear discriminant functions\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_samples*num_features): features of samples\n",
    "            \n",
    "        Returns:\n",
    "            pred_label (vector, num_samples): the predicted labels of samples. The $j$-th entry represents the predicted label of the sample at row $j$.\n",
    "        '''\n",
    "        pred_value = self.linear_discriminant_func(X)\n",
    "        pred_label = np.argmax(pred_value, axis = 1)\n",
    "        return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4177, 4)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def data(df, dfy):\n",
    "    data_X = df\n",
    "    data_y = dfy\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_X, data_y = data(df, dfy)\n",
    "\n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.33, random_state=2200)\n",
    "    return train_X, test_X, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4177, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\josep\\Dropbox\\PC\\Documents\\GitHub\\statistical_learning\\code\\code_classification\\Abalone\\abalone.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m myLDA()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m### fit the model with training data and get the estimation of mu and Sigma\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mu, Sigma \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(main()[\u001b[39m0\u001b[39m], main()[\u001b[39m2\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m### predict the label of test data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(main()[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\josep\\Dropbox\\PC\\Documents\\GitHub\\statistical_learning\\code\\code_classification\\Abalone\\abalone.ipynb Cell 17\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_X, data_y \u001b[39m=\u001b[39m data(df, dfy)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Randomly assingning a train and test set\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_X, test_X, train_y, test_y \u001b[39m=\u001b[39m train_test_split(data_X, data_y, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.33\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m2200\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m train_X, test_X, train_y, test_y\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2445\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2442\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2443\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2445\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[0;32m   2447\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m   2448\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2449\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[0;32m   2450\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:433\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \n\u001b[0;32m    416\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    432\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[1;32m--> 433\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[0;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4177, 1]"
     ]
    }
   ],
   "source": [
    "### initiate the LDA model\n",
    "model = myLDA()\n",
    "### fit the model with training data and get the estimation of mu and Sigma\n",
    "mu, Sigma = model.fit(main()[0], main()[2])\n",
    "### predict the label of test data\n",
    "y_pred = model.predict(main()[1])\n",
    "### calculate the accuracy of the fitted LDA model on test data\n",
    "accuracy = np.sum(y_pred == main()[3])/len(main()[3])\n",
    "print(\"Accuracy of LDA on the test dataset is {}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>0_F</th>\n",
       "      <th>0_I</th>\n",
       "      <th>0_M</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1      2      3       4       5       6     7  0_F  0_I  0_M   8\n",
       "0  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.15    0    0    1  15\n",
       "1  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.07    0    0    1   7\n",
       "2  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.21    1    0    0   9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am loading the full dataset and renaming the columns to keep better track of each attribute\n",
    "data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "data_path = os.path.join(data_dir, \"abalone_classification.csv\")\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "col_names = ['col_1','col_2','col_3','col_4','col_5','col_6','col_7','col_8','name']\n",
    "df.columns = col_names\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>col_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.163951</td>\n",
       "      <td>0.158175</td>\n",
       "      <td>0.064922</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.075043</td>\n",
       "      <td>-0.124540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_3</th>\n",
       "      <td>-0.163951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059668</td>\n",
       "      <td>-0.008083</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>-0.185805</td>\n",
       "      <td>-0.022043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_4</th>\n",
       "      <td>0.158175</td>\n",
       "      <td>0.059668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005931</td>\n",
       "      <td>-0.009040</td>\n",
       "      <td>-0.103591</td>\n",
       "      <td>-0.054797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_5</th>\n",
       "      <td>0.064922</td>\n",
       "      <td>-0.008083</td>\n",
       "      <td>-0.005931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.002829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_6</th>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>-0.009040</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>-0.035659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_7</th>\n",
       "      <td>0.075043</td>\n",
       "      <td>-0.185805</td>\n",
       "      <td>-0.103591</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_8</th>\n",
       "      <td>-0.124540</td>\n",
       "      <td>-0.022043</td>\n",
       "      <td>-0.054797</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>-0.035659</td>\n",
       "      <td>0.089690</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          col_1     col_3     col_4     col_5     col_6     col_7     col_8\n",
       "col_1  1.000000 -0.163951  0.158175  0.064922  0.005597  0.075043 -0.124540\n",
       "col_3 -0.163951  1.000000  0.059668 -0.008083  0.009378 -0.185805 -0.022043\n",
       "col_4  0.158175  0.059668  1.000000 -0.005931 -0.009040 -0.103591 -0.054797\n",
       "col_5  0.064922 -0.008083 -0.005931  1.000000 -0.009674  0.043627  0.002829\n",
       "col_6  0.005597  0.009378 -0.009040 -0.009674  1.000000  0.020900 -0.035659\n",
       "col_7  0.075043 -0.185805 -0.103591  0.043627  0.020900  1.000000  0.089690\n",
       "col_8 -0.124540 -0.022043 -0.054797  0.002829 -0.035659  0.089690  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for high correlation among features\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500121</td>\n",
       "      <td>0.500034</td>\n",
       "      <td>0.261186</td>\n",
       "      <td>0.504717</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.499885</td>\n",
       "      <td>0.276199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.137299</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>0.137098</td>\n",
       "      <td>0.048351</td>\n",
       "      <td>0.075683</td>\n",
       "      <td>0.057797</td>\n",
       "      <td>0.106491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             col_1        col_3        col_4        col_5        col_6  \\\n",
       "count  1484.000000  1484.000000  1484.000000  1484.000000  1484.000000   \n",
       "mean      0.500121     0.500034     0.261186     0.504717     0.007500   \n",
       "std       0.137299     0.086670     0.137098     0.048351     0.075683   \n",
       "min       0.110000     0.210000     0.000000     0.500000     0.000000   \n",
       "25%       0.410000     0.460000     0.170000     0.500000     0.000000   \n",
       "50%       0.490000     0.510000     0.220000     0.500000     0.000000   \n",
       "75%       0.580000     0.550000     0.320000     0.500000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.830000   \n",
       "\n",
       "             col_7        col_8  \n",
       "count  1484.000000  1484.000000  \n",
       "mean      0.499885     0.276199  \n",
       "std       0.057797     0.106491  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.480000     0.220000  \n",
       "50%       0.510000     0.220000  \n",
       "75%       0.530000     0.300000  \n",
       "max       0.730000     1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYT    463\n",
      "NUC    429\n",
      "MIT    244\n",
      "ME3    163\n",
      "ME2     51\n",
      "ME1     44\n",
      "EXC     35\n",
      "VAC     30\n",
      "POX     20\n",
      "ERL      5\n",
      "Name: name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.name.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x3600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mise en place de la matrice de coorélation\n",
    "corr                             = df.corr()\n",
    "mask                             = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax                            = plt.subplots(figsize=(10,50))\n",
    "cmap                             = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Visualisation de la matrice\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.1, cbar_kws={\"shrink\": .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found a correlation between the different measurements of Hold, Pressure and Finger-Area with their averages. I also found a correlation with the different measurements of Hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA on the test dataset is 0.5993265993265994.\n"
     ]
    }
   ],
   "source": [
    "### initiate the LDA model\n",
    "data_X = df.iloc[:,:-1]\n",
    "data_y = df.iloc[:,-1]\n",
    "scaler_X = StandardScaler()\n",
    "data_X = scaler_X.fit_transform(data_X)\n",
    "data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "# Randomly assingning a train and test set\n",
    "train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.20, random_state=13)\n",
    "\n",
    "model = myLDA()\n",
    "### fit the model with training data and get the estimation of mu and Sigma\n",
    "mu, Sigma = model.fit(train_X, train_y)\n",
    "### predict the label of test data\n",
    "y_pred = model.predict(test_X)\n",
    "### calculate the accuracy of the fitted LDA model on test data\n",
    "accuracy = np.sum(y_pred == test_y)/len(test_y)\n",
    "print(\"Accuracy of LDA on the test dataset is {}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_f = RandomForestClassifier()\n",
    "algo_lda = LinearDiscriminantAnalysis()\n",
    "algo_dt = DecisionTreeClassifier()\n",
    "model_forrest = algo_f.fit(main()[0], main()[2])\n",
    "model_LDA = algo_lda.fit(main()[0], main()[2])\n",
    "model_DT = algo_dt.fit(main()[0], main()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 48.485 for the DT model\n",
      " Accuracy: 62.626 for the forrest model\n",
      " Accuracy: 54.882 for the LDA model\n"
     ]
    }
   ],
   "source": [
    "print(\" Accuracy: %.3f for the DT model\" % (model_DT.score(main()[1], main()[3])*100.0))\n",
    "print(\" Accuracy: %.3f for the forrest model\" % (model_forrest.score(main()[1], main()[3])*100.0))\n",
    "print(\" Accuracy: %.3f for the LDA model\" % (model_LDA.score(main()[1], main()[3])*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I again use the processed data from the logistic regression but this time I specify SVC\n",
    "\n",
    "SVM_best_scores = {}\n",
    "tree_param = {'criterion':['gini','entropy'],'max_depth':np.arange(3, 15)}\n",
    "search_svm = GridSearchCV(estimator = algo_dt ,param_grid= tree_param,\n",
    "                    cv = 5, return_train_score = True,\n",
    "                    n_jobs = -1)\n",
    "\n",
    "search_svm.fit(main()[0], main()[2])\n",
    "SVM_best_scores = {'model':search_svm, 'best_params':search_svm.best_params_,\n",
    "                        'best_score':search_svm.best_score_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 5}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SVM_best_scores['best_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(DecisionTreeClassifier(), tree_param, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "              param_grid={'bootstrap': [False], 'criterion': ['gini', 'entropy'],\n",
       "                          'max_depth': range(1, 10),\n",
       "                          'max_features': ['auto', 'sqrt'],\n",
       "                          'min_samples_split': [3, 7, 10],\n",
       "                          'n_estimators': [10, 20, 71]}),\n",
       " 'best_params': {'bootstrap': False,\n",
       "  'criterion': 'entropy',\n",
       "  'max_depth': 9,\n",
       "  'max_features': 'auto',\n",
       "  'min_samples_split': 10,\n",
       "  'n_estimators': 71},\n",
       " 'best_score': 0.626809204694536}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# utilisation d'une grille complete avec toutes les parametres jugés nécessaires\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10,20,71],\n",
    "    \"bootstrap\": [False],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_split\" : [3,7, 10],\n",
    "    \"max_depth\" :range(10)[1:],\n",
    "    \"max_features\": ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Effectuer grid search\n",
    "grid_search = GridSearchCV(algo_f, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(main()[0], main()[2])\n",
    "forrest_best_scores = {'model':grid_search, 'best_params':grid_search.best_params_,\n",
    "                        'best_score':grid_search.best_score_}\n",
    "forrest_best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path, header=1)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    # read dataset from csv file\n",
    "    data_name = \"abalone_classification\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "\n",
    "    # Train and test set\n",
    "    # kf = KFold(n_splits=10)\n",
    "    # res_list = []\n",
    "    # for train_index, test_index in kf.split(data_X):\n",
    "    #     train_X, train_y = data_X[train_index,:], data_y[train_index]\n",
    "    #     test_X, test_y = data_X[test_index,:], data_y[test_index]\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion -- Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_counts(y, sample_weight, classes_):\n",
    "    '''\n",
    "    the function used to calculate the summation of weights of samples from each class. Generally speaking,\n",
    "    the weights are all set as one. But for Adaboost, each sample has different values.\n",
    "    '''\n",
    "    class_counts = np.zeros(shape=classes_.shape[0], dtype=np.float64)\n",
    "    for i, label in enumerate(classes_):\n",
    "        idx = y == label\n",
    "        if idx.sum() > 0:\n",
    "            class_counts[i] = sample_weight[idx].sum()\n",
    "        else:\n",
    "            class_counts[i] = 0\n",
    "    return class_counts\n",
    "\n",
    "def gini(y, sample_weight):\n",
    "    classes_ = np.unique(y)\n",
    "    class_counts = calculate_weighted_counts(y, sample_weight, classes_)\n",
    "    if class_counts.sum() > 0:\n",
    "        pk = class_counts / class_counts.sum()\n",
    "        pk = pk[pk > 0]\n",
    "        return 1 - np.sum(pk**2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def gini_index(X, y, feat, point, sample_weight):\n",
    "    '''\n",
    "    calculate the difference of gini index before and after splitting\n",
    "    '''\n",
    "    S = gini(y, sample_weight)\n",
    "    new_S = 0\n",
    "    n = sample_weight.sum()\n",
    "    assert n > 0\n",
    "    idx1 = X[:, feat] < point\n",
    "    nv = sample_weight[idx1].sum()\n",
    "    if nv > 0:\n",
    "        new_S += nv / n * gini(y[idx1], sample_weight[idx1])\n",
    "    idx2 = X[:, feat] >= point\n",
    "    nv = sample_weight[idx2].sum()\n",
    "    if nv > 0:\n",
    "        new_S += nv / n * gini(y[idx2], sample_weight[idx2])\n",
    "    return S - new_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Trees for Classification\n",
    "\n",
    "Different from the classification tree implemented in the last tutorial:\n",
    "1. Each internal node has two child nodes regardless of values of the splitting feature are continuous or discrete.\n",
    "2. Add the parameter `max_depth` for providing another condition to stop splitting procedures.\n",
    "3. Add the parameter `max_features` to use the subset of features to build decision tree.\n",
    "\n",
    "Options 2&3 are designed for constructing trees in the random forest implemented in Section 5. \n",
    "\n",
    "If you'd like to build the classification tree, you can ignore options 2&3 by setting `max_depth = None` and `max_features = None`. Then we can combine it with the pre-pruning or post-pruning technique implemented in Section 4 to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(object):\n",
    "    '''\n",
    "    This class is for classification tree\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of classification tree\n",
    "        - tree: a nested dictionary representing the decision tree structure.\n",
    "        - max_depth: the parameter to control the depth of tree. If the depth is larger than max_depth, we will stop splitting.\n",
    "        - max_feature: the number of selected features to build decision tree\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 criterion=gini_index,\n",
    "                 max_depth=None,\n",
    "                 max_features=None,\n",
    "                 random_seed=None):\n",
    "        self.f_criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        if self.max_depth is None:\n",
    "            self.max_depth = 2**10\n",
    "        self.max_features = max_features\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        np.random.seed(self.random_seed)\n",
    "        num_samples, num_features = X.shape\n",
    "        if self.max_features is None:\n",
    "            self.max_features = num_features\n",
    "        elif self.max_features == \"sqrt\":\n",
    "            self.max_features = np.int(np.round(np.sqrt(num_features)))\n",
    "        self.classes_ = np.unique(y)\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(num_samples, dtype=np.float64)\n",
    "        # build the decision tree\n",
    "        self.tree = self.create_tree(X, y, sample_weight, depth=0)\n",
    "\n",
    "    def create_tree(self, X, y, sample_weight, depth):\n",
    "        Tree = {}\n",
    "        Tree[\"depth\"] = depth\n",
    "        class_counts = calculate_weighted_counts(y, sample_weight, self.classes_)\n",
    "        # create a leaf node if all samples belong to the same class\n",
    "        if (class_counts != 0).sum() == 1:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            Tree[\"pred\"] = self.classes_[class_counts != 0]\n",
    "        # using the majority vote to get the prediction at each node\n",
    "        majority_class = self.classes_[np.argmax(class_counts)]\n",
    "        Tree[\"pred\"] = majority_class\n",
    "        # create a leaf node if feature set is empty\n",
    "        feat, point = self.choose_best_split(X, y, sample_weight)\n",
    "        if feat is None or depth == self.max_depth:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            return Tree\n",
    "        # otherwise, create an internal node\n",
    "        Tree[\"is_leaf\"] = False\n",
    "        Tree[\"split_feat\"] = feat\n",
    "        Tree[\"split_point\"] = point\n",
    "        # build the left subtree\n",
    "        idx = X[:, feat] < point\n",
    "        Tree[\"left\"] = self.create_tree(X[idx], y[idx], sample_weight[idx],\n",
    "                                        depth + 1)\n",
    "        # build the right subtree\n",
    "        idx = X[:, feat] >= point\n",
    "        Tree[\"right\"] = self.create_tree(X[idx], y[idx], sample_weight[idx],\n",
    "                                         depth + 1)\n",
    "        return Tree\n",
    "\n",
    "    def choose_best_split(self, X, y, sample_weight):\n",
    "        # initialization\n",
    "        best_feat, best_point = None, None\n",
    "        best_score = 0.0\n",
    "        # search for each candidate feature\n",
    "        num_features = X.shape[1]\n",
    "        if self.max_features < num_features:\n",
    "            candidate_feat = np.random.permutation(\n",
    "                num_features)[:self.max_features]\n",
    "        else:\n",
    "            candidate_feat = np.arange(num_features)\n",
    "        for feat in candidate_feat:\n",
    "            # if all values of this feature are equal, do not split this feature\n",
    "            X_feat_value = np.unique(X[:, feat])\n",
    "            if len(X_feat_value) == 1:\n",
    "                continue\n",
    "            # search for each possible split point\n",
    "            for i in range(len(X_feat_value) - 1):\n",
    "                # divide the dataset into two parts according to the split\n",
    "                point = (X_feat_value[i] + X_feat_value[i + 1]) / 2.0\n",
    "                # calculate score to evaluate the quality of a split\n",
    "                score = self.f_criterion(X, y, feat, point, sample_weight)\n",
    "                if score > best_score:\n",
    "                    best_feat = feat\n",
    "                    best_point = point\n",
    "                    best_score = score\n",
    "        return best_feat, best_point\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to fit the decision tree classifier\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "\n",
    "        Returns:\n",
    "            y - predictions of test samples, a pandas series with shape (n,)\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y = []\n",
    "        for i in range(n):\n",
    "            y.append(DecisionTreeClassifier.predict_each(X[i], self.tree))\n",
    "        y = np.array(y, dtype=np.int32)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def predict_each(x, tree):\n",
    "        '''\n",
    "        for each sample, get the prediction of decision tree classifier in a recursive manner.\n",
    "\n",
    "        Args:\n",
    "            x - features of a sample, a pandas Series with shape (d,)\n",
    "            tree - a nested dictionary representing the decision tree structure.\n",
    "\n",
    "        Returns:\n",
    "            the prediction of the sample `x`\n",
    "        '''\n",
    "        if tree[\"is_leaf\"] is True:\n",
    "            # if the `tree` is a leaf node, get the prediction at the leaf node\n",
    "            return tree[\"pred\"]\n",
    "        else:\n",
    "            # the 'tree' is a nested dictionary\n",
    "            # get the value of the feature used to split\n",
    "            feat = tree[\"split_feat\"]\n",
    "            point = tree[\"split_point\"]\n",
    "            # get the value of the feature for the sample `x`\n",
    "            value = x[feat]\n",
    "            if value < point:\n",
    "                return DecisionTreeClassifier.predict_each(x, tree[\"left\"])\n",
    "            else:\n",
    "                return DecisionTreeClassifier.predict_each(x, tree[\"right\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_t():\n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(main()[0], main()[1], test_size=0.33, random_state=100)\n",
    "    return train_X, test_X, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training data is: 1.0\n",
      "The accuracy of test data is: 0.20449601160261058\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = t_t()\n",
    "model = DecisionTreeClassifier(criterion=gini_index,\n",
    "                                   max_depth=None,\n",
    "                                   max_features=None,\n",
    "                                   random_seed=None)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"The accuracy of training data is:\", acc_train)\n",
    "print(\"The accuracy of test data is:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Random Forest for Classification\n",
    "\n",
    "In this section, we implement the random forest where each tree is built with the class DecisionTreeClassifier(). In our model, the values of parameters are listed below. \n",
    "1. The number of trees $T$ is set as ``num_estimators = 20``\n",
    "2. the number of subsampled features for each tree is $k =\\sqrt{d}$, which corresponds to ``max_features = \"sqrt\"`` in the code.\n",
    "3. The maximum depth of each tree is ``max_depth = 6``.\n",
    "\n",
    "We will not use the pruning technique for each tree in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier(object):\n",
    "    '''\n",
    "    This class is for random forest classification\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of classification tree\n",
    "        - num_estimators: the number of trees in the random forest \n",
    "        - tree: a nested dictionary representing the decision tree structure\n",
    "        - max_depth: the parameter to control the depth of tree. If the depth is larger than max_depth, we will stop splitting.\n",
    "        - max_feature: the number of selected features to build decision tree\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 num_estimators,\n",
    "                 random_state,\n",
    "                 criterion=gini_index,\n",
    "                 max_depth=None,\n",
    "                 max_features=\"sqrt\"):\n",
    "        self.num_estimators = num_estimators\n",
    "        self.random_state = random_state\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        function used to fit all trees in the random forest\n",
    "        \n",
    "        Args:\n",
    "            X - the features of the training samples\n",
    "            y - the labels of the training samples\n",
    "        Returns:\n",
    "            self.model_list - the model list containing `num_estimators` tree models\n",
    "        '''\n",
    "        n, d = X.shape\n",
    "        RandomState = np.random.RandomState(self.random_state)\n",
    "        self.model_list = []\n",
    "        for t in range(self.num_estimators):\n",
    "            random_seed = RandomState.randint(0, np.iinfo(np.int32).max)\n",
    "            ### draw a bootstrapped dataset from X\n",
    "            sample_index = RandomState.choice(np.arange(n), size=n, replace=True)\n",
    "            X_sampled = X[sample_index, :]\n",
    "            y_sampled = y[sample_index]\n",
    "            ### initialize the tree model by using DecisionTreeClassifier()\n",
    "            model = DecisionTreeClassifier(criterion=self.criterion,\n",
    "                                           max_depth=self.max_depth,\n",
    "                                           max_features=self.max_features,\n",
    "                                           random_seed=random_seed)\n",
    "            ### fit the tree model to the bootstrapped dataset by using DecisionTreeClassifier.fit()\n",
    "            model.fit(X_sampled, y_sampled)\n",
    "            self.model_list.append(model)\n",
    "        return self.model_list\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to predict the labels of X\n",
    "        \n",
    "        Args:\n",
    "            X - the features of the test samples\n",
    "        Returns:\n",
    "            y_pred_label - the predicted labels of test samples\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y_pred = np.zeros([self.num_estimators, n], dtype=np.int32)\n",
    "        y_pred_label = np.zeros(n, dtype=np.int32)\n",
    "        ### use T tree classifiers to make predictions by using DecisionTreeClassifier.predict()\n",
    "        for i in range(self.num_estimators):\n",
    "            model_i = self.model_list[i]\n",
    "            y_pred[i, :] = model_i.predict(X)\n",
    "        ### take the majority vote \n",
    "        for i in range(n):\n",
    "            classes, count = np.unique(y_pred[:, i], return_counts=True)\n",
    "            y_pred_label[i] = classes[np.argmax(count)]\n",
    "        return y_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_3900\\4230132589.py:29: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.max_features = np.int(np.round(np.sqrt(num_features)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of the random forest is: 0.3385770468358956\n",
      "Testing accuracy of the random forest is: 0.25598259608411894\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = t_t()\n",
    "model = RandomForestClassifier(num_estimators=20,\n",
    "                                   random_state=101,\n",
    "                                   criterion=gini_index,\n",
    "                                   max_depth=6)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"Training accuracy of the random forest is:\", acc_train)\n",
    "print(\"Testing accuracy of the random forest is:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
