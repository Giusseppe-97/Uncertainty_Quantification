{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from encodings import search_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import necessary tools from the sklearn library\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "# Import sklearn library tools used ONLY for validating my results\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_curve, f1_score, accuracy_score, recall_score, precision_score, auc, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import randint as sp_randint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 9 attributes. These attributes are the columns of the data set. The age column is not included in the dataset. But in the description it is given that age = Rings + 1.5.\n",
    "\n",
    "- Sex – The data type is categorical and there are three types in this data. M, F and I.\n",
    "- Length – It is the continuous datatype and the units are in mm. It is the longest shell measurement.\n",
    "- Diameter – It is also continuous datatype and the units are in mm. It is the perpendicular to length.\n",
    "- Height – It is a continuous data type. It is numerical just like diameter and length. Its units are in mm and it is - the meat in the shell.\n",
    "- Whole weight – A continuous data measured in grams. It is the weight of whole abalone.\n",
    "- Shucked weight – It is same as whole weight, continuous data, measured in grams and it is the weight of the meat.\n",
    "- Viscera weight – It is also continuous data, measured in grams and it is the git weight after bleeding.\n",
    "- Shell weight – It is the weight of the shell after being dried. It is continuously varying data.\n",
    "- Rings – It is an integer and adding 1.5 to rings gives the age of the abalone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'..\\..\\..\\data\\data_classification\\abalone_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>0_F</th>\n",
       "      <th>0_I</th>\n",
       "      <th>0_M</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1      2      3       4       5       6      7  0_F  0_I  0_M   8\n",
       "0  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150    0    0    1  15\n",
       "1  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.070    0    0    1   7\n",
       "2  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.210    1    0    0   9\n",
       "3  0.440  0.365  0.125  0.5160  0.2155  0.1140  0.155    0    0    1  10\n",
       "4  0.330  0.255  0.080  0.2050  0.0895  0.0395  0.055    0    1    0   7"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # read dataset from csv file\n",
    "    data_name = \"abalone_classification\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "\n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.33, random_state=2200)\n",
    "    return train_X, test_X, train_y, test_y, data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(main()[4])\n",
    "dfy = pd.DataFrame(main()[5])\n",
    "col_names = ['Length',\t'Diameter',\t'Height',\t'Whole weight',\t'Shucked weight',\t'Viscera weight',\t'Shell weight', 'Female', 'I', 'Male']\n",
    "col_names_y = ['Rings']\n",
    "df.columns = col_names\n",
    "dfy.columns = col_names_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Female</th>\n",
       "      <th>I</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.574558</td>\n",
       "      <td>-0.432149</td>\n",
       "      <td>-1.064424</td>\n",
       "      <td>-0.641898</td>\n",
       "      <td>-0.607685</td>\n",
       "      <td>-0.726212</td>\n",
       "      <td>-0.638217</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.448986</td>\n",
       "      <td>-1.439929</td>\n",
       "      <td>-1.183978</td>\n",
       "      <td>-1.230277</td>\n",
       "      <td>-1.170910</td>\n",
       "      <td>-1.205221</td>\n",
       "      <td>-1.212987</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.050033</td>\n",
       "      <td>0.122130</td>\n",
       "      <td>-0.107991</td>\n",
       "      <td>-0.309469</td>\n",
       "      <td>-0.463500</td>\n",
       "      <td>-0.356690</td>\n",
       "      <td>-0.207139</td>\n",
       "      <td>1.481846</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>-0.759488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.699476</td>\n",
       "      <td>-0.432149</td>\n",
       "      <td>-0.347099</td>\n",
       "      <td>-0.637819</td>\n",
       "      <td>-0.648238</td>\n",
       "      <td>-0.607600</td>\n",
       "      <td>-0.602294</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>-0.688018</td>\n",
       "      <td>1.316677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.615544</td>\n",
       "      <td>-1.540707</td>\n",
       "      <td>-1.423087</td>\n",
       "      <td>-1.272086</td>\n",
       "      <td>-1.215968</td>\n",
       "      <td>-1.287337</td>\n",
       "      <td>-1.320757</td>\n",
       "      <td>-0.674834</td>\n",
       "      <td>1.453451</td>\n",
       "      <td>-0.759488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Length  Diameter    Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0 -0.574558 -0.432149 -1.064424     -0.641898       -0.607685       -0.726212   \n",
       "1 -1.448986 -1.439929 -1.183978     -1.230277       -1.170910       -1.205221   \n",
       "2  0.050033  0.122130 -0.107991     -0.309469       -0.463500       -0.356690   \n",
       "3 -0.699476 -0.432149 -0.347099     -0.637819       -0.648238       -0.607600   \n",
       "4 -1.615544 -1.540707 -1.423087     -1.272086       -1.215968       -1.287337   \n",
       "\n",
       "   Shell weight    Female         I      Male  \n",
       "0     -0.638217 -0.674834 -0.688018  1.316677  \n",
       "1     -1.212987 -0.674834 -0.688018  1.316677  \n",
       "2     -0.207139  1.481846 -0.688018 -0.759488  \n",
       "3     -0.602294 -0.674834 -0.688018  1.316677  \n",
       "4     -1.320757 -0.674834  1.453451 -0.759488  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Female</th>\n",
       "      <th>I</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>4.177000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-5.834718e-16</td>\n",
       "      <td>-3.027929e-16</td>\n",
       "      <td>3.912493e-16</td>\n",
       "      <td>9.185853e-17</td>\n",
       "      <td>-1.020650e-17</td>\n",
       "      <td>2.704723e-16</td>\n",
       "      <td>2.976897e-16</td>\n",
       "      <td>-4.252710e-17</td>\n",
       "      <td>-7.144552e-17</td>\n",
       "      <td>1.169495e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "      <td>1.000120e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.739154e+00</td>\n",
       "      <td>-3.556267e+00</td>\n",
       "      <td>-3.335953e+00</td>\n",
       "      <td>-1.686092e+00</td>\n",
       "      <td>-1.614731e+00</td>\n",
       "      <td>-1.643173e+00</td>\n",
       "      <td>-1.705134e+00</td>\n",
       "      <td>-6.748338e-01</td>\n",
       "      <td>-6.880179e-01</td>\n",
       "      <td>-7.594876e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.161975e-01</td>\n",
       "      <td>-5.833158e-01</td>\n",
       "      <td>-5.862075e-01</td>\n",
       "      <td>-7.897577e-01</td>\n",
       "      <td>-7.811585e-01</td>\n",
       "      <td>-7.946415e-01</td>\n",
       "      <td>-7.819095e-01</td>\n",
       "      <td>-6.748338e-01</td>\n",
       "      <td>-6.880179e-01</td>\n",
       "      <td>-7.594876e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.749513e-01</td>\n",
       "      <td>1.725193e-01</td>\n",
       "      <td>1.156329e-02</td>\n",
       "      <td>-5.963767e-02</td>\n",
       "      <td>-1.052891e-01</td>\n",
       "      <td>-8.753202e-02</td>\n",
       "      <td>-3.470794e-02</td>\n",
       "      <td>-6.748338e-01</td>\n",
       "      <td>-6.880179e-01</td>\n",
       "      <td>-7.594876e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.579031e-01</td>\n",
       "      <td>7.267984e-01</td>\n",
       "      <td>6.093341e-01</td>\n",
       "      <td>6.613049e-01</td>\n",
       "      <td>6.426730e-01</td>\n",
       "      <td>6.606355e-01</td>\n",
       "      <td>6.478319e-01</td>\n",
       "      <td>1.481846e+00</td>\n",
       "      <td>1.453451e+00</td>\n",
       "      <td>1.316677e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.423480e+00</td>\n",
       "      <td>2.440025e+00</td>\n",
       "      <td>2.368329e+01</td>\n",
       "      <td>4.072271e+00</td>\n",
       "      <td>5.085388e+00</td>\n",
       "      <td>5.286500e+00</td>\n",
       "      <td>5.504642e+00</td>\n",
       "      <td>1.481846e+00</td>\n",
       "      <td>1.453451e+00</td>\n",
       "      <td>1.316677e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Length      Diameter        Height  Whole weight  Shucked weight  \\\n",
       "count  4.177000e+03  4.177000e+03  4.177000e+03  4.177000e+03    4.177000e+03   \n",
       "mean  -5.834718e-16 -3.027929e-16  3.912493e-16  9.185853e-17   -1.020650e-17   \n",
       "std    1.000120e+00  1.000120e+00  1.000120e+00  1.000120e+00    1.000120e+00   \n",
       "min   -3.739154e+00 -3.556267e+00 -3.335953e+00 -1.686092e+00   -1.614731e+00   \n",
       "25%   -6.161975e-01 -5.833158e-01 -5.862075e-01 -7.897577e-01   -7.811585e-01   \n",
       "50%    1.749513e-01  1.725193e-01  1.156329e-02 -5.963767e-02   -1.052891e-01   \n",
       "75%    7.579031e-01  7.267984e-01  6.093341e-01  6.613049e-01    6.426730e-01   \n",
       "max    2.423480e+00  2.440025e+00  2.368329e+01  4.072271e+00    5.085388e+00   \n",
       "\n",
       "       Viscera weight  Shell weight        Female             I          Male  \n",
       "count    4.177000e+03  4.177000e+03  4.177000e+03  4.177000e+03  4.177000e+03  \n",
       "mean     2.704723e-16  2.976897e-16 -4.252710e-17 -7.144552e-17  1.169495e-17  \n",
       "std      1.000120e+00  1.000120e+00  1.000120e+00  1.000120e+00  1.000120e+00  \n",
       "min     -1.643173e+00 -1.705134e+00 -6.748338e-01 -6.880179e-01 -7.594876e-01  \n",
       "25%     -7.946415e-01 -7.819095e-01 -6.748338e-01 -6.880179e-01 -7.594876e-01  \n",
       "50%     -8.753202e-02 -3.470794e-02 -6.748338e-01 -6.880179e-01 -7.594876e-01  \n",
       "75%      6.606355e-01  6.478319e-01  1.481846e+00  1.453451e+00  1.316677e+00  \n",
       "max      5.286500e+00  5.504642e+00  1.481846e+00  1.453451e+00  1.316677e+00  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "y =np.array(dfy)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 20, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.25912627169359664\n",
      "Confusion Matrix: \n",
      "[[  0   5   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0  13   7   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   9  13  20   7   0   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   2   9  35  35   5  14   1   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   1  23  61  31  31   3   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   1   5  31  50 113  22   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   1   7  34 142  54  25   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   1  11  11 126  67  53   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   3   7  72  54  52   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   2   4  46  27  31   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   1   1   6  26  33  17   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   0   1  24  16   9   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   0   3  14  13   7   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   0   1   8  11  10   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   0   0   6  12   6   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   0   1   5   7   7   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   0   1   4   8   4   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   0   0   1   8   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   0   0   1   2   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   0   0   2   0   2   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   0   0   2   3   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svcModel = SVC()\n",
    "svcModel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svcModel.predict(X_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "accuracyScore = accuracy_score(y_test, y_pred)\n",
    "confusionMatrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy Score: {accuracyScore}')\n",
    "print(f'Confusion Matrix: \\n{confusionMatrix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path)\n",
    "    a_X = df.iloc[:,:-1]\n",
    "    a_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return a_X, a_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ped\n",
       "0   15\n",
       "1    7\n",
       "2    9\n",
       "3   10\n",
       "4    7"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name = \"abalone_classification\"\n",
    "dat_X, dat_y = load_data(\"{}.csv\".format(data_name))\n",
    "dfy = pd.DataFrame(dat_y)\n",
    "df = pd.DataFrame(dat_X)\n",
    "col_names = ['col_1','col_2','col_3','col_4','col_5','col_6','col_7','col_8','col_9','col_10']\n",
    "col_names_y = ['ped']\n",
    "df.columns = col_names\n",
    "dfy.columns = col_names_y\n",
    "dfy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_2</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>col_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>col_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986812</td>\n",
       "      <td>0.827554</td>\n",
       "      <td>0.925261</td>\n",
       "      <td>0.897914</td>\n",
       "      <td>0.903018</td>\n",
       "      <td>0.897706</td>\n",
       "      <td>0.309666</td>\n",
       "      <td>-0.551465</td>\n",
       "      <td>0.236543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_2</th>\n",
       "      <td>0.986812</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833684</td>\n",
       "      <td>0.925452</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.899724</td>\n",
       "      <td>0.905330</td>\n",
       "      <td>0.318626</td>\n",
       "      <td>-0.564315</td>\n",
       "      <td>0.240376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_3</th>\n",
       "      <td>0.827554</td>\n",
       "      <td>0.833684</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819221</td>\n",
       "      <td>0.774972</td>\n",
       "      <td>0.798319</td>\n",
       "      <td>0.817338</td>\n",
       "      <td>0.298421</td>\n",
       "      <td>-0.518552</td>\n",
       "      <td>0.215459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_4</th>\n",
       "      <td>0.925261</td>\n",
       "      <td>0.925452</td>\n",
       "      <td>0.819221</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969405</td>\n",
       "      <td>0.966375</td>\n",
       "      <td>0.955355</td>\n",
       "      <td>0.299741</td>\n",
       "      <td>-0.557592</td>\n",
       "      <td>0.252038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_5</th>\n",
       "      <td>0.897914</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.774972</td>\n",
       "      <td>0.969405</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931961</td>\n",
       "      <td>0.882617</td>\n",
       "      <td>0.263991</td>\n",
       "      <td>-0.521842</td>\n",
       "      <td>0.251793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_6</th>\n",
       "      <td>0.903018</td>\n",
       "      <td>0.899724</td>\n",
       "      <td>0.798319</td>\n",
       "      <td>0.966375</td>\n",
       "      <td>0.931961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907656</td>\n",
       "      <td>0.308444</td>\n",
       "      <td>-0.556081</td>\n",
       "      <td>0.242194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_7</th>\n",
       "      <td>0.897706</td>\n",
       "      <td>0.905330</td>\n",
       "      <td>0.817338</td>\n",
       "      <td>0.955355</td>\n",
       "      <td>0.882617</td>\n",
       "      <td>0.907656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.306319</td>\n",
       "      <td>-0.546953</td>\n",
       "      <td>0.235391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_8</th>\n",
       "      <td>0.309666</td>\n",
       "      <td>0.318626</td>\n",
       "      <td>0.298421</td>\n",
       "      <td>0.299741</td>\n",
       "      <td>0.263991</td>\n",
       "      <td>0.308444</td>\n",
       "      <td>0.306319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.464298</td>\n",
       "      <td>-0.512528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_9</th>\n",
       "      <td>-0.551465</td>\n",
       "      <td>-0.564315</td>\n",
       "      <td>-0.518552</td>\n",
       "      <td>-0.557592</td>\n",
       "      <td>-0.521842</td>\n",
       "      <td>-0.556081</td>\n",
       "      <td>-0.546953</td>\n",
       "      <td>-0.464298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.522541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_10</th>\n",
       "      <td>0.236543</td>\n",
       "      <td>0.240376</td>\n",
       "      <td>0.215459</td>\n",
       "      <td>0.252038</td>\n",
       "      <td>0.251793</td>\n",
       "      <td>0.242194</td>\n",
       "      <td>0.235391</td>\n",
       "      <td>-0.512528</td>\n",
       "      <td>-0.522541</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           col_1     col_2     col_3     col_4     col_5     col_6     col_7  \\\n",
       "col_1   1.000000  0.986812  0.827554  0.925261  0.897914  0.903018  0.897706   \n",
       "col_2   0.986812  1.000000  0.833684  0.925452  0.893162  0.899724  0.905330   \n",
       "col_3   0.827554  0.833684  1.000000  0.819221  0.774972  0.798319  0.817338   \n",
       "col_4   0.925261  0.925452  0.819221  1.000000  0.969405  0.966375  0.955355   \n",
       "col_5   0.897914  0.893162  0.774972  0.969405  1.000000  0.931961  0.882617   \n",
       "col_6   0.903018  0.899724  0.798319  0.966375  0.931961  1.000000  0.907656   \n",
       "col_7   0.897706  0.905330  0.817338  0.955355  0.882617  0.907656  1.000000   \n",
       "col_8   0.309666  0.318626  0.298421  0.299741  0.263991  0.308444  0.306319   \n",
       "col_9  -0.551465 -0.564315 -0.518552 -0.557592 -0.521842 -0.556081 -0.546953   \n",
       "col_10  0.236543  0.240376  0.215459  0.252038  0.251793  0.242194  0.235391   \n",
       "\n",
       "           col_8     col_9    col_10  \n",
       "col_1   0.309666 -0.551465  0.236543  \n",
       "col_2   0.318626 -0.564315  0.240376  \n",
       "col_3   0.298421 -0.518552  0.215459  \n",
       "col_4   0.299741 -0.557592  0.252038  \n",
       "col_5   0.263991 -0.521842  0.251793  \n",
       "col_6   0.308444 -0.556081  0.242194  \n",
       "col_7   0.306319 -0.546953  0.235391  \n",
       "col_8   1.000000 -0.464298 -0.512528  \n",
       "col_9  -0.464298  1.000000 -0.522541  \n",
       "col_10 -0.512528 -0.522541  1.000000  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for high correlation among features\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x2520 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convertir les données en float\n",
    "df = df.astype(float)\n",
    "\n",
    "# Mise en place de la matrice de coorélation\n",
    "corr                             = df.corr()\n",
    "mask                             = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax                            = plt.subplots(figsize=(12,35))\n",
    "cmap                             = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Visualisation de la matrice\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.1, cbar_kws={\"shrink\": .1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns =['col_1',\t'col_2',\t'col_3',\t'col_4',\t'col_5']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "      <th>col_9</th>\n",
       "      <th>col_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    col_6  col_7  col_8  col_9  col_10\n",
       "0  0.1010  0.150    0.0    0.0     1.0\n",
       "1  0.0485  0.070    0.0    0.0     1.0\n",
       "2  0.1415  0.210    1.0    0.0     0.0\n",
       "3  0.1140  0.155    0.0    0.0     1.0\n",
       "4  0.0395  0.055    0.0    1.0     0.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x2520 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convertir les données en float\n",
    "df = df.astype(float)\n",
    "\n",
    "# Mise en place de la matrice de coorélation\n",
    "corr                             = df.corr()\n",
    "mask                             = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax                            = plt.subplots(figsize=(12,35))\n",
    "cmap                             = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Visualisation de la matrice\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.1, cbar_kws={\"shrink\": .1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myLDA(object):\n",
    "    '''\n",
    "    This class is for linear discriminant analysis classification.\n",
    "    \n",
    "    The class contains the parameters of LDA, including the number of classes and the prior probability p(i) of \n",
    "    each class $i$, where $i=1,2,\\ldots,num_classes$. Moreover, the class contains the the mean vectors $\\mu_i$ \n",
    "    and covariance matrix $\\Sigma$ of probability distributions $p(x|i)$ for the class $i$.\n",
    "    \n",
    "    It also contains the functions for initializing the class, fitting the LDA classifier model, use \n",
    "    the fitted model to calculate the linear discriminant functions $\\delta_i(x)$ and decision function $h^*(x)$.\n",
    "    \n",
    "    Attributes:\n",
    "        mu (matrix, num_classes*num_features)    : mean vectors of distributions $p(x|i)$. The $i$-th row represents $\\mu_i$.\n",
    "        Sigma (matrix, num_features*num_features): covariance matrix\n",
    "        num_classes (positive integer)           : the number of classes\n",
    "        priorProbs (vector, num_classes)         : the prior probability vector and its $i$-th element is $p(i)$\n",
    "        \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize the class by just assigning zero to all atrributes. \n",
    "        '''\n",
    "        self.mu = 0 \n",
    "        self.Sigma = 0\n",
    "        self.num_classes = 0\n",
    "        self.priorProbs = 0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        estimate the mean vector and covariance matrix of each class in the LDA model\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_train*num_features): features of training samples\n",
    "            y (matrix, num_train): label of training samples\n",
    "            \n",
    "        Returns:\n",
    "            mu (matrix, num_classes*num_features)    : mean vectors of distributions $p(x|i)$. The $i$-th row represents $\\mu_i$.\n",
    "            Sigma (matrix, num_features*num_features): covariance matrix\n",
    "        ''' \n",
    "        num_samples, num_features = X.shape\n",
    "        values, counts = np.unique(y, return_counts = True)\n",
    "        num_classes = len(values)\n",
    "        ### calculate the prior probability $p(i)$\n",
    "        self.priorProbs = counts / num_samples\n",
    "        ### calculate the mean vector of each class $\\mu_i$\n",
    "        self.mu = np.zeros((num_classes, num_features))\n",
    "        for k in range(num_samples):\n",
    "            self.mu[int(y[k]),:] += X[k,:]\n",
    "        self.mu = self.mu / np.expand_dims(counts, 1) \n",
    "        ### calculate the covariance matrix $\\Sigma$\n",
    "        Sigma_i = [np.cov(X[y == i].T)*(X[y == i].shape[0]-1) for i in range(num_classes)] \n",
    "        self.Sigma = sum(Sigma_i) / (X.shape[0]-num_classes)\n",
    "        return self.mu, self.Sigma\n",
    "    \n",
    "    def linear_discriminant_func(self, X):\n",
    "        '''\n",
    "        calculate the linear discriminant functions $\\delta_i(X)$\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_samples*num_features): features of samples\n",
    "            \n",
    "        Returns:\n",
    "            value (matrix, num_samples*num_classes): the linear discriminant function values. \n",
    "            The $(j,i)$-th entry of value represents $\\delta_i(X[j,:])$, which is the linear discriminant function value for the class $i$ of the sample at row $j$.\n",
    "        '''\n",
    "        ### calculate the inverse matrix of the covariance matrix $\\Sigma$\n",
    "        U, S, V = np.linalg.svd(self.Sigma)\n",
    "        Sn = np.linalg.inv(np.diag(S))\n",
    "        Sigma_inv = np.dot(np.dot(V.T, Sn), U.T)\n",
    "        ### calculate the linear discriminant function values of X\n",
    "        value = np.dot(np.dot(X, Sigma_inv), self.mu.T) - \\\n",
    "                0.5 * np.multiply(np.dot(self.mu, Sigma_inv).T, self.mu.T).sum(axis = 0).reshape(1, -1) + \\\n",
    "                np.log(np.expand_dims(self.priorProbs, axis = 0))\n",
    "        return value\n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        calculate the linear discriminant functions\n",
    "        \n",
    "        Args: \n",
    "            X (matrix, num_samples*num_features): features of samples\n",
    "            \n",
    "        Returns:\n",
    "            pred_label (vector, num_samples): the predicted labels of samples. The $j$-th entry represents the predicted label of the sample at row $j$.\n",
    "        '''\n",
    "        pred_value = self.linear_discriminant_func(X)\n",
    "        pred_label = np.argmax(pred_value, axis = 1)\n",
    "        return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4177, 5)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def data(dfx, dfy):\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(dfx)\n",
    "    data_y = pd.Categorical(dfy).codes.reshape(-1)\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_():\n",
    "    X, Y = data(dfx, dfy)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, Y, test_size=0.33, random_state=2200)\n",
    "    return train_X, test_X, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [4177, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\josep\\Dropbox\\PC\\Documents\\GitHub\\statistical_learning\\code\\code_classification\\Abalone\\abalone.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m myLDA()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m### fit the model with training data and get the estimation of mu and Sigma\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mu, Sigma \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(main_()[\u001b[39m0\u001b[39m], main_()[\u001b[39m2\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m### predict the label of test data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(main_()[\u001b[39m1\u001b[39m])\n",
      "\u001b[1;32mc:\\Users\\josep\\Dropbox\\PC\\Documents\\GitHub\\statistical_learning\\code\\code_classification\\Abalone\\abalone.ipynb Cell 15\u001b[0m in \u001b[0;36mmain_\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m Y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(Y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Randomly assingning a train and test set\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m train_X, test_X, train_y, test_y \u001b[39m=\u001b[39m train_test_split(X, Y, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.33\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m2200\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mreturn\u001b[39;00m train_X, test_X, train_y, test_y\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2445\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2442\u001b[0m \u001b[39mif\u001b[39;00m n_arrays \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   2443\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAt least one array required as input\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2445\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39;49marrays)\n\u001b[0;32m   2447\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[0;32m   2448\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2449\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m\n\u001b[0;32m   2450\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:433\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[39m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \n\u001b[0;32m    416\u001b[0m \u001b[39mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[39m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    432\u001b[0m result \u001b[39m=\u001b[39m [_make_indexable(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m iterables]\n\u001b[1;32m--> 433\u001b[0m check_consistent_length(\u001b[39m*\u001b[39;49mresult)\n\u001b[0;32m    434\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [4177, 1]"
     ]
    }
   ],
   "source": [
    "### initiate the LDA model\n",
    "model = myLDA()\n",
    "### fit the model with training data and get the estimation of mu and Sigma\n",
    "mu, Sigma = model.fit(main_()[0], main_()[2])\n",
    "### predict the label of test data\n",
    "y_pred = model.predict(main_()[1])\n",
    "### calculate the accuracy of the fitted LDA model on test data\n",
    "accuracy = np.sum(y_pred == main_()[3])/len(main_()[3])\n",
    "print(\"Accuracy of LDA on the test dataset is {}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 11 elements, new values have 9 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\josep\\Dropbox\\PC\\Documents\\GitHub\\statistical_learning\\code\\code_classification\\Abalone\\abalone.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(data_path, header\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m col_names \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mcol_1\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcol_2\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcol_3\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcol_4\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcol_5\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcol_6\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcol_7\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcol_8\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m col_names\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/josep/Dropbox/PC/Documents/GitHub/statistical_learning/code/code_classification/Abalone/abalone.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m df\u001b[39m.\u001b[39mhead(\u001b[39m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5588\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5586\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   5587\u001b[0m     \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(\u001b[39mself\u001b[39m, name)\n\u001b[1;32m-> 5588\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__setattr__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name, value)\n\u001b[0;32m   5589\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m   5590\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\properties.pyx:70\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:769\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_axis\u001b[39m(\u001b[39mself\u001b[39m, axis: \u001b[39mint\u001b[39m, labels: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    768\u001b[0m     labels \u001b[39m=\u001b[39m ensure_index(labels)\n\u001b[1;32m--> 769\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mset_axis(axis, labels)\n\u001b[0;32m    770\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:214\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_axis\u001b[39m(\u001b[39mself\u001b[39m, axis: \u001b[39mint\u001b[39m, new_labels: Index) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m     \u001b[39m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_set_axis(axis, new_labels)\n\u001b[0;32m    215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis] \u001b[39m=\u001b[39m new_labels\n",
      "File \u001b[1;32mc:\\Users\\josep\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\base.py:69\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[39melif\u001b[39;00m new_len \u001b[39m!=\u001b[39m old_len:\n\u001b[1;32m---> 69\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     70\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLength mismatch: Expected axis has \u001b[39m\u001b[39m{\u001b[39;00mold_len\u001b[39m}\u001b[39;00m\u001b[39m elements, new \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     71\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalues have \u001b[39m\u001b[39m{\u001b[39;00mnew_len\u001b[39m}\u001b[39;00m\u001b[39m elements\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     72\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 11 elements, new values have 9 elements"
     ]
    }
   ],
   "source": [
    "# I am loading the full dataset and renaming the columns to keep better track of each attribute\n",
    "data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "data_path = os.path.join(data_dir, \"abalone_classification.csv\")\n",
    "df = pd.read_csv(data_path, header=0)\n",
    "col_names = ['col_1','col_2','col_3','col_4','col_5','col_6','col_7','col_8','name']\n",
    "df.columns = col_names\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>col_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.163951</td>\n",
       "      <td>0.158175</td>\n",
       "      <td>0.064922</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.075043</td>\n",
       "      <td>-0.124540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_3</th>\n",
       "      <td>-0.163951</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.059668</td>\n",
       "      <td>-0.008083</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>-0.185805</td>\n",
       "      <td>-0.022043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_4</th>\n",
       "      <td>0.158175</td>\n",
       "      <td>0.059668</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.005931</td>\n",
       "      <td>-0.009040</td>\n",
       "      <td>-0.103591</td>\n",
       "      <td>-0.054797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_5</th>\n",
       "      <td>0.064922</td>\n",
       "      <td>-0.008083</td>\n",
       "      <td>-0.005931</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.002829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_6</th>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>-0.009040</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>-0.035659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_7</th>\n",
       "      <td>0.075043</td>\n",
       "      <td>-0.185805</td>\n",
       "      <td>-0.103591</td>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.089690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col_8</th>\n",
       "      <td>-0.124540</td>\n",
       "      <td>-0.022043</td>\n",
       "      <td>-0.054797</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>-0.035659</td>\n",
       "      <td>0.089690</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          col_1     col_3     col_4     col_5     col_6     col_7     col_8\n",
       "col_1  1.000000 -0.163951  0.158175  0.064922  0.005597  0.075043 -0.124540\n",
       "col_3 -0.163951  1.000000  0.059668 -0.008083  0.009378 -0.185805 -0.022043\n",
       "col_4  0.158175  0.059668  1.000000 -0.005931 -0.009040 -0.103591 -0.054797\n",
       "col_5  0.064922 -0.008083 -0.005931  1.000000 -0.009674  0.043627  0.002829\n",
       "col_6  0.005597  0.009378 -0.009040 -0.009674  1.000000  0.020900 -0.035659\n",
       "col_7  0.075043 -0.185805 -0.103591  0.043627  0.020900  1.000000  0.089690\n",
       "col_8 -0.124540 -0.022043 -0.054797  0.002829 -0.035659  0.089690  1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for high correlation among features\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_1</th>\n",
       "      <th>col_3</th>\n",
       "      <th>col_4</th>\n",
       "      <th>col_5</th>\n",
       "      <th>col_6</th>\n",
       "      <th>col_7</th>\n",
       "      <th>col_8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500121</td>\n",
       "      <td>0.500034</td>\n",
       "      <td>0.261186</td>\n",
       "      <td>0.504717</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.499885</td>\n",
       "      <td>0.276199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.137299</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>0.137098</td>\n",
       "      <td>0.048351</td>\n",
       "      <td>0.075683</td>\n",
       "      <td>0.057797</td>\n",
       "      <td>0.106491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             col_1        col_3        col_4        col_5        col_6  \\\n",
       "count  1484.000000  1484.000000  1484.000000  1484.000000  1484.000000   \n",
       "mean      0.500121     0.500034     0.261186     0.504717     0.007500   \n",
       "std       0.137299     0.086670     0.137098     0.048351     0.075683   \n",
       "min       0.110000     0.210000     0.000000     0.500000     0.000000   \n",
       "25%       0.410000     0.460000     0.170000     0.500000     0.000000   \n",
       "50%       0.490000     0.510000     0.220000     0.500000     0.000000   \n",
       "75%       0.580000     0.550000     0.320000     0.500000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.830000   \n",
       "\n",
       "             col_7        col_8  \n",
       "count  1484.000000  1484.000000  \n",
       "mean      0.499885     0.276199  \n",
       "std       0.057797     0.106491  \n",
       "min       0.000000     0.000000  \n",
       "25%       0.480000     0.220000  \n",
       "50%       0.510000     0.220000  \n",
       "75%       0.530000     0.300000  \n",
       "max       0.730000     1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CYT    463\n",
      "NUC    429\n",
      "MIT    244\n",
      "ME3    163\n",
      "ME2     51\n",
      "ME1     44\n",
      "EXC     35\n",
      "VAC     30\n",
      "POX     20\n",
      "ERL      5\n",
      "Name: name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.name.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x3600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Mise en place de la matrice de coorélation\n",
    "corr                             = df.corr()\n",
    "mask                             = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax                            = plt.subplots(figsize=(10,50))\n",
    "cmap                             = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Visualisation de la matrice\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, square=True, linewidths=.1, cbar_kws={\"shrink\": .1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found a correlation between the different measurements of Hold, Pressure and Finger-Area with their averages. I also found a correlation with the different measurements of Hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LDA on the test dataset is 0.5993265993265994.\n"
     ]
    }
   ],
   "source": [
    "### initiate the LDA model\n",
    "data_X = df.iloc[:,:-1]\n",
    "data_y = df.iloc[:,-1]\n",
    "scaler_X = StandardScaler()\n",
    "data_X = scaler_X.fit_transform(data_X)\n",
    "data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "# Randomly assingning a train and test set\n",
    "train_X, test_X, train_y, test_y = train_test_split(data_X, data_y, test_size=0.20, random_state=13)\n",
    "\n",
    "model = myLDA()\n",
    "### fit the model with training data and get the estimation of mu and Sigma\n",
    "mu, Sigma = model.fit(train_X, train_y)\n",
    "### predict the label of test data\n",
    "y_pred = model.predict(test_X)\n",
    "### calculate the accuracy of the fitted LDA model on test data\n",
    "accuracy = np.sum(y_pred == test_y)/len(test_y)\n",
    "print(\"Accuracy of LDA on the test dataset is {}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_f = RandomForestClassifier()\n",
    "algo_lda = LinearDiscriminantAnalysis()\n",
    "algo_dt = DecisionTreeClassifier()\n",
    "model_forrest = algo_f.fit(main()[0], main()[2])\n",
    "model_LDA = algo_lda.fit(main()[0], main()[2])\n",
    "model_DT = algo_dt.fit(main()[0], main()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 48.485 for the DT model\n",
      " Accuracy: 62.626 for the forrest model\n",
      " Accuracy: 54.882 for the LDA model\n"
     ]
    }
   ],
   "source": [
    "print(\" Accuracy: %.3f for the DT model\" % (model_DT.score(main()[1], main()[3])*100.0))\n",
    "print(\" Accuracy: %.3f for the forrest model\" % (model_forrest.score(main()[1], main()[3])*100.0))\n",
    "print(\" Accuracy: %.3f for the LDA model\" % (model_LDA.score(main()[1], main()[3])*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I again use the processed data from the logistic regression but this time I specify SVC\n",
    "\n",
    "SVM_best_scores = {}\n",
    "tree_param = {'criterion':['gini','entropy'],'max_depth':np.arange(3, 15)}\n",
    "search_svm = GridSearchCV(estimator = algo_dt ,param_grid= tree_param,\n",
    "                    cv = 5, return_train_score = True,\n",
    "                    n_jobs = -1)\n",
    "\n",
    "search_svm.fit(main()[0], main()[2])\n",
    "SVM_best_scores = {'model':search_svm, 'best_params':search_svm.best_params_,\n",
    "                        'best_score':search_svm.best_score_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_depth': 5}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SVM_best_scores['best_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(DecisionTreeClassifier(), tree_param, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "              param_grid={'bootstrap': [False], 'criterion': ['gini', 'entropy'],\n",
       "                          'max_depth': range(1, 10),\n",
       "                          'max_features': ['auto', 'sqrt'],\n",
       "                          'min_samples_split': [3, 7, 10],\n",
       "                          'n_estimators': [10, 20, 71]}),\n",
       " 'best_params': {'bootstrap': False,\n",
       "  'criterion': 'entropy',\n",
       "  'max_depth': 9,\n",
       "  'max_features': 'auto',\n",
       "  'min_samples_split': 10,\n",
       "  'n_estimators': 71},\n",
       " 'best_score': 0.626809204694536}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# utilisation d'une grille complete avec toutes les parametres jugés nécessaires\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": [10,20,71],\n",
    "    \"bootstrap\": [False],\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_split\" : [3,7, 10],\n",
    "    \"max_depth\" :range(10)[1:],\n",
    "    \"max_features\": ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Effectuer grid search\n",
    "grid_search = GridSearchCV(algo_f, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(main()[0], main()[2])\n",
    "forrest_best_scores = {'model':grid_search, 'best_params':grid_search.best_params_,\n",
    "                        'best_score':grid_search.best_score_}\n",
    "forrest_best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets function\n",
    "def load_data(data_file_name):\n",
    "    data_dir = \"..\\..\\..\\data\\data_classification\"\n",
    "    data_path = os.path.join(data_dir, data_file_name)\n",
    "    df = pd.read_csv(data_path, header=1)\n",
    "    data_X = df.iloc[:,:-1]\n",
    "    data_y = df.iloc[:,-1]\n",
    "    scaler_X = StandardScaler()\n",
    "    data_X = scaler_X.fit_transform(data_X)\n",
    "    data_y = pd.Categorical(data_y).codes.reshape(-1)\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    # read dataset from csv file\n",
    "    data_name = \"abalone_classification\"\n",
    "    data_X, data_y = load_data(\"{}.csv\".format(data_name))\n",
    "\n",
    "    # Train and test set\n",
    "    # kf = KFold(n_splits=10)\n",
    "    # res_list = []\n",
    "    # for train_index, test_index in kf.split(data_X):\n",
    "    #     train_X, train_y = data_X[train_index,:], data_y[train_index]\n",
    "    #     test_X, test_y = data_X[test_index,:], data_y[test_index]\n",
    "    return data_X, data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criterion -- Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_counts(y, sample_weight, classes_):\n",
    "    '''\n",
    "    the function used to calculate the summation of weights of samples from each class. Generally speaking,\n",
    "    the weights are all set as one. But for Adaboost, each sample has different values.\n",
    "    '''\n",
    "    class_counts = np.zeros(shape=classes_.shape[0], dtype=np.float64)\n",
    "    for i, label in enumerate(classes_):\n",
    "        idx = y == label\n",
    "        if idx.sum() > 0:\n",
    "            class_counts[i] = sample_weight[idx].sum()\n",
    "        else:\n",
    "            class_counts[i] = 0\n",
    "    return class_counts\n",
    "\n",
    "def gini(y, sample_weight):\n",
    "    classes_ = np.unique(y)\n",
    "    class_counts = calculate_weighted_counts(y, sample_weight, classes_)\n",
    "    if class_counts.sum() > 0:\n",
    "        pk = class_counts / class_counts.sum()\n",
    "        pk = pk[pk > 0]\n",
    "        return 1 - np.sum(pk**2)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def gini_index(X, y, feat, point, sample_weight):\n",
    "    '''\n",
    "    calculate the difference of gini index before and after splitting\n",
    "    '''\n",
    "    S = gini(y, sample_weight)\n",
    "    new_S = 0\n",
    "    n = sample_weight.sum()\n",
    "    assert n > 0\n",
    "    idx1 = X[:, feat] < point\n",
    "    nv = sample_weight[idx1].sum()\n",
    "    if nv > 0:\n",
    "        new_S += nv / n * gini(y[idx1], sample_weight[idx1])\n",
    "    idx2 = X[:, feat] >= point\n",
    "    nv = sample_weight[idx2].sum()\n",
    "    if nv > 0:\n",
    "        new_S += nv / n * gini(y[idx2], sample_weight[idx2])\n",
    "    return S - new_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Decision Trees for Classification\n",
    "\n",
    "Different from the classification tree implemented in the last tutorial:\n",
    "1. Each internal node has two child nodes regardless of values of the splitting feature are continuous or discrete.\n",
    "2. Add the parameter `max_depth` for providing another condition to stop splitting procedures.\n",
    "3. Add the parameter `max_features` to use the subset of features to build decision tree.\n",
    "\n",
    "Options 2&3 are designed for constructing trees in the random forest implemented in Section 5. \n",
    "\n",
    "If you'd like to build the classification tree, you can ignore options 2&3 by setting `max_depth = None` and `max_features = None`. Then we can combine it with the pre-pruning or post-pruning technique implemented in Section 4 to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier(object):\n",
    "    '''\n",
    "    This class is for classification tree\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of classification tree\n",
    "        - tree: a nested dictionary representing the decision tree structure.\n",
    "        - max_depth: the parameter to control the depth of tree. If the depth is larger than max_depth, we will stop splitting.\n",
    "        - max_feature: the number of selected features to build decision tree\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 criterion=gini_index,\n",
    "                 max_depth=None,\n",
    "                 max_features=None,\n",
    "                 random_seed=None):\n",
    "        self.f_criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        if self.max_depth is None:\n",
    "            self.max_depth = 2**10\n",
    "        self.max_features = max_features\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        np.random.seed(self.random_seed)\n",
    "        num_samples, num_features = X.shape\n",
    "        if self.max_features is None:\n",
    "            self.max_features = num_features\n",
    "        elif self.max_features == \"sqrt\":\n",
    "            self.max_features = np.int(np.round(np.sqrt(num_features)))\n",
    "        self.classes_ = np.unique(y)\n",
    "        if sample_weight is None:\n",
    "            sample_weight = np.ones(num_samples, dtype=np.float64)\n",
    "        # build the decision tree\n",
    "        self.tree = self.create_tree(X, y, sample_weight, depth=0)\n",
    "\n",
    "    def create_tree(self, X, y, sample_weight, depth):\n",
    "        Tree = {}\n",
    "        Tree[\"depth\"] = depth\n",
    "        class_counts = calculate_weighted_counts(y, sample_weight, self.classes_)\n",
    "        # create a leaf node if all samples belong to the same class\n",
    "        if (class_counts != 0).sum() == 1:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            Tree[\"pred\"] = self.classes_[class_counts != 0]\n",
    "        # using the majority vote to get the prediction at each node\n",
    "        majority_class = self.classes_[np.argmax(class_counts)]\n",
    "        Tree[\"pred\"] = majority_class\n",
    "        # create a leaf node if feature set is empty\n",
    "        feat, point = self.choose_best_split(X, y, sample_weight)\n",
    "        if feat is None or depth == self.max_depth:\n",
    "            Tree[\"is_leaf\"] = True\n",
    "            return Tree\n",
    "        # otherwise, create an internal node\n",
    "        Tree[\"is_leaf\"] = False\n",
    "        Tree[\"split_feat\"] = feat\n",
    "        Tree[\"split_point\"] = point\n",
    "        # build the left subtree\n",
    "        idx = X[:, feat] < point\n",
    "        Tree[\"left\"] = self.create_tree(X[idx], y[idx], sample_weight[idx],\n",
    "                                        depth + 1)\n",
    "        # build the right subtree\n",
    "        idx = X[:, feat] >= point\n",
    "        Tree[\"right\"] = self.create_tree(X[idx], y[idx], sample_weight[idx],\n",
    "                                         depth + 1)\n",
    "        return Tree\n",
    "\n",
    "    def choose_best_split(self, X, y, sample_weight):\n",
    "        # initialization\n",
    "        best_feat, best_point = None, None\n",
    "        best_score = 0.0\n",
    "        # search for each candidate feature\n",
    "        num_features = X.shape[1]\n",
    "        if self.max_features < num_features:\n",
    "            candidate_feat = np.random.permutation(\n",
    "                num_features)[:self.max_features]\n",
    "        else:\n",
    "            candidate_feat = np.arange(num_features)\n",
    "        for feat in candidate_feat:\n",
    "            # if all values of this feature are equal, do not split this feature\n",
    "            X_feat_value = np.unique(X[:, feat])\n",
    "            if len(X_feat_value) == 1:\n",
    "                continue\n",
    "            # search for each possible split point\n",
    "            for i in range(len(X_feat_value) - 1):\n",
    "                # divide the dataset into two parts according to the split\n",
    "                point = (X_feat_value[i] + X_feat_value[i + 1]) / 2.0\n",
    "                # calculate score to evaluate the quality of a split\n",
    "                score = self.f_criterion(X, y, feat, point, sample_weight)\n",
    "                if score > best_score:\n",
    "                    best_feat = feat\n",
    "                    best_point = point\n",
    "                    best_score = score\n",
    "        return best_feat, best_point\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to fit the decision tree classifier\n",
    "\n",
    "        Args:\n",
    "            X - features of test samples, a pandas dataframe with shape (n, d)\n",
    "\n",
    "        Returns:\n",
    "            y - predictions of test samples, a pandas series with shape (n,)\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y = []\n",
    "        for i in range(n):\n",
    "            y.append(DecisionTreeClassifier.predict_each(X[i], self.tree))\n",
    "        y = np.array(y, dtype=np.int32)\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def predict_each(x, tree):\n",
    "        '''\n",
    "        for each sample, get the prediction of decision tree classifier in a recursive manner.\n",
    "\n",
    "        Args:\n",
    "            x - features of a sample, a pandas Series with shape (d,)\n",
    "            tree - a nested dictionary representing the decision tree structure.\n",
    "\n",
    "        Returns:\n",
    "            the prediction of the sample `x`\n",
    "        '''\n",
    "        if tree[\"is_leaf\"] is True:\n",
    "            # if the `tree` is a leaf node, get the prediction at the leaf node\n",
    "            return tree[\"pred\"]\n",
    "        else:\n",
    "            # the 'tree' is a nested dictionary\n",
    "            # get the value of the feature used to split\n",
    "            feat = tree[\"split_feat\"]\n",
    "            point = tree[\"split_point\"]\n",
    "            # get the value of the feature for the sample `x`\n",
    "            value = x[feat]\n",
    "            if value < point:\n",
    "                return DecisionTreeClassifier.predict_each(x, tree[\"left\"])\n",
    "            else:\n",
    "                return DecisionTreeClassifier.predict_each(x, tree[\"right\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_t():\n",
    "    # Randomly assingning a train and test set\n",
    "    train_X, test_X, train_y, test_y = train_test_split(main()[0], main()[1], test_size=0.33, random_state=100)\n",
    "    return train_X, test_X, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of training data is: 1.0\n",
      "The accuracy of test data is: 0.20449601160261058\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = t_t()\n",
    "model = DecisionTreeClassifier(criterion=gini_index,\n",
    "                                   max_depth=None,\n",
    "                                   max_features=None,\n",
    "                                   random_seed=None)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"The accuracy of training data is:\", acc_train)\n",
    "print(\"The accuracy of test data is:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Random Forest for Classification\n",
    "\n",
    "In this section, we implement the random forest where each tree is built with the class DecisionTreeClassifier(). In our model, the values of parameters are listed below. \n",
    "1. The number of trees $T$ is set as ``num_estimators = 20``\n",
    "2. the number of subsampled features for each tree is $k =\\sqrt{d}$, which corresponds to ``max_features = \"sqrt\"`` in the code.\n",
    "3. The maximum depth of each tree is ``max_depth = 6``.\n",
    "\n",
    "We will not use the pruning technique for each tree in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier(object):\n",
    "    '''\n",
    "    This class is for random forest classification\n",
    "\n",
    "    Attributes:\n",
    "        - criterion: a function used as the criterion of classification tree\n",
    "        - num_estimators: the number of trees in the random forest \n",
    "        - tree: a nested dictionary representing the decision tree structure\n",
    "        - max_depth: the parameter to control the depth of tree. If the depth is larger than max_depth, we will stop splitting.\n",
    "        - max_feature: the number of selected features to build decision tree\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 num_estimators,\n",
    "                 random_state,\n",
    "                 criterion=gini_index,\n",
    "                 max_depth=None,\n",
    "                 max_features=\"sqrt\"):\n",
    "        self.num_estimators = num_estimators\n",
    "        self.random_state = random_state\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        function used to fit all trees in the random forest\n",
    "        \n",
    "        Args:\n",
    "            X - the features of the training samples\n",
    "            y - the labels of the training samples\n",
    "        Returns:\n",
    "            self.model_list - the model list containing `num_estimators` tree models\n",
    "        '''\n",
    "        n, d = X.shape\n",
    "        RandomState = np.random.RandomState(self.random_state)\n",
    "        self.model_list = []\n",
    "        for t in range(self.num_estimators):\n",
    "            random_seed = RandomState.randint(0, np.iinfo(np.int32).max)\n",
    "            ### draw a bootstrapped dataset from X\n",
    "            sample_index = RandomState.choice(np.arange(n), size=n, replace=True)\n",
    "            X_sampled = X[sample_index, :]\n",
    "            y_sampled = y[sample_index]\n",
    "            ### initialize the tree model by using DecisionTreeClassifier()\n",
    "            model = DecisionTreeClassifier(criterion=self.criterion,\n",
    "                                           max_depth=self.max_depth,\n",
    "                                           max_features=self.max_features,\n",
    "                                           random_seed=random_seed)\n",
    "            ### fit the tree model to the bootstrapped dataset by using DecisionTreeClassifier.fit()\n",
    "            model.fit(X_sampled, y_sampled)\n",
    "            self.model_list.append(model)\n",
    "        return self.model_list\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        function used to predict the labels of X\n",
    "        \n",
    "        Args:\n",
    "            X - the features of the test samples\n",
    "        Returns:\n",
    "            y_pred_label - the predicted labels of test samples\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        y_pred = np.zeros([self.num_estimators, n], dtype=np.int32)\n",
    "        y_pred_label = np.zeros(n, dtype=np.int32)\n",
    "        ### use T tree classifiers to make predictions by using DecisionTreeClassifier.predict()\n",
    "        for i in range(self.num_estimators):\n",
    "            model_i = self.model_list[i]\n",
    "            y_pred[i, :] = model_i.predict(X)\n",
    "        ### take the majority vote \n",
    "        for i in range(n):\n",
    "            classes, count = np.unique(y_pred[:, i], return_counts=True)\n",
    "            y_pred_label[i] = classes[np.argmax(count)]\n",
    "        return y_pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\josep\\AppData\\Local\\Temp\\ipykernel_3900\\4230132589.py:29: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.max_features = np.int(np.round(np.sqrt(num_features)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy of the random forest is: 0.3385770468358956\n",
      "Testing accuracy of the random forest is: 0.25598259608411894\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = t_t()\n",
    "model = RandomForestClassifier(num_estimators=20,\n",
    "                                   random_state=101,\n",
    "                                   criterion=gini_index,\n",
    "                                   max_depth=6)\n",
    "model.fit(X_train, y_train)\n",
    "y_train_hat = model.predict(X_train)\n",
    "y_test_hat = model.predict(X_test)\n",
    "acc_train = (y_train == y_train_hat).mean()\n",
    "acc_test = (y_test == y_test_hat).mean()\n",
    "print(\"Training accuracy of the random forest is:\", acc_train)\n",
    "print(\"Testing accuracy of the random forest is:\", acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "93028d5495cf3fdad3791cfb45569ed1ffef5b94a8e8037ba1bdda77d837769f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
